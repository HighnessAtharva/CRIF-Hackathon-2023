Article,Title,Description,URL,Content,Published
0,Get a Special Deal on This Top Python Bundle,Make 2023 the year you finally start to code.,https://www.entrepreneur.com/science-technology/get-a-special-deal-on-this-top-python-bundle/441891,Get a Special Deal on This Top Python Bundle Make 2023 the year you finally start to code. Entrepreneurcontributors are their own. In the digital world there are many good reasons to learn to code. That goes double for entrepreneurs carving out their own place in the world. In addition learning to code can help you make many technical ideas a reality save you copious amounts of time and guide your way as you scale. And if you want to learn to code theres no better place to start than with Python. During our Same You New Job promotion you can save a bundle on The Premium Python Certification Bootcamp Bundle. This 13course bundle typically costs over 2000 but you can get it at 98 off if you grab it by 1159 p. m. Pacific on January 9th. The massive bundle is filled with courses from some of the webs top instructors from Dr. Chris Mall 4. 65star instructor rating to ZENVA Academy 4. 45star rating. So even if youve never seen a line of code before youll be able to build a foundation in Python that will help you graduate into other languages. After all Python is the top coding language that most professionals recommend you start with. Starting out youll learn the basics of computer programming as you explore topics like arithmetic operators strings lists and more. Youll make your first statement utilize variables and operators manage data with lists tuples ranges and dictionaries control program flow with conditionals and loops and more. Eventually youll work your way up to using Python Django for web development and explore several other Python applications including machine learning data science ethical hacking image processing and beyond. Make 2023 the year you learn to code. Enroll by January 9th to get The Premium Python Certification Bootcamp Bundle for only 29. 99. Prices subject to change. Entrepreneur Editors Picks These Are the Hottest Franchises to Watch in 2023 This One Question Will Make You a Better Thinker. Are You Asking It Already How the CEO of Zoom Room Is Leading the Way in Dog Training and Education Before Pressing the Layoff Button Leaders Need to Ask Themselves 1 Question Then Do These 3 Things 3 Simple Strategies for Coping With Overwork Pressure Taco Bell Uses This LittleKnown Secret to Stay on Top Year After Year Once a Skeptic Elon Musk Now Embraces This Divisive Workplace Policy and You Should Too.,2023-01-04T17:00:00Z
1,"TIOBE Calculates C++, C, and Python Rose the Most in Popularity in 2022","""The Tiobe index gauges language popularity using a formula that assesses searches on programming languages in Google, Bing, Yahoo, Wikipedia, and other search engines,"" writes InfoWorld. And they add that this year the ""vaunted"" C++ programming language was …",https://developers.slashdot.org/story/23/01/08/0252232/tiobe-calculates-c-c-and-python-rose-the-most-in-popularity-in-2022,TIOBE Calculates C C and Python Rose the Most in Popularity in 2022 infoworld. com 83 TIOBEs announcement includes their calculation that C rose 4. 62 in popularity in 2022 Runners up are C 3. 82 and Python 2. 78. Interestingly C surpassed Java to become the number 3 of the TIOBE index in November 2022. The reason for Cs popularity is its excellent performance while being a high level objectoriented language. Because of this it is possible to develop fast and vast software systems over millions of lines of code in C without necessarily ending up in a maintenance nightmare. So which programming languages are most popular now For what its worth heres TIOBEs latest ranking Python C C Java C Visual Basic JavaScript SQL Assembly Language PHP InfoWorld adds that Helping C popularity was the publication of new language standards with interesting features such as C 11 and C 20. More from TIOBE What else happened in 2022 Performance seemed to be important. C competitor Rust entered the top 20 again being at position 26 one year ago but this time it seems to be for real. Lua which is known for its easy interfacing with C jumped from position 30 to 24. F is another language that made an interesting move from position 74 to position 33 in one years time. Promising languages such as Kotlin from 29 to 25 Julia from 28 to 29 and Dart from 37 to 38 still have a long way to go before they reach the top 20. Lets see what happens in 2023.,2023-01-08T08:34:00Z
2,A Guide To Command-Line Data Manipulation,"No more random scripts in Python and JavaScript to transform CSV or JSON data. In this article, Alvin Bryan shows you how to use Miller, a small and powerful CLI tool, to do all your data processing.",https://www.smashingmagazine.com/2022/12/guide-command-line-data-manipulation-cli-miller/,A Guide To CommandLine Data Manipulation Allow me to preface this article by saying that Im not a terminal person. I dont use Vim. I find sed grep and awk convoluted and counterintuitive. I prefer seeing my files in a nice UI. Despite all that I got into the habit of reaching for commandline interfaces CLIs when I had small dedicated tasks to complete. Why Ill explain all of that below. In this article youll also learn how to use a CLI tool named Miller to manipulate data from CSV TSV andor JSON files. Why Use The Command Line Everything that Im showing here can be done with regular code. You can load the file parse the CSV data and then transform it using regular JavaScript Python or any other language. But there are a few reasons why I reach out for commandline interfaces CLIs whenever I need to transform data Easier to read. It is faster for me to write a script in JavaScript or Python for my usual data processing. But a script can be confusing to come back to. In my experience commandline manipulations are harder to write initially but easier to read afterward. Easier to reproduce. Thanks to package managers like Homebrew CLIs are much easier to install than they used to be. No need to figure out the correct version of Node. js or Python the package manager takes care of that for you. Ages well. Compared to modern programming languages CLIs are old. They change a lot more slowly than languages and frameworks. What Is Miller The main reason I love Miller is that its a standalone tool. There are many great tools for data manipulation but every other tool I found was part of a specific ecosystem. The tools written in Python required knowing how to use pip and virtual environments for those written in Rust it was cargo and so on. On top of that its fast. The data files are streamed not held in memory which means that you can perform operations on large files without freezing your computer. As a bonus Miller is actively maintained John Kerl really keeps on top of PRs and issues. As a developer I always get a satisfying feeling when I see a neat and maintained opensource project with great documentation. Installation Linux aptget install milleror Homebrew. macOS brew install millerusing Homebrew. Windows choco install millerusing Chocolatey. Thats it and you should now have the mlr command available in your terminal. Run mlr help topics to see if it worked. This will give you instructions to navigate the builtin documentation. You shouldnt need it though thats what this tutorial is for How mlr Works Miller commands work the following way mlr inputoutput file formats verbs file Example mlr csv filter color red example. csv Lets deconstruct csvspecifies the input file format. Its a CSV file. filteris what were doing on the file called a verb in the documentation. In this case were filtering every row that doesnt have the field colorset to red. There are many other verbs like sortand cutthat well explore later. example. csvis the file that were manipulating. Operations Overview We can use those verbs to run specific operations on your data. Theres a lot we can do. Lets explore. Data Ill be using a dataset of IMDb ratings for American TV dramas created by The Economist. You can download it here or find it in the repo for this article. Note For the sake of brevity Ive renamed the file from mlr csv head. IMDbEconomisttvratings. csv to tvratings. csv. Above I mentioned that every command contains a specific operation or verb. Lets learn our first one called head. What it does is show you the beginning of the file the head rather than print the entire file in the console. You can run the following command mlr csv head. tvratings. csv And this is the output youll see titleIdseasonNumbertitledateavratingsharegenres tt2879552111. 22. 63201603108. 4890. 51DramaMysterySciFi tt3148266112 Monkeys201502278. 34070. 46AdventureDramaMystery tt3148266212 Monkeys201605308. 81960. 25AdventureDramaMystery tt3148266312 Monkeys201705199. 03690. 19AdventureDramaMystery tt3148266412 Monkeys201806269. 13630. 38AdventureDramaMystery tt1837492113 Reasons Why201703318. 4372. 38DramaMystery tt1837492213 Reasons Why201805187. 50892. 19DramaMystery tt0285331124200202168. 56416. 67ActionCrimeDrama tt0285331224200302098. 70287. 13ActionCrimeDrama tt0285331324200402098. 71735. 88ActionCrimeDrama This is a bit hard to read so lets make it easier on the eye by adding opprint. mlr csv opprint head. tvratings. csv The resulting output will be the following titleId seasonNumber title date avrating share genres tt2879552 1 11. 22. 63 20160310 8. 489 0. 51 DramaMysterySciFi tt3148266 1 12 Monkeys 20150227 8. 3407 0. 46 AdventureDramaMystery tt3148266 2 12 Monkeys 20160530 8. 8196 0. 25 AdventureDramaMystery tt3148266 3 12 Monkeys 20170519 9. 0369 0. 19 AdventureDramaMystery tt3148266 4 12 Monkeys 20180626 9. 1363 0. 38 AdventureDramaMystery tt1837492 1 13 Reasons Why 20170331 8. 437 2. 38 DramaMystery tt1837492 2 13 Reasons Why 20180518 7. 5089 2. 19 DramaMystery tt0285331 1 24 20020216 8. 5641 6. 67 ActionCrimeDrama tt0285331 2 24 20030209 8. 7028 7. 13 ActionCrimeDrama tt0285331 3 24 20040209 8. 7173 5. 88 ActionCrimeDrama Much better isnt it Note Rather than typing csv opprint every time we can use the c2p option which is a shortcut. Chaining Thats where the fun begins. Rather than run multiple commands we can chain the verbs together by using the then keyword. Remove columns You can see that theres a titleId column that isnt very useful. Lets get rid of it using the cut verb. mlr c2p cut x f titleId then head. tvratings. csv It gives you the following output seasonNumber title date avrating share genres 1 11. 22. 63 20160310 8. 489 0. 51 DramaMysterySciFi 1 12 Monkeys 20150227 8. 3407 0. 46 AdventureDramaMystery 2 12 Monkeys 20160530 8. 8196 0. 25 AdventureDramaMystery 3 12 Monkeys 20170519 9. 0369 0. 19 AdventureDramaMystery 4 12 Monkeys 20180626 9. 1363 0. 38 AdventureDramaMystery 1 13 Reasons Why 20170331 8. 437 2. 38 DramaMystery 2 13 Reasons Why 20180518 7. 5089 2. 19 DramaMystery 1 24 20020216 8. 5641 6. 67 ActionCrimeDrama 2 24 20030209 8. 7028 7. 13 ActionCrimeDrama 3 24 20040209 8. 7173 5. 88 ActionCrimeDrama Fun Fact This is how I first learned about Miller I was playing with a CSV dataset for httpsdetails. town that had a useless column and I looked up how to remove a column from CSV command line. I discovered Miller loved it and then pitched an article to Smashing magazine. Now here we are Filter This is the verb that I first showed earlier. We can remove all the rows that dont match a specific expression letting us clean our data with only a few characters. If we only want the rating of the first seasons of every series in the dataset this is how you do it mlr c2p filter seasonNumber 1 then head. tvratings. csv Sorting We can sort our data based on a specific column like it would be in a UI like Excel or macOS Numbers. Heres how you would sort your data based on the series with the highest rating mlr c2p sort nr avrating then head. tvratings. csv The resulting output will be the following titleId seasonNumber title date avrating share genres tt0098887 1 Parenthood 19901113 9. 6824 1. 68 ComedyDrama tt0106028 6 Homicide Life on the Street 19971205 9. 6 0. 13 CrimeDramaMystery tt0108968 5 Touched by an Angel 19981115 9. 6 0. 08 DramaFamilyFantasy tt0903747 5 Breaking Bad 20130220 9. 554 18. 95 CrimeDramaThriller tt0944947 6 Game of Thrones 20160525 9. 4943 15. 18 ActionAdventureDrama tt3398228 5 BoJack Horseman 20180914 9. 4738 0. 45 AnimationComedyDrama tt0103352 3 Are You Afraid of the Dark 19940223 9. 4349 2. 6 DramaFamilyFantasy tt0944947 4 Game of Thrones 20140509 9. 4282 11. 07 ActionAdventureDrama tt0976014 4 Greek 20110307 9. 4 0. 01 ComedyDrama tt0090466 4 L. A. Law 19900405 9. 4 0. 1 Drama We can see that Parenthood from 1990 has the highest rating on IMDb who knew Saving Our Operations By default Miller only prints your processed data to the console. If we want to save it to another CSV file we can use the operator. If we wanted to save our sorted data to a new CSV file this is what the command would look like mlr csv sort nr avrating. tvratings. csv sorted. csv Convert CSV To JSON Most of the time you dont use CSV data directly in your application. You convert it to a format that is easier to read or doesnt require additional dependencies like JSON. Miller gives you the c2j option to convert your data from CSV to JSON. Heres how to do this for our sorted data mlr c2j sort nr avrating. tvratings. csv sorted. json Case study Top 5 Athletes With Highest Number Of Medals In Rio 2016 Lets apply everything we learned above to a realworld use case. Lets say that you have a detailed dataset of every athlete who participated in the 2016 Olympic games in Rio and you want to know who the 5 with the highest number of medals are. First download the athlete data as a CSV then save it in a file named athletes. csv. Lets open up the following file mlr c2p head. athletes. csv The resulting output will be something like the following id name nationality sex dateofbirth height weight sport gold silver bronze info 736041664 A Jesus Garcia ESP male 19691017 1. 72 64 athletics 0 0 0 532037425 A Lam Shin KOR female 19860923 1. 68 56 fencing 0 0 0 435962603 Aaron Brown CAN male 19920527 1. 98 79 athletics 0 0 1 521041435 Aaron Cook MDA male 19910102 1. 83 80 taekwondo 0 0 0 33922579 Aaron Gate NZL male 19901126 1. 81 71 cycling 0 0 0 173071782 Aaron Royle AUS male 19900126 1. 80 67 triathlon 0 0 0 266237702 Aaron Russell USA male 19930604 2. 05 98 volleyball 0 0 1 382571888 Aaron Younger AUS male 19910925 1. 93 100 aquatics 0 0 0 87689776 Aauri Lorena Bokesa ESP female 19881214 1. 80 62 athletics 0 0 0 Optional Clean Up The File The CSV file has a few fields we dont need. Lets clean it up by removing the info id weight and dateofbirth columns. mlr csv I cut x f idinfoweightdateofbirth athletes. csv Now we can move to our original problem we want to find who won the highest number of medals. We have how many of each medal bronze silver and gold the athletes won but not the total number of medals per athlete. Lets compute a new value called medals which corresponds to this total number bronze silver and gold added together. mlr c2p put medalsbronzesilvergold then head. athletes. csv It gives you the following output name nationality sex height sport gold silver bronze medals A Jesus Garcia ESP male 1. 72 athletics 0 0 0 0 A Lam Shin KOR female 1. 68 fencing 0 0 0 0 Aaron Brown CAN male 1. 98 athletics 0 0 1 1 Aaron Cook MDA male 1. 83 taekwondo 0 0 0 0 Aaron Gate NZL male 1. 81 cycling 0 0 0 0 Aaron Royle AUS male 1. 80 triathlon 0 0 0 0 Aaron Russell USA male 2. 05 volleyball 0 0 1 1 Aaron Younger AUS male 1. 93 aquatics 0 0 0 0 Aauri Lorena Bokesa ESP female 1. 80 athletics 0 0 0 0 Ababel Yeshaneh ETH female 1. 65 athletics 0 0 0 0 Sort by the highest number of medals by adding a sort. mlr c2p put medalsbronzesilvergold then sort nr medals then head. athletes. csv Respectively the resulting output will be the following name nationality sex height sport gold silver bronze medals Michael Phelps USA male 1. 94 aquatics 5 1 0 6 Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 Madeline Dirado USA female 1. 76 aquatics 2 1 1 4 Nathan Adrian USA male 1. 99 aquatics 2 0 2 4 Penny Oleksiak CAN female 1. 86 aquatics 1 1 2 4 Simone Manuel USA female 1. 78 aquatics 2 2 0 4 Alexandra Raisman USA female 1. 58 gymnastics 1 2 0 3 Restrict to the top 5 by adding n 5 to your head operation. mlr c2p put medalsbronzesilvergold then sort nr medals then head n 5. athletes. csv You will end up with the following file name nationality sex height sport gold silver bronze medals Michael Phelps USA male 1. 94 aquatics 5 1 0 6 Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 As a final step lets convert this into a JSON file with the c2j option. Here is our final command mlr c2j put medalsbronzesilvergold then sort nr medals then head n 5. athletes. csv top5. json With a single command weve computed new data sorted the result truncated it and converted it to JSON. name Michael Phelps nationality USA sex male height 1. 94 weight 90 sport aquatics gold 5 silver 1 bronze 0 medals 6 Other entries omitted for brevity. Bonus If you wanted to show the top 5 women you could add a filter. mlr c2p put medalsbronzesilvergold then sort nr medals then filter sex female then head n 5. athletes. csv Respectively you would end up with the following output name nationality sex height sport gold silver bronze medals Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 Madeline Dirado USA female 1. 76 aquatics 2 1 1 4 Conclusion I hope this article showed you how versatile Miller is and gave you a taste of the power of commandline tools. Feel free to scourge the internet for the best CLI next time you find yourself writing yet another random script. Resources Miller Dataset of IMDb ratings for American TV dramas Further Reading on Smashing Magazine Powerful Terminal And CommandLine CLI Tools For Modern Web Development How Should Designers Learn To Code The Terminal And Text Editors Part 1 How To Develop An Interactive Command Line Application Using Node. js A Deep Dive Into Serverless UI With TypeScript,2022-12-27T11:00:00Z
3,Mapping Python to LLVM,Modernizing high-performance computing.,https://blog.exaloop.io/python-llvm/,Firstly thank you to everyone who engaged with tried out or gave feedback on Codon over the past few weeks This is the first in a series of blog posts that describe how Codon works internally various design decisions performance considerations and so on. At a high level the Codon compiler works in these steps Parse source code into an abstract syntax tree AST. Perform type checking on the AST using a modified HindleyMilnerlike algorithm. Convert the typed AST to an intermediate representation called CIR. Perform numerous analyses transformations and optimizations on the CIR. Convert from CIR to LLVM IR. Run the LLVM optimization pipeline among other things. This post will mainly focus on the LLVM IR generation and how we map various Python constructs to LLVM IR. Sometimes this is very straightforward while other times it can be very tricky to get right as well see below. This post assumes a basic familiarity with LLVM and code generation. See Eli Benderskys excellent blog posts for an informal introduction or refresher or this great tutorial. Getting started Python types to LLVM types One of the first things we need to consider is how to convert from Python types to LLVM IR types. Some of the conversions are quite easy int can become an LLVM i64 note that this deviates from Pythons arbitrary width integers but it turns out 64 bits are more than enough for most applications float can become an LLVM double bool can become an LLVM i8 we could use i1 in theory but i8 is compatible with CC What about tuples We can convert tuple types to structs containing the tuples element types. For example the type of 42 3. 14 True becomes the LLVM structure type i64 double i8. Since tuples are immutable we can pass these structs by value and in many cases tuples can be completely optimized out by LLVMs optimization passes. What about userdefined classes We can actually do something very similar to what we did for tuples except instead of passing by value we dynamically allocate and pass by pointer which allows mutations to be handled correctly. For example consider class C a int b float c bool When we instantiate C under the hood well dynamically allocate memory to store the contents of the same i64 double i8 and return a pointer to that memory as the result of instantiation after calling C. init. There are a few other LLVM types that we expose through Codon like PtrT to represent a pointer to an object of type T. Most of the other Python types though can be constructed from these building blocks. For example the builtin collection types like list dict and set are all implemented within Codon as classes some other builtin types are implemented as named tuples and so on. Operators Now that we know how to do type conversions we need to think about how to actually do operations on those types. For example how do we add two ints as in 2 2 Python implements various operators like etc. through its magic method interface. For example a b typically becomes a. addb a b becomes a. subb and so on. It turns out we can do the same in Codon and in fact all operators are defined by magic methods within Codons standard library. For example list. add might look like class List... def addself other result copyself result. extendother return result Magic methods are convenient because they allow all operators to be implemented in standard library code rather than having some be hardcoded in the compiler. But you may ask what about operators of primitive types like int. add Thats where Codons inline LLVM IR functionality comes into play primitive operators are all implemented using inline LLVM IR via the llvm decorator class int... llvm def addself other int int tmp add i64 self other ret i64 tmp llvm def addself other float float tmp1 sitofp i64 self to double tmp2 fadd double tmp1 other ret double tmp2 Note that Codon supports method overloading the compiler will choose the correct add based on the righthand sides type. Now actually figuring out which magic methods need to be called for a given expression is a complex task in itself and is handled by the type checker which well discuss in detail in a future blog post. For now well assume we have all of this information at our disposal which is indeed the case by the time were generating LLVM IR. Control flow At this point we more or less know how to generate code for expressions so the natural next step is higherlevel control flow like if while for etc. Unlike most expressions control flow constructs require us to generate multiple basic blocksblocks of straightline code without any branches themselves linked together by branches or conditional branches. The basic block is a fundamental concept in LLVM and compilers in general and acts as the substrate for LLVM IR. The compiler itself maintains a pointer to the current basic block B. Different control flow constructs require us to generate a number of basic blocks and link them together in various ways. Lets look at an example if condition truebranch else falsebranch Compilation would roughly proceed as follows remember we always generate code in the current basic block B Create four new basic blocks Bmathrmcond Bmathrmtrue Bmathrmfalse and Bmathrmexit. Generate a branch to Bmathrmcond. Set B gets Bmathrmcond and generate code C for condition then generate a conditional branch to Bmathrmtrue and Bmathrmfalse based on C. Set B gets Bmathrmtrue and generate code for truebranch then add a branch to Bmathrmexit. Set B gets Bmathrmfalse and generate code for falsebranch then add a branch to Bmathrmexit. Set B gets Bmathrmexit. As a diagram graph TD B0B 0 Bcond BcondB cond C is true Btrue Bcond C is false Bfalse BtrueB true BexitB exit BfalseB false Bexit Lets look at one other example Heres how compilation would go Create three new basic blocks Bmathrmcond Bmathrmbody and Bmathrmexit Generate a branch to Bmathrmcond. Set B gets Bmathrmcond and generate code C for condition then generate a conditional branch to Bmathrmbody and Bmathrmexit based on C. Set B gets Bmathrmbody and generate code for body then add a branch to Bmathrmcond. Set B gets Bmathrmexit. Again as a diagram graph TD B0B 0 Bcond BcondB cond C is true Bbody Bcond C is false BexitB exit BbodyB body Bcond What about break and continue These turn out to be very straightforward when thinking in terms of basic blocks break simply becomes a branch to Bmathrmexit and continue becomes a branch to Bmathrmcond. Other control flow constructs like elif work in an analogous way and in fact can be constructed using just if else and while. There is one particular statement that requires special care however as well see next. try except finally While most of Pythons control flow constructs are relatively easy to compile try except finally is quite possibly one of the most difficult and nuanced things to get right when it comes to mapping Python to LLVM IR on a number of different levels. Exception handling itself is actually not too difficult as we can use the Itanium C ABI for zerocost exceptions along with LLVMs exception handling instructions landingpad resume invoke and appropriate personality function. We wont go into too much detail since this is all very similar to what C does. Just know that there is some additional bookkeeping required for knowing when were inside a try block which requires us to call functions with the LLVM invoke instruction rather than the usual call and to specify a basic block to branch to if an exception occurs i. e. the basic block corresponding to except. The real difficulty with try except finally comes from the finally. It turns out finally has certain semantics that many programmers might not even be aware of. For example take a look at this code def foo try return 1 finally return 2 What does foo return If youre not familiar with finally semantics you might be inclined to say 1 but the correct answer is 2 finally blocks are always executed which makes sense as they will typically be performing some cleanup action that shouldnt be skipped. This has some important implications for compilation namely branches always need to be aware of enclosing. Lets take another concrete example remember when I said try finally blocks break and continue are straightforward That turns out to not be entirely true in the presence of finally def bar for i in range10 printi try if i 5 break finally continue What does bar print Well when i 5 well reach the break statement but the otherwise the break needs to actually branch to the finally block finally block would we skipped over. Now the finally itself has a continue which overrides the previous break and resumes the loop. So in the end all the integers 0 9 are printed. To actually generate correct code for try except finally we need a couple different ingredients Firstly we need to maintain a stack of enclosing try except finally blocks since if we reach a return break or continue we need to know whether we really need to branch to some finally block. Next we need to set up a state machine for each series of nested try except finally blocks since once we do reach the finally we need to know how we got there in order to determine what action we need to take next. For instance if we got there via a return we need to actually execute that return statement at the end of the block or perhaps we got there by catching an exception that needs to be delegated to a parent try except finally. At the end of the day we need to construct a state machine with the following states NORMAL We reached the finally through normal execution of the code. Nothing special needs to be done we can just branch to the next block normally. THROWN Weve thrown an exception and are executing the finally before propagating the exception out of the function. The exception object itself will be stored in a predefined place that we can access from the finally block. CAUGHT Weve caught an exception and are reaching the finally through some except block. Again nothing special needs to be done here we can just branch to the next block normally. RETURN Weve reached the finally after encountering an enclosed return statement. After executing the finally we need to execute the return. The return value itself will be stored in a predefined place that we can access from the finally block. BREAK Weve reached the finally after encountering a break. We need to actually execute the break after executing the finally block. CONTINUE Weve reached the finally after encountering a continue. We need to actually execute the continue after executing the finally block. Importantly these actions are recursive so when we say execute the that may entail branching to return after executing the finally block another enclosing finally block and repeating the same action. Lets look at a real example of what this state machine looks like. Heres a function def baz for i in range5 try try if i 3 return i finally if i 2 break finally if i 4 continue return i And here are the various transitions between the states for different conditions throughout the loop stateDiagramv2 NORMAL NORMAL RETURN i 3 NORMAL BREAK i 3 NORMAL CONTINUE i 2 RETURN BREAK i 2 RETURN CONTINUE i 2 RETURN otherwise BREAK CONTINUE i 4 BREAK otherwise CONTINUE NORMAL The internal state machine will transition between the states based on these conditions until the loop terminates signified by reaching the end state. Variables and functions LLVM IR uses static single assignment form or SSA which effectively means LLVM IR variables must be assigned exactly once. As a result we cant map Python variables directly to LLVM IR variables. Instead we map each Python variable to a stackallocated piece of memory becomes x alloca i64 align 8 store i64 42 i64 x align 8 alloca is an LLVM IR instruction that allocates space on the current stack frame alloca i64 allocates space for a 64bit integer. Treating variables this way is standard practice when compiling to LLVM IR and CC compilers will do the same e. g. long x 42 produces this exact code with Clang. Notice also that alloca actually returns a pointer so when we dereference a variable by e. g. using x in a later expression the compiler inserts an implicit dereference of the underlying alloca pointer. In C we can avoid this dereference via the addressof operator as in x Codon actually supports this via the ptr builtin ptrx returns the address of x. This is often useful when interfacing with C functions that take a pointer as an argument. Many variables are implicitly introduced by the parser andor type checker. For example a common Python idiom for swapping the values of two variables will implicitly be transformed into tmp b a a tmp0 b tmp1 thus introducing the new variable tmp. Well discuss these types of frontend transformations in much more detail in a subsequent blog post. As for functions we can actually map Python functions directly to LLVM functions def fooa int b int return a b becomes define i64 fooi64 0 i64 1 a alloca i64 align 8 b alloca i64 align 8 store i64 0 i64 a align 8 store i64 1 i64 b align 8 a0 load i64 i64 a align 8 b0 load i64 i64 b align 8 ans add nsw i64 a0 b0 ret i64 ans Notice that we allocate space for the arguments via alloca since they should be treated like normal variables inside the function. The only complication to this otherwise fairly straightforward conversion is generators which well talk about next. Generators Generators are one of the hallmarks of the Python language and are used extensively in the standard library. For example many of the builtin functions operate on or return generators maplambda x x 1 1 2 3 generator giving 2 3 4 reversedabc generator giving c b a zipabc 1 2 3 generator giving a 1 b 2 c 3 Generators can be defined by using yield def squaresn for i in range1 n 1 yield i i listsquares5 gives all the square numbers up to 5 2 1 4 9 16 25. Since generators are so fundamental in Python we need a way to handle them efficiently in LLVM. Fortunately LLVM supports coroutines which we can use as the foundation for implementing generators. Note that C20 also adds support for coroutines. You can learn about all the intricacies of LLVM coroutines in the docs. Briefly though coroutines are like functions that allow suspension and resumption much like what happens with Pythons yield. Coroutines maintain their state i. e. local variables position in the function yielded value in whats called a coroutine frame. Coroutines in LLVM are indeed also like normal functions but delineate their resumesuspend points with special intrinsics and return a handle to their coroutine frames. Here are some of the more important LLVM intrinsics well use when generating code for coroutines llvm. coro. id Gives us a token that can identify the coroutine which well pass to many of the other intrinsics. llvm. coro. size. i64 Tells us the size of the coroutine frame for dynamic allocation. llvm. coro. begin Gives us a handle to the coroutine frame. llvm. coro. suspend Marks a suspension point. llvm. coro. end Marks the end of the coroutine and destroys the coroutine frame. These intrinsics are all used within the coroutine function itself but how do we actually call and manipulate the coroutine externally Well there are intrinsics for that as well llvm. coro. resume Resumes a coroutine given a coroutine handle. llvm. coro. done Checks if a coroutine is at its final suspend point. llvm. coro. promise Returns a pointer to the coroutine promise a region of memory that stores values yielded from the coroutine. llvm. coro. destroy Destroys a finished coroutine. With these primitives in hand we can implement all we need to support generators Functions with yield will be converted to LLVM coroutines. We can generate code for yield statements by storing the yielded value in the coroutine promise then calling llvm. coro. suspend. We can implement Pythons next builtin by calling llvm. coro. done to see if the given generator is finished then calling llvm. coro. resume to resume it and finally llvm. coro. promise to obtain the generated value. We can implement for x in generator by repeatedly calling llvm. coro. resume llvm. coro. promise until llvm. coro. done tells us to stop. Lets look at an example for i in range3 printi Heres the simplified LLVM IR well generate for this snippet entry g call i8 rangei64 3 br label for for call void llvm. coro. resumei8 g done call i1 llvm. coro. donei8 g br i1 done label exit label body body p1 call i8 llvm. coro. promisei8 g i32 8 i1 false p2 bitcast i8 p1 to i64 i load i64 i64 p2 call void printi32 i br label for exit call void llvm. coro. destroyi8 g Just to recap The call to range gives us a handle with type i8 to the range generator. We resume the generator with llvm. coro. resume note that all coroutines will be initially suspended to match Pythons generator semantics. If llvm. coro. done tells us the generator is done we exit the loop and call llvm. coro. destroy to destroy it. Otherwise in the body of the loop we obtain the next value for i by calling llvm. coro. promise the other arguments are simply the alignment of the promise i32 8 and whether we want to obtain a promise given a handle or vice versa i1 false. At this point you might be asking yourself isnt that a lot of code to just loop over 0 1 2 and print each value Thats where LLVMs coroutine optimization passes come in it turns out coroutines are particularly amenable to compiler optimizations that can typically eliminate all of the coroutine overhead entirely. Specifically by far the most common case when dealing with generators in Python is for them to be created and destroyed in the same function in which case the coroutine passes can inline them elide heap allocation for the frame and so on. Heres the result of running the coroutine passes on our example above call void printi64 0 call void printi64 1 call void printi64 2 We were able to get rid of everything coroutinerelated By default LLVMs coroutine passes rely on an analysis of llvm. coro. destroy calls. Codons LLVM fork instead relies on an escape analysis of the coroutine handle which we found to be much better at determining which coroutines can be optimized. Program structure We have most of the key ingredients at this point but what about the overarching program structure Python doesnt have an explicit main function as an entry point like C does. LLVM on the other hand does so we need to reconcile that difference. The way we handle this in Codon is by putting everything at the top level into its own implicit function becomes a 0 def main global a a 42 printa 2 This also allows us to do some setup and initialization as well such as initializing the runtimeGC setting up sys. argv etc. This approach usually works fairly seamlessly but there is one subtle case that needs special care exporting Codon functions in a shared library. For example consider the following def bar int... some complex operation M bar def foon int int return n M and assume we want to generate a shared library libfoo. so from Codon that exposes the foo function. If we do so naively the global variable M will actually not be initialized we need to run the implicit main we created in order to initialize M. We solve this by running main as a global constructor if were compiling to a shared object. Optimizations Now is a good time to talk about optimizations. LLVM has an extensive repertoire of IR passes for doing various things. These include many of the standard compiler optimizations like constant folding dead code elimination loop invariant code motion etc. These passes are grouped into standard pipelines like the O0 pipeline or the O3 pipeline which correspond to the O0 and O3 flags on a CC compiler like Clang. Codon uses these default optimization pipelines but also adds a few of its own LLVM passes. One example is AllocationRemover which 1 removes unused heap GC allocations and 2 demotes small heap allocations to the stack i. e. to an alloca instruction. This turns out to be useful when instantiating classes for example creating an instance of a class C that never escapes its enclosing function can actually be done purely on the stack def foo c Chello codegend as dynamic allocation demoted to alloca... C never escapes foo The same goes for the builtin collection types like lists or dictionaries a 1 2 3 printa0 a1 a2 optimized to 6 no allocations take place Codon uses a couple other custom LLVM passes but the vast majority of its custom optimizations are performed in its own IR which well discuss in a future post. Examples To conclude this post I want to showcase a few neat examples that illustrate many of these pieces coming together combined with LLVMs optimization passes. Ive simplified the naming in the LLVM IR outputs for clarity. Userdefined generator with loop def squaresn for i in range1 n 1 yield i2 x 0 for s in squares1000 x s printx This gets optimized away entirely and the final answer 333833500 is printed directly answer call i64 i8 inttostri64 333833500 Tail recursion elimination from sys import argv N intargv1 def collatzn if n 1 return 0 else return 1 collatz3n 1 if n 2 else n 2 printcollatzN The recursion is eliminated and replaced with a loop collatzentry tmp1 icmp eq i64 N 1 br i1 tmp1 label collatzexit label if. false if. false tmp2 phi i64 tmp7 ternary. exit N collatzentry accumulator phi i64 tmp8 ternary. exit 0 collatzentry tmp3 and i64 tmp2 1 not1 icmp eq i64 tmp3 0 br i1 not1 label ternary. false label ternary. true ternary. true tmp4 mul i64 tmp2 3 tmp5 add i64 tmp4 1 br label ternary. exit ternary. false tmp6 sdiv i64 tmp2 2 br label ternary. exit ternary. exit tmp7 phi i64 tmp5 ternary. true tmp6 ternary. false tmp8 add i64 accumulator 1 not2 icmp eq i64 tmp7 1 br i1 not2 label collatzexit label if. false collatzexit answer phi i64 0 collatzentry tmp8 ternary. exit List of tuples from sys import argv from random import random from math import hypot Random points in quadrant points random random for in range10000000 How many points are insideoutside of quartercircle inside outside 0 0 for x y in points if hypotx y 1 inside 1 else outside 1 Estimate pi using the ratio pi 4 inside inside outside printpi Tuples are eliminated and the inside and outside variables are converted to LLVM IR SSA variables for. body idx phi i64 idxadd for. body 0 for. cleanup inside phi i64 insideadd for. body 0 for. cleanup outside phi i64 outsideadd for. body 0 for. cleanup p1 getelementptr double double ptr points i64 idx x load double ptr p1 align 8 p2 getelementptr double double ptr points i64 idx i32 1 y load double ptr p2 align 8 h tail call double hypotdouble x double y tmp1 fcmp uge double h 1. 000000e00 tmp2 zext i1 tmp1 to i64 outsideadd add i64 outside tmp2 tmp3 xor i1 tmp1 true tmp4 zext i1 tmp3 to i64 insideadd add i64 inside tmp4 idxadd add nuw nsw i64 idx 1 exitcond icmp eq i64 idx N br i1 exitcond label for. exit label for. body Conclusion I hope this post helped shed some light on the various methods we can employ to compile Python code to LLVM. You can view the LLVM IR output of your own code with codon build llvm remember to add the release flag to enable optimizations. There are many things we took for granted here like how we determine the data types to begin with or how we put the source code in a format thats suitable for code generation. These among other things will be topics of future posts in this series. Stay tuned,2023-01-09T16:52:56Z
4,Pure Python Distributed SQL Engine,Open source SQL engine in Python. Contribute to marsupialtail/quokka development by creating an account on GitHub.,https://github.com/marsupialtail/quokka/blob/master/blog/why.md,Why I wrote a SQL engine in only Python Ive worked on Quokka for a year now. Before I started I was on leave from Stanford working on a startup writing assembly speeding up machine learning primitives like matrix multiplications and decision trees. After a while I realized though I could perhaps make a living doing the above what most customers want sped up is not ML model inferencetraining but data pipelines. After all most ML in industry today seems to be lightweight models applied to heavily engineered features not GPT3 on Common Crawl. Virtually all feature engineering today is done with SQLDataFrames with a prized library of userdefinedfunctions UDFs that encode business logic. Think fraud detection search recommendation personalization pipelines. In model training materializing those features is often the bottleneck especially if the actual model used is not a neural network. People now either use a managed feature platform like Tecton Feathr. ai or roll their own pipelines with SparkSQL. With robust UDF support SparkSQL seems to be the defacto standard for these feature engineering workloads and is used under the hood by virually every managed feature platform. Unless you are on GCP in which case BigQuery is also a strong contender. Of course these problems only happen when you have big data 100GB cant use Pandas and need to use a distributed framework like SparkSQL. Having lost all the money I made from my startup on shitcoins and the stock market I returned to my PhD program to build a better distributed query engine Quokka to speed up those feature engineering workloads. When I set out I had several objectives Easy to install and run especially for distributed deployments. Good support for Python UDFswhich might involve numpy sklearn or even Pytorch. At least 2x SparkSQL performance. Otherwise what are we even doing here. Fault tolerant. This is perhaps not important for small scale frameworks but its critical for TBscale jobs that run over many hours that are a painintheass to restart. SparkSQL can recover from worker failures due to spot instance preemptions Quokka should be as well. The first two objectives strongly scream Python as the language of choice for Quokka. PySpark supports Python UDFs reasonably well but there are a myriad of inconveniences that arise from its Java background you have to sudo install all required Python packages on EMR Python UDF error messages will not get displayed properly no finegrained control of intraUDF parallelism etc. While each issue seems minor on its own these footguns are extremely annoying for Pythonnative data scientists like me whose last experience with Java was AP Computer Science. I had pushback on this I know major tech players who maintain UDF libraries in ScalaJava and senior engineers who claim Java is not so bad and all serious engineers should know it anyways. My argument I got a CS degree at MIT without writing a single line of Java. I know many who did the same. I want to empower data scientists without a formal CS education and its unlikely their first language of choice is Java based on the number of tutorial videos available on YouTube. Ever wondered why Tensorflow4j exists Do you even want to learn how to use it instead of just writing PyTorch But how do you build a distributed engine on top of Python After all Python is not known for its distributed prowess... Until Ray came about. Not going to waste space here describing how amazing it is but its basically Akka in Python that actually works. It allows you to easily instantiate a custom Python class object on a remote worker machine and call its functions which is pretty much all you need to build a distributed query engine. Ray also lets you easily spin up remote clusters with a few lines of code and manage arbitrary Python dependencies programmatically which easily satisfied my first two objectives. Well now what about performance Python is so reputed for being slow there are memes for it. However Pythons slowness actually works in its favor since its so slow people have built amazing opensource libraries for it in C or Rust that speeds up common operations like numpy Pandas or Polars If you use those libraries as much as possible then your code can actually be extremely performant e. g. if you implement a data analytics workflow using only columnar Pandas APIs it will beat a handcoded Java or even C program almost any day. Specifically for a distributed query engine you want A library to parse and optimize SQL the one and only SQLGlot. Its a SQL parser optimizer planner and transpiler and can even execute simple SQL queries in pure Python. Very fast kernels for SQL primitives like joins filtering and aggregations. Quokka uses Polars to implement these. I sponsor Polars on Github and you should too. I am also exploring DuckDB but I have found Polars to be faster so far. Fast reading writing decoding encoding CSVs and Parquet files. Quokka uses Apache Arrow which is probably the fastest opensource option and will become even faster when my PR gets merged A tool to send data quickly from one worker to another without having to serialize. Rays object store provides a builtin solution but Quokka opts for the higherperformance Arrow Flight. In database parlance Quokka adopts a pipelined approach where multiple processing stages can happen at once similar to Presto or Snowflake. For example if your workload involves reading from two data sources in S3 and joining them Quokka can execute the download decode and join all at once. This offers higher performance than SparkSQLs model where stages execute one at a time Spark would download the data sources first and only after they are fully buffered in memory or on disk would it execute the join stage. Of course the pipelined execution model leads to complications with fault handling. In Spark fault tolerance is pretty simple if a stage fails it just gets reexecuted. Only one stage could fail at a time. In Quokka multiple stages can fail at once and its hard to see how you can recover from failures without just restarting the whole damned thing. Efficient fault handling in pipelined query execution is actually the main academic innovation of Quokka and how I hope to earn my PhD so its perhaps best to save it for another post. But a sneak peak similar to how Kafka writes control information to Zookeper or how Kubernetes manages control plane with etcd Quokka maintains a consistent global state in Redis. This allows Quokka to reason about failures and recover from them using dynamically tracked lineage. OK enough talk Whats the state of Quokka right now and can I use it Yes of course. Quokka currently supports a DataFramelike API documented here. It should work on local machine no problem and should be a lot faster than Pandas. Distributed setup is a bit more involved and only supports EC2 right now. SQL support is in the works and currently passes half of the TPCH benchmark. For a look at how to implement these queries in the DataFrame API check here. Quokkas performance is similar to Trino who is not fault tolerant at the moment on these queries for Parquet and a lot faster than everybody else if the input is in CSV format. Quokka is under active development. In the new year I hope to add a lot more functionality related to feature engineering i. e. range joins PIT joins window functions etc. as well as improve upon the SQL execution engines performance potentially using SIMD kernels. I will also add some more data sources like the Twitter API Ethereum API JSON files and probably JDBC. Finally I plan to add support for connecting to an existing RayKubernetes cluster and GCPAzure. Quokkas core execution engine is only 1000 lines of code. It is meant to be simple and easily extensible. I welcome any contributions on new data source readers or executors If you think Quokkas cool please join the Discord raise a Github issue or shoot me an email zihengwstanford. edu.,2022-12-30T20:08:29Z
5,Underappreciated challenges with Python packaging,"pypackaging-native is a collection of content about key Python packaging topics and issues for projects using native code - with a focus in particular scientific, data science and ML/AI projects in the PyData ecosystem.",https://pypackaging-native.github.io/,Home Introduction Packaging is an important and timeconsuming part of authoring and maintaining Python packages. This is particularly true for projects that are not pure Python but contain code that needs to be compiled and have to deal with distributing compiled extensions and with build dependencies. Many projects in the PyData ecosystem which includes scientific computing data science and MLAI projects fall into that category. This site aims to provide an overview of the most important Python packaging issues for such projects with indepth explanations and references. The content on this site is meant to provide insights and good reference material. This will hopefully provide common ground when discussing potential solutions for those problems or design changes in Python packaging as a whole or in individual packaging tools. The content is divided into meta topics and key issues. Meta topics are mainly descriptions of aspects of Python packaging that are more or less inherent to the whole design of it and consequences and limitations that follow from that. Key issues are more specific pain points felt by projects with native code. Key issues may also be more tractable to devise solutions or workarounds for. How are these topics chosen and ranked The initial list of topics was constructed by soliciting input from 25 people who together are a representative subset of stakeholders maintainers of widely used PyData projects like NumPy scikitlearn Apache Arrow CuPy Matplotlib SciPy H5py Jupyter Hub and Spyder maintainers of package repositories package managers and build systems Pip PyPI Conda Condaforge Spack Nix pypabuild Meson and numpy. distutils engineers from hardware vendors like Intel and NVIDIA engineers responsible for deploying software for HPC users educators and organisers of user groups WiMLDS SciPy Lectures Data Umbrella Adding new topics and making changes to existing content on this site happens through community input on GitHub. Meta topics Build package management concepts and terminology The multiple purposes of PyPI PyPIs authorled social model and its limitations Lack of a build farm for PyPI Expectations that projects provide ever more wheels Key issues Native dependencies This is by some distance the most important issue. Several types of native dependencies are discussed in detail Depending on packages for which an ABI matters Packaging projects with GPU code Metadata handling on PyPI Distributing a package containing SIMD code Unsuspecting users getting failing from source builds Contributing All contributions are very welcome and appreciated Ways to contribute include Improving existing content on the website extending or clarifying descriptions adding relevant references diagrams etc. Providing feedback on existing content Proposing new topics for inclusion on the website and writing the content for them... and anything else you consider useful The content for this website is maintained on GitHub. Acknowledgements Initial development of this website was sponsored by Intel Initial development effort was led by Quansight Labs Created December 20 2022,2023-01-03T19:45:06Z
6,Why I'm still using Python,16 years and counting...,https://mostlypython.substack.com/p/why-im-still-using-python,Why Im still using Python 16 years and counting... Ive been using Python since 2006 and every year I ask myself if its still the right language for me. I dont want to get stuck using a language just because its the one Ive become comfortable using. Languages are constantly evolving and if theres a language that better suits my needs Ill happily invest the time needed to make a change. In the end though Python is still right for me for the same reasons it was right in 2006 it lets me get the work done that I want to do enjoyably and efficiently. Theres also the added bonus of getting to be part of one of the best communities Ive ever been involved in. I grew up in the 70s and 80s and my father was a software engineer at the time. We had a kit computer in our basement before most people had even thought of having a computer at home. I learned the fundamentals of programming when I was nine or ten years old the first programs I wrote were in BASIC. Over the next twenty years I dabbled in a variety of languages LOGO Pascal C Fortran Perl Javascript Java and PHP. I was a hobbyist programmer and I enjoyed learning a new language every few years. In 2006 I was working on a larger for me project in Java and a friend told me I should check out Python. Your programs will do the same things theyll just be one third as long as they were in Java. That was a bold claim but as I looked at a thousandline file it seemed like a pretty good idea to find out if he was right. Rewriting that first project in Python was magical. As I reimplemented sections of the project I watched my files grow shorter and they looked cleaner as well. Id always enjoyed programming but writing Python felt different. Ideas that were newer at the time such as semantic whitespace and not needing to declare variable types went from strange new patterns to ideas that made perfect sense in retrospect. My files looked consistent and wellstructured and they were much easier to read review and debug. Also they were just plain fun to write. When the project was finished the files were in fact less than half the length of the corresponding Java files. My programming efforts shifted from hobbyist to professional over the next ten years and as my projects grew more significant Python continued to serve me well. The code got out of my way much more than it had in the other languages Id been using. I was still doing programming work but I found myself spending more of my time thinking about the realworld problems I cared to solve and less time thinking about syntax and languagespecific constructs. I went to my first Python conference in 2012. I was intimidated about going because I was a teacher first and a programmer second and I assumed everyone there would be a professional programmer. When I showed up I found an entirely welcoming community. Half the people there were clearly better programmers than Id ever be because its what they focused on. But half the people there were just like me they had realworld problems they wanted to solve and they were finding that Python could help them work more effectively and more efficiently. My life got better the moment I stepped into the Python community and its been one of the best parts of my life ever since. Im still interested in other languages my innate curiosity about programming will always be there. But work life and parenting life doesnt leave me as much time for exploratory learning as I used to have. I want to learn Go Rust a functional language like Haskell and others as well but I dont have a compelling reason to spend significant time on those languages at this point. Im sure I will at some point but for now I have every reason to stick with Python for most of my work. There are aspects of aging that I dont enjoy but I deeply appreciate the decadeslong perspective I have on programming languages and the role of technology in society overall. Its been fascinating to see the development from lowerlevel languages to higherlevel languages over the course of half a lifetime. Most criticisms I see leveled at Python are still completely unfounded. Many times the criticism can be addressed by using the language in a different way. Python isnt a perfect fit for all problem domains. There are some areas where most experienced Python programmers would recognize its not the best fit. So be it if Im not working in one of those areas then Python is still probably the best fit for me. I used to hear that Python wasnt the best at any one thing but it was second best at most things. I agreed with that line of reasoning for a long time but these days Python is as good as any of its peers for many things and its still quite effective in many areas where it might not objectively be the best fit. Heading into 2023 I couldnt be more excited to continue using Python. I hope you are as well.,2022-12-30T16:06:02Z
7,"Hackaday Podcast 200: Happy New Year, the Ultimate Game Boy, and Python All the Things","This week, Editor-in-Chief Elliot Williams and Managing Editor Tom Nardi ring in the New Year with…well, pretty much the same stuff they do every other week. After taking some time to talk ab…",https://hackaday.com/2023/01/06/hackaday-podcast-200-happy-new-year-the-ultimate-game-boy-and-python-all-the-things/,This week EditorinChief Elliot Williams and Managing Editor Tom Nardi ring in the New Year withwell pretty much the same stuff they do every other week. After taking some time to talk about the nuts and bolts of the podcast in honor of Episode 200 discussion moves on to favorite stories of the week including an impeccably cloned Dyson lamp one hackers yearslong quest to build the ultimate Game Boy developing hardware in Python building a breadboard computer with the 6502s simplified sibling and the latest developments surrounding the NABU settop box turned retrocomputer. The episode wraps up with a review of some of the biggest themes we saw in 2022 and how theyre likely to shape the tech world in the coming years. Check out the links below if you want to follow along and as always tell us what you think about this episode in the comments Download it in living MP3. Episode 200 Show Notes Whats that Sound Think you know this weeks sound Fill out the form for a chance to win a Hackaday Podcast tshirt. Interesting Hacks of the Week DIYson Lamp Hides Cables Between The Seams An Epic Quest To Build The Ultimate Game Boy The Whole Thing In Python Squeezing A Minimalist 6502 Retrocomputer Onto A Single Breadboard A Homebrew SMD Vise Built From Scrap Wood NABU PC Gets CPU Upgrade Emulates A TRS80 Quick Hacks Elliots Picks Toms Picks,2023-01-06T17:00:33Z
8,Writing a Python SQL engine from scratch,Python SQL Parser and Transpiler. Contribute to tobymao/sqlglot development by creating an account on GitHub.,https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md,Writing a Python SQL engine from scratch Introduction When I first started writing SQLGlot in early 2021 my goal was just to translate SQL queries from SparkSQL to Presto and vice versa. However over the last year and a half Ive ended up with a fullfledged SQL engine. SQLGlot can now parse and transpile between 18 SQL dialects and can execute all 24 TPCH SQL queries. The parser and engine are all written from scratch using Python. This post will cover why I went through the effort of creating a Python SQL engine and how a simple query goes from a string to actually transforming data. The following steps are briefly summarized Why I started working on SQLGlot because of my work on the experimentation and metrics platform at Netflix where I built tools that allowed data scientists to define and compute SQLbased metrics. Netflix relied on multiple engines to query data Spark Presto and Druid so my team built the metrics platform around PyPika a Python SQL query builder. This way definitions could be reused across multiple engines. However it became quickly apparent that writing python code to programmatically generate SQL was challenging for data scientists especially those with academic backgrounds since they were mostly familiar with R and SQL. At the time the only Python SQL parser was sqlparse which is not actually a parser but a tokenizer so having users write raw SQL into the platform wasnt really an option. Some time later I randomly stumbled across Crafting Interpreters and realized that I could use it as a guide towards creating my own SQL parsertranspiler. Why did I do this Isnt a Python SQL engine going to be extremely slow The main reason why I ended up building a SQL engine was... just for entertainment. Its been fun learning about all the things required to actually run a SQL query and seeing it actually work is extremely rewarding. Before SQLGlot I had zero experience with lexers parsers or compilers. In terms of practical use cases I planned to use the Python SQL engine for unit testing SQL pipelines. Big data pipelines are tough to test because many of the engines are not open source and cannot be run locally. With SQLGlot you can take a SQL query targeting a warehouse such as Snowflake and seamlessly run it in CI on mock Python data. Its easy to mock data and create arbitrary UDFs because everything is just Python. Although the implementation is slow and unsuitable for large amounts of data 1 million rows theres very little overheadstartup and you can run queries on test data in a couple of milliseconds. Finally the components that have been built to support execution can be used as a foundation for a faster engine. Im inspired by what Apache Calcite has done for the JVM world. Even though Python is commonly used for data there hasnt been a Calcite for Python. So you could say that SQLGlot aims to be that framework. For example it wouldnt take much work to replace the Python execution engine with numpypandasarrow to become a respectablyperforming query engine. The implementation would be able to leverage the parser optimizer and logical planner only needing to implement physical execution. There is a lot of work in the Python ecosystem around high performance vectorized computation which I think could benefit from a pure Pythonbased ASTplan. Parsing and planning doesnt have to be fast when the bottleneck of running queries is processing terabytes of data. So having a Pythonbased ecosystem around SQL is beneficial given the ease of development in Python despite not having bare metal performance. Parts of SQLGlots toolkit are being used today by the following Ibis A Python library that provides a lightweight universal interface for data wrangling. Uses the Python SQL expression builder and leverages the optimizerplanner to convert SQL into dataframe operations. mysqlmimic PurePython implementation of the MySQL server wire protocol Parses transforms SQL and executes INFORMATIONSCHEMA queries. Quokka Pushbased vectorized query engine Parse and optimizes SQL. Splink Fast accurate and scalable probabilistic data linkage using your choice of SQL backend. Transpiles queries. How There are many steps involved with actually running a simple query like SELECT bar. a b 1 AS b FROM bar JOIN baz ON bar. a baz. a WHERE bar. a 1 In this post Ill walk through all the steps SQLGlot takes to run this query over Python objects. Tokenizing The first step is to convert the sql string into a list of tokens. SQLGlots tokenizer is quite simple and can be found here. In a while loop it checks each character and either appends the character to the current token or makes a new token. Running the SQLGlot tokenizer shows the output. Each keyword has been converted to a SQLGlot Token object. Each token has some metadata associated with it like linecolumn information for error messages. Comments are also a part of the token so that comments can be preserved. Parsing Once a SQL statement is tokenized we dont need to worry about white space and other formatting so its easier to work with. We can now convert the list of tokens into an AST. The SQLGlot parser is a handwritten recursive descent parser. Similar to the tokenizer it consumes the tokens sequentially but it instead uses a recursive algorithm. The tokens are converted into a single AST node that presents the SQL query. The SQLGlot parser was designed to support various dialects so it contains many options for overriding parsing functionality. The AST is a generic representation of a given SQL query. Each dialect can override or implement its own generator which can convert an AST object into syntaticallycorrect SQL. Optimizing Once we have our AST we can transform it into an equivalent query that produces the same results more efficiently. When optimizing queries most engines first convert the AST into a logical plan and then optimize the plan. However I chose to optimize the AST directly for the following reasons Its easier to debug and validate the optimizations when the input and output are both SQL. Rules can be applied a la carte to transform SQL into a more desirable form. I wanted a way to generate canonical sql. Having a canonical representation of SQL is useful for understanding if two queries are semantically equivalent e. g. SELECT 1 1and SELECT 2. Ive yet to find another engine that takes this approach but Im quite happy with this decision. The optimizer currently does not perform any physical optimizations such as join reordering. Those are left to the execution layer as additional statistics and information could become relevant. The optimizer currently has 17 rules. Each of these rules is applied transforming the AST in place. The combination of these rules creates canonical sql that can then be more easily converted into a logical plan and executed. Some example rules are qualifytables and qualifycolumns Adds all dbcatalog qualifiers to tables and forces an alias. Ensure each column is unambiguous and expand stars. SELECT FROM x SELECT db. x AS x simplify Boolean and math simplification. Check out all the test cases. NOT FALSE AND x x AND TRUE OR 1 3 x x 1 1 2 normalize Attempts to convert all predicates into conjunctive normal form. DNF A AND B OR B AND C AND D CNF A OR C AND A OR D AND B unnestsubqueries Converts subqueries in predicates into joins. The subquery can be converted into a left join SELECT FROM x AS x WHERE SELECT y. a AS a FROM y AS y WHERE x. a y. a 1 SELECT FROM x AS x LEFT JOIN SELECT y. a AS a FROM y AS y WHERE TRUE GROUP BY y. a AS u0 ON x. a u0. a WHERE u0. a 1 AND NOT u0. a IS NULL pushdownpredicates Push down filters into the innermost query. SELECT FROM SELECT FROM x AS x AS y WHERE y. a 1 SELECT FROM SELECT FROM x AS x WHERE y. a 1 AS y WHERE TRUE annotatetypes Infer all types throughout the AST given schema information and function type definitions. Planning After the SQL AST has been optimized its much easier to convert into a logical plan. The AST is traversed and converted into a DAG consisting of one of five steps. The different steps are Scan Selects columns from a table applies projections and finally filters the table. Sort Sorts a table for order by expressions. Set Applies the operators unionunion allexceptintersect. Aggregate Applies an aggregationgroup by. Join Joins multiple tables together. The logical plan is quite simple and contains the information required to convert it into a physical plan execution. Executing Finally we can actually execute the SQL query. The Python engine is not fast but its very small 400 LOC It iterates the DAG with a queue and runs each step passing each intermediary table to the next step. In order to keep things simple it evaluates expressions with eval. Because SQLGlot was built primarily to be a transpiler it was simple to create a Python SQL dialect. So a SQL expression x 1 can just be converted into scopex 1. Whats next SQLGlots main focus will always be on parsingtranspiling but I plan to continue development on the execution engine. Id like to pass TPCDS. If someone doesnt beat me to it I may even take a stab at writing a PandasArrow execution engine. Im hoping that over time SQLGlot will spark the Python SQL ecosystem just like Calcite has for Java. Special thanks SQLGlot would not be what it is without its core contributors. In particular the execution engine would not exist without Barak Alon and George Sittas. Get in touch If youd like to chat more about SQLGlot please join my Slack Channel,2023-01-03T16:35:29Z
9,Delphi VCL and FMX Libraries for Python,"Delphi VCL & FMX Libraries for Python .emb-events event-411 h3 not(.event-date), .hide-on-featured What Are The VCL and FMX Libraries? VCL and FireMonkey(FMX) are mature GUI libraries used by thousands of Delphi and C++Builder developers around",https://www.embarcadero.com/delphi-libraries-for-python,What Are The VCL and FMX Libraries VCL and FireMonkeyFMX are mature GUI libraries used by thousands of Delphi and CBuilder developers around the world. What Are The DelphiVCL and DelphiFMX Libraries for Python The DelphiVCL and DelphiFMX libraries for Python are a set of Python modules that put the robust and mature VCL and FireMonkey FMX GUI libraries in the hands of Python developers. The libraries are built with the same opensource Python4Delphi library that powers the popular PyScripter Python IDE. DelphiVCL for Python is focused on native Windows development and employs the Windowsonly VCL framework while DelphiFMX for Python employs the crossplatform FireMonkey framework and brings a powerful flexible GUI framework to Windows Linux macOS and Android. Do I need to know Delphi to use the DelphiVCL and DelphiFMX libraries for Python No. Although knowledge of Delphi is a big advantage when working with these libraries using the VCL and FMX libraries for Python development does not require previous knowledge of Delphi. Where To Get The Libraries DelphiVCL Python Module The Visual Component Library VCL is a key part of Delphis stateoftheart user interface support. It contains most native Windows controls and controls with additional features and functionality. DelphiFMX Python Module FireMonkey FMX is Delphis crossplatform GUI library. It takes advantage of GPU hardware acceleration using OpenGL or DirectX to create slick modern and highperformance user interfaces. The DelphiFMX Python module supports Windows macOS Linux and Android development with Python. Both modules are freely available via GitHub or the PIP Python Package Manager,2023-01-08T06:39:52Z
10,"Control Pandas, Polars, or SQL from One DSL",Codd method-chained SQL generator and Pandas data processing in Python. - GitHub - WinVector/data_algebra: Codd method-chained SQL generator and Pandas data processing in Python.,https://github.com/WinVector/data_algebra,dataalgebra dataalgebra is a piped data wrangling system based on Codds relational algebra and experience working with data manipulation languages at scale. The primary purpose of the package is to support an easy to compose and maintain grammar of data processing steps that in turn can be used to generate database specific SQL. The package also implements the same transforms for Pandas DataFrames. The package is available on PyPi and can be installed with pip install dataalgebra. A good introduction can be found here and many worked examples are here. A catalog of expression methods is found here. The pydoc documentation is here. And the README is a good place to check for news or updates. Currently the system is primarily adapted and testing for Pandas Polars Google BigQuery PostgreSQL SQLite and Spark. Porting and extension is designed to be easy. This is to be the Python equivalent of the R packages rquery rqdatatable and cdata. This package supplies piped Coddtransform style notation that can perform data engineering in Pandas or still in development Polars and generate SQL queries from the same specification. Installing Install dataalgebra with pip install dataalgebra Announcement This article introduces the dataalgebra project a data processing tool family available in R and Python. These tools are designed to transform data either inmemory or on remote databases. For an example with video of using dataalgebra to rearrange data layout please see here. The key question is what operators or major steps are supported by the data algebra and what methods operations on columns are supported. The operators are documented here and which methods can be used in which contexts is linsted here. Also please check the README for news. In particular we will discuss the Python implementation also called dataalgebra and its relation to the mature R implementations rquery and rqdatatable. Introduction The project intent is to realize a method chained data processing language based on Codds relational operators that is easy to maintain has helpful tooling and has very similar realizations or dialects for SQLdatabases accessed from Python useful working at scale with PostgreSQLor Apache Spark Sparkexample here. Pandas DataFrameobjects in Python. SQLdatabases access from Rimplementation is here and is mature and ready for production use. The intent is the notation should look idiomatic in each language. Working in Python should feel like working in Python and working in R should feel like working in R. The data semantics however are designed to be close to the SQL realizations given the close connection of SQL to the relational algebra in particular row numbering starts at 1 and row and column order is not preserved except at roworder steps or selectcolumns steps respectively. The intent is it should be very easy to use the system in either Python or R a boon to multilanguage data science projects and it is easy to port either code or experience from one system to another a boon for porting projects or for data scientists working with more than one code base or computer language. Related work includes Codds relational algebra SQL data. table dfply dplython LINQ Apache Calcite dplyr dtplyr table. express Pandas pandasply Polars SQLAlchemy rquery cdata siuba tidypolars Preql The dataalgebra principles include Writing data transforms as a pipeline or methodchain of many simple transform steps. Treating data transform pipelines or directed acyclic graphs DAGs as themselves being sharable data. Being able to use the same transform specification many places in memory on databases in R in Python. The dataalgebra supplies two primary services Building composite data processing pipelines which we demonstrate in this note. Building record transforms which we demonstrate here. Example Lets start with a pipeline example in Python for a record transform example please see here. For our example we will assume we have a data set of how many points different subjects score in a psychological survey. The goal is transform the data so that we see what fraction of the subjects answers are in each category subject to an exponential transform as often used in logistic regression. We then treat the persubject renormalized data as a probability or diagnosis. The exact meaning of such a scoring method are not the topic of this note. It is a notional example to show a nontrivial data transformation need. In particular having to normalize persubject divide some set of scores persubject by a persubject total is a classic pain point in dataprocessing. In classic SQL this can only be done by joining against a summary table or in more modern SQL with a window function. We want to show by working in small enough steps this can be done simply. Set up Lets start our Python example. First we import the packages we are going to use and set a few options. import polars as pl import dataalgebra as da import dataalgebra. BigQuery da. version 1. 5. 1 Now lets type in our example data. Notice this is an inmemory Polars Data. Frame. dlocal pl. DataFrame subjectID1 1 2 2 surveyCategory withdrawal behavior positive reframing withdrawal behavior positive reframing assessmentTotal 5. 2. 3. 4. irrelevantCol1 irrel14 irrelevantCol2 irrel24 dlocal Lets also copy this data to a database. Normally big data is already in the system one wants to work with so the copying over is just to simulate the data already being there. dbhandle dataalgebra. BigQuery. examplehandle printdbhandle BigQueryDBHandledbmodelBigQueryModel conngoogle. cloud. bigquery. client. Client object at 0x7fb1c0cad270 remotetabledescription dbhandle. inserttable dlocal. topandas tablenamed allowoverwriteTrue remotetabledescription. head Normally one does not read data back from a database but instead materializes results in the database with SQL commands such as CREATE TABLE tablename AS SELECT.... Also note case in columns is a bit of nightmare. It is often best to lowercase them all. Back to the dataalgebra Now we continue our example by importing the dataalgebra components we need. Now we use the dataalgebra to define our processing pipeline ops. We are writing this pipeline using a method chaining notation. This notation will look very much like a pipe to R magrittr users. scale 0. 237 ops da. descrddlocal. extendprobability fassessmentTotal scale. exp. extendtotal probability. sum partitionbysubjectID. extendprobability probability total. extendrownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability. selectrowsrownumber 1. selectcolumnssubjectID surveyCategory probability. renamecolumnsdiagnosis surveyCategory We are deliberately writing a longer pipeline of simple steps so we can use the same pipeline locally with Pandas or Polars and potentially great scale with PostgreSQL or Apache Spark. A more concise variation of this pipeline can be found in the R example here. The intent is the user can build up very sophisticated processing pipelines using a small number of primitive steps. The pipelines tend to be long but can still be very efficient as they are well suited for use with Polars and with SQL query optimizers. Most of the heavy lifting is performed by the very powerful window functions triggered by use of partitionby and orderby available on the extend step. Multiple statements can be combined into extend steps but only when they have the same windowstructure and dont create and use the same value name in the same statement except for replacement which is shown in this example. Many conditions are checked and enforced during pipeline construction making debugging very easy. For a more Pythonic way of writing the same pipeline we can show how the code would have been formatted by black. pysource ops. topythonprettyTrue printpysource TableDescription tablenamed columnnames subjectID surveyCategory assessmentTotal irrelevantCol1 irrelevantCol2. extendprobability assessmentTotal 0. 237. exp. extendtotal probability. sum partitionbysubjectID. extendprobability probability total. extend rownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability. selectrowsrownumber 1. selectcolumnssubjectID surveyCategory probability. renamecolumnsdiagnosis surveyCategory In either case the pipeline is read as a sequence of operations top to bottom and left to right. What it is saying is We start with a table named d that is known to have columns subjectID surveyCategory assessmentTotal irrelevantCol1 and irrelevantCol2. We produce a new table by transforming this table through a sequence of extend operations which add new columns. The first extendcomputes probability expscaleassessmentTotal this is similar to the inverselink step of a logistic regression. We assume when writing this pipeline we were given this math as a requirement. The next few extendsteps total the probabilitypersubject this is controlled by the partitionbyargument and then rank the normalized probabilities persubject grouping again specified by the partitionbyargument and order controlled by the orderbyclause. The first We then select the persubject topranked rows by the selectrowsstep. And finally we clean up the results for presentation with the selectcolumns renamecolumns and orderrowssteps. The names of these methods are intended to evoke what they do. The point is each step is deliberately so trivial one can reason about it. However the many steps in sequence do quite a lot. SQL Once we have the ops object we can do quite a lot with it. We have already exhibited the prettyprinting of the pipeline. Next we demonstrate translating the operator pipeline into SQL. sql dbhandle. tosqlops printsql dataalgebra SQL httpsgithub. comWinVectordataalgebra dialect BigQueryModel 1. 5. 1 string quote identifier quote WITH tablereference0 AS SELECT subjectID surveyCategory assessmentTotal FROM dataalgebratest. test1. d extend1 AS SELECT. extend probability assessmentTotal 0. 237. exp subjectID surveyCategory EXPassessmentTotal 0. 237 AS probability FROM tablereference0 extend2 AS SELECT. extend total probability. sum partitionbysubjectID subjectID surveyCategory probability SUMprobability OVER PARTITION BY subjectID AS total FROM extend1 extend3 AS SELECT. extend probability probability total subjectID surveyCategory probability total AS probability FROM extend2 extend4 AS SELECT. extend rownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability subjectID surveyCategory probability SUM1 OVER PARTITION BY subjectID ORDER BY probability DESC AS rownumber FROM extend3 selectrows5 AS SELECT. selectrowsrownumber 1 subjectID surveyCategory probability FROM extend4 WHERE rownumber 1 SELECT. renamecolumnsdiagnosis surveyCategory surveyCategory AS diagnosis subjectID probability FROM selectrows5 Older SQL with use of with or common table expressions can be hard to read as SQL expresses composition by innernesting inside SELECT statements happen first. The operator pipeline expresses composition by sequencing or methodchaining which can be a lot more legible. In this example we use the SQL99 common table expression WITH notation to manage the composition in a more legible manner. A huge advantage of the SQL is we can send it to the database for execution as we do now. Also notice the generated SQL has applied query narrowing columns not used in the outer queries are removed from the inner queries. The irrelevant columns are not carried into the calculation as they would be with a SELECT. This early optimization comes in quite handy. dbhandle. readquerysql What comes back is one row per subject with the highest persubject diagnosis and the estimated probability. Again the math of this is outside the scope of this note think of that as something coming from a specification the ability to write such a pipeline is our actual topic. The hope is that the dataalgebra pipeline is easier to read write and maintain than the SQL query. If we wanted to change the calculation we would just add a stage to the dataalgebra pipeline and then regenerate the SQL query. Polars An advantage of the pipeline is it can also be directly used on Pandas or Polars DataFrames. Lets see how that is achieved. ops. evald dlocal There is also a shorthand notation for single table source pipelines ops. transformdlocal eval takes a dictionary of DataFrames names matching names specified in the pipeline and returns the result of applying the pipeline to this data. Currently our Pandas and Polars implementation only allows very simple window functions. This is why we didnt write probability probabilitysumprobability but instead broken the calculation into multiple steps by introducing the total column the SQL realization does in fact support more complex window functions. This is a small issue with the grammar but our feeling encourage simple steps is in fact a good thing improves debuggability and in SQL the query optimizers likely optimize the different query styles into very similar realizations anyway. Pandas The exact same pipeline can be applied directly to Pandas data frames. ops. transformdlocal. topandas ExportImport Because our operator pipeline is a Python object with no references to external objects such as the database connection it can be saved through standard methods such as pickling. Some Advantages of dataalgebra A dataalgebra operator pipeline carries around usable knowledge of the data transform. For example report all source table columns used by the query ops. columnsused d assessmentTotal subjectID surveyCategory what columns does this operation produce ops. columnnames subjectID diagnosis probability Conclusion The dataalgebra is part of a powerful crosslanguage and mutliimplementaiton family data manipulation tools. These tools can greatly reduce the development and maintenance cost of data science projects while improving the documentation of project intent. Win Vector LLC is looking for sponsors and partners to further the package. In particular if your group is using both R and Python in bigdata projects where SQL is a need including Apache Spark or are porting a project from one of these languages to another please get in touch. be neat dbhandle. close Note mysql is not fully supported as it doesnt name quoted common table expression columns in an obvious way. Current primary databases are PostgreSQL Google Big Query SparkSQL and SQLite.,2022-12-30T21:24:53Z
11,Python Malware Starting to Employ Anti-Debug Techniques,"First time anti-debug techniques are discovered in PyPI malware. Read how these techniques are implemented, including analysis and tips from JFrog Security Research.",https://jfrog.com/blog/pypi-malware-creators-are-starting-to-employ-anti-debug-techniques/,PyPI malware creators are starting to employ AntiDebug techniques The JFrog Security Research team continuously monitors popular opensource software OSS repositories with our automated tooling and reports any vulnerabilities or malicious packages discovered to repository maintainers and the wider community. Most PyPI malware today tries to avoid static detection using various techniques starting from primitive variable mangling to sophisticated code flattening and steganography techniques. Use of these techniques makes the package extremely suspicious but it does prevent novice researchers from understanding the exact operation of the malware using static analysis tools. However any dynamic analysis tool such as a malware sandbox quickly removes the malwares static protection layers and reveals the underlying logic. Recently it seems that attackers have stepped up a notch weve recently detected and disclosed the cookiezlog package which seemed to employ Antidebugging code designed to thwart dynamic analysis tools in addition to regular obfuscation tools and techniques. This is the first time our research team or any publication have spotted these kinds of defenses in PyPI malware. In this post we will give an overview of the techniques used in this Python malware and how to unpack similar malware. Installation triggers Similar to most malicious packages the cookiezlog package runs immediately upon installation. This is achieved via develop and install triggers in setup. py class PostDevelopCommanddevelop def runself execute install. runself class PostInstallCommandinstall def runself execute install. runself... setup namecookiezlog version0. 0. 1 descriptionExtra Package for Roblox grabbing... cmdclass develop PostDevelopCommand install PostInstallCommand Static Obfuscation Part 1 The trivial stuff The first and simplest layer of protection is zlibencoded code which is executed immediately after the package is installed def execute import marshalzlibexecmarshal. loadszlib. decompressbxx9cMx90xc1Jxc3x10x86xebxb5Oxb1xecx01xd9xdd4Ix93x08x84xe0Axa8xa1x1ex85x98x0c6hvxd7... The decoded payload downloads a file from a hardcoded URL and executes it on the victims machine URL httpscdn. discordapp. comattachments10377234414800896001039359352957587516Cleaner. exe response requests. getURL openCleaner. exe wb. writeresponse. content os. systemset COMPACTLAYERRunAsInvoker start Cleaner. exe The executable is a Windows PE file. Looking at the strings in the executable we can see that its not actual native code but rather a Python script packed into the PE format strings Cleaner. exe grep PyIns Cannot open PyInstaller archive from executable s or external archive s PyInstaller FormatMessageW failed. PyInstaller pyiwin32utilstoutf8 failed. It can be quickly unpacked with the opensource tool PyInstaller Extractor. The extracted code contains a lot of files primarily thirdparty libraries. The most interesting extracted file is main. pyc which contains the malware code as Python bytecode. Static Obfuscation Part 2 Unpacking PyArmor Normally we would be able to decompile the bytecode in main. pyc to Python source code using tools such as uncompyle6. However in this case another run of strings on main. pyc shows that the binary has been obfuscated with PyArmor pytransformr pyarmor Distobfmain. py PyArmor is a commercial packer and obfuscator which applies obfuscation techniques to the original code encrypts it and protects it from analysis. Fortunately for the researchers PyArmor keeps much of the information thats necessary for introspection. Knowing this we can try to restore the names of the functions and constants used in the original code. Although PyArmor does not have any publiclyavailable unpacker it can be fully unpacked with some manual effort. In this case we chose to perform a quick unpacking shortcut by using library injection since we were mostly interested in the original symbols and strings. Trying to run the packed module as a standalone script produces an error specifying that the system doesnt have the required module python. exe. main. pyc Traceback most recent call last File distobfmain. py line 3 in File line 1 in ModuleNotFoundError No module named psutil Because the module looks for the psutil module we can create a module with the same name somewhere in the PYTHONPATH and it will be executed in the context of the process. This can be used as an easy entry point for injecting our own code into the process. We created our own file named psutil. py in the same directory as the protected file main. pyc with the following code import inspect for frame in inspect. stack for c in frame. frame. fcode. coconsts if not inspect. iscodec continue dis. showcodec The snippet uses the inspect module which allows to get a runtime information about the code being executed it iterates over execution frames and extracts the names of the code blocks and referenced constants. After running our snippet it returned a list of strings that allowed us to discern the capabilities and origin of the malicious code. The most interesting strings were the URL of an injection module pointing to the possible attackers repository and references to antiVM functionalities in the code Injector appd. d httpsraw. githubusercontent. comSyntheticcinjection1maininjection. js WEBHOOK IP index. js checkvm None VMwareService. exe VMwareTray. exe AntiDebug Techniques The Syntheticc GitHub profile mentioned in the strings was still available at the time of writing. The profiles repositories contain a bunch of opensource hacking tools. Among others there was a repository called Advanced Anti Debug containing methods that could be used to prevent analysis of the malware We can split the dynamic methods the malware used into two categories AntiDebug and AntiVM. The AntiDebug checks look for suspicious system activity related to any debuggers or disassemblers and includes the following functions checkprocesses looks whether debugger process runs on the system comparing the active process list to the list of over 50 known tools including idau64. exe IDA Pro Disassembler x64dbg. exe x64dbg Debugger Windbg. exe WinDbg Debugger Devenv. exe Visual Studio IDE Processhacker. exe Process Hacker PROCNAMES ProcessHacker. exe httpdebuggerui. exe wireshark. exe fiddler. exe regedit. exe... for proc in psutil. processiter if proc. name in PROCNAMES proc. kill checkresearchtools has almost the same functionality comparing substrings of process names to a humble list of five traffic analysis tools wireshark Wireshark network protocol analyzer fiddler Fiddler proxy http HTTP Debugger and possibly more tools traffic generic term packet generic term If any of these processes are found to be running the AntiDebug code tries to kill the process via psutil. Process. kill not a very subtle approach. Malware that is more stealthconscious would just stop running without any indication instead of interacting with external processes. The other antidebug techniques try to make sure the malware is not running inside a virtual machine checkdll checks the system root directories for DLLs indicating that the system is running under a VMWare vmGuestLib. dll or VirtualBox vboxmrxnp. dll virtual machine guest. checkvm checks if any VMwarerelated processes are running specifically VMwareService. exe or VMwareTray. exe. checkregistry looks for keys used by virtual machines for example a wellknown registry key that gets added when VMWare drivers are installed HKEYLOCALMACHINESYSTEM ControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000DriverDesc def checkregistry if systemREG QUERY HKEYLOCALMACHINESYSTEMControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000DriverDesc 2 nul 1 and systemREG QUERY HKEYLOCALMACHINESYSTEMControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000ProviderName 2 nul 1exitprogramDetected Vm handle OpenKeyHKEYLOCALMACHINE SYSTEMCurrentControlSetServicesDiskEnum try if VMware in QueryValueExhandle 00 or VBOX in QueryValueExhandle 00 exitprogramDetected Vm finally CloseKeyhandle Last but not least the checkspecs function analyzes the current machine usage def checkspecs if intstrvirtualmemory0102410241024. split. 0 4 exitprogramMemory Ammount Invalid if intstrdiskusage0102410241024. split. 0 50 exitprogramStorage Ammount Invalid if intcpucount 1 exitprogramCpu Counts Invalid If there is a small amount of memory disk space or only one CPU it assumes that the process is running inside a virtual machine. All of the checks mentioned above are relatively simple but with the respectable protection against static analysis the malware already employed it offers adequate protection against novice researchers especially ones who only use automated analysis tools which wouldnt be able to breach the defenses of this specific malware. The Payload Simple Password Grabber The payload is disappointingly simple compared to the amount of defenses used by the malware but it is still harmful. The payload is a password grabber which gathers autocomplete passwords saved in the data caches of popular browsers and sends them to the C2 server in this case a Discord hook httpsdiscord. comapiwebhooks1039353898445582376cvrsu8CslmIYzNyXMpkjbkNEyO0yjg08x5Ra7mPdgooQquALPINn1YfD5CuJ11dM7h. From the strings extracted from the malware we can deduce that in addition to the industry standard Discord token leaker functionality the payload also hunts for passwords of several financial services as can be seen by strings used by the sendinfo function Name sendinfo Filename Argument count 0... Constants 0 None 1 USERPROFILE... 5 coinbase... 7 binance... 9 paypal... Summary We can once again see that malware developers constantly evolve their arsenal adding new methods of evasion and new layers of protection against analysis of their tools. Just a couple of years ago the only tools that PyPI malware authors used were simple payload encoders. Today we see that malware thats uploaded to OSS repositories is becoming more complex has a few levels of static and dynamic protection and utilize combinations of commercial and homebrew tools. This is similar to their colleagues in the world of native malware and as such we are expecting OSSrepo malware to continue to evolve perhaps with advanced techniques such as custom polymorphic encoding and deeper antidebug methods. Stay uptodate with JFrog Security Research Follow the latest discoveries and technical updates from the JFrog Security Research team in our security research blog posts and on Twitter at JFrogSecurity.,2022-12-23T07:04:29Z
12,Signed distance functions in 46 lines of Python,A walkthrough of 46 lines of code that render a 3D ASCII donut using signed distance functions.,https://vgel.me/posts/donut/,Signed distance functions in 46 lines of Python Signed distance functions are a really cool method of 3D rendering But they unfortunately have a reputation for being difficult to understand. It makes sense whythey usually get shown off in beautiful but complicated ShaderToy examples written in GLSL an unfamiliar language for most programmers 1. But at their core SDFs are a really simple idea. Im going to prove that by walking you through a program that raymarches an animated SDF donut in only 46 lines of Python. Just for fun and to make it easy to port to your favorite language that can also print strings to the terminal well also be doing it with ASCII art instead of a graphics API. So come along By the end you wont just have this deliciouslooking spinning ASCII donut but an understanding of a cool rendering technique you can use for all kinds of neat things. Setting up So to start off with lets slap down some Python to render our ASCII frame. Well also add in a game loop so we can do animation import time def samplex int y int str draw an alternating checkboard pattern if x y inttime. time 2 return else return while True loop over each position and sample a character framechars for y in range20 for x in range80 framechars. appendsamplex y framechars. appendn print out a control sequence to clear the terminal then the frame on windows add import os os. system to the beginning of the program to enable ANSI escape sequences. print0332J. joinframechars cap at 30fps time. sleep130 This renders a 80x20 checkerboard which alternates every second This is a nice foundation but not too visually interesting so lets move on. The task ahead is simple for each character on the screen decide what the character should beeasy Drawing a circle Lets start with something simple. We have an x and a y coordinate so we can easily draw a circle if not anything 3D yet. Theres a few different ways we could go about this but in the spirit of for each character on the screen decide what character it should be well do a characterbycharacter approach. The basic algorithm will be for each x y coordinate of a character Calculate the distance of x yfrom the center of the screen x02 y02 aka x2y2 Subtract the desired circle radius. That way if the point is inside or on the edge of the circle the value will be 0 and otherwise it will be 0. Now test that value against 0 to either return if the point is inside or on the edge of the circle or otherwise. Well also remap x and y to 1.. 1 and. 5... 5 respectively to gesture in the direction of resolution independence and to keep the aspect ratio correct 2 2080 0. 5 as y covers only 20 characters while x covers 80 and terminal characters are roughly twice as tall as they are wide. This prevents the circle from looking instead like an unappetizingly squished bean. import math time def circlex float y float float since the range of x is 1.. 1 the circles radius will be 40 meaning the circles diameter is 40 of the screen radius 0. 4 calculate the distance from the center of the screen and subtract the radius so d will be 0 inside the circle 0 on the edge and 0 outside return math. sqrtx2 y2 radius def samplex float y float str return a if were inside the circle and otherwise if circlex y 0 return else return while True framechars for y in range20 for x in range80 remap to 1.. 1 range for x... remappedx x 80 2 1... and corrected for aspect ratio range for y remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 Hey thats a circle if Ive ever seen one We didnt use time. time in this one so the circle isnt animatedwell bring animation back later I promise. A 2D donut A circle is just half a 2D donutthe hole is just another circle if you think about it. 2 So lets add the other circle to make a 2D donut Theres a few different ways to do this see if you can figure out a way on your own but a nice way is to define it as a radius thickness around that radius import math time def donut2dx float y float float same radius as before though the donut will appear larger as half the thickness is outside this radius radius 0. 4 how thick the donut will be thickness 0. 3 take the abs of the circle calculation from before subtracting thickness 2. abs... will be 0 on the edge of the circle and increase as you move away. therefore abs... thickness 2 will be 0 only thickness 2 units away from the circles edge on either side giving a donut with a total width of thickness return absmath. sqrtx2 y2 radius thickness 2 def samplex float y float str if donut2dx y 0 return else return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 This representation radius thickness is artistically nice because radius and thickness are relatively independent parameters they can be changed with little reference to each other. 3 But its also nice for our code since its only a slight tweak from how we were calculating the distance before we calculated the distance from the center of the circle and now we calculate the distance from the edge of that circle. By subtracting thickness 2 from that edge distance the result will be 0 if the point is less than thickness 2 from the edge of the circle resulting in a ring of the given thickness centered on the edge of the circle with the given radius. The other cool thing is that the tweak was so small that our code was able to stay almost exactly the same. We just had to update the signed distance functionoops gave it awayand didnt have to change the rest of the rendering loop More generally regardless of what SDF we use our rendering loop can stay the samewere just sampling distances at pixels. Going 3D mode Time to enter the exciting decade of the 1990s and bring 3D graphics to the table Well step back from the complexity of the donut slightly and start by rendering a sphere which has an almost identical SDF to a circle but with a new term Z def spherex float y float z float float radius 0. 4 return math. sqrtx2 y2 z2 radius Before X was the horizontal axis and Y the vertical Z will give our image depth. 4 Well also reuse the same framechars loop as before. The only function that needs to substantially change is sample which now needs to handle the third dimension. Fundamentally we need a function that takes in an x y 2D point and samples 3D space in some way related to that point. In other words we need to come up with the correct Z to sample to get a reasonable character to render. We could cheat and simply always sample at z 0 that would render a 2D slice of our 3D world showing the inside of any object that happened to cross the z0 plane. 5 But to get a more interesting view we need to simulate the real world. Imagine an eye which is to a first approximation a 2D plane how does it see distant objects Well the sun shoots out rays which either hit the eyeplane directly not good wear sunglasses folks or bounce off one or more objects and then hit the eye. We could treat our screen the same way for each call to samplex y well shoot out millions of rays from a simulated light hoping that at least one will bounce off an object and pass through x y cameraz. That approach would be slightly slow however. The odds of any given ray hitting that specific point would be vanishingly small and most of the work would be completely wasted. You could throw all of useast1 at this python code sorry Reddit need to borrow your servers for a second but lets take a shortcut instead. In samplex y we only care about the ray of light that passes through x y cameraz. So why bother with all the other rays Well shoot the ray backwards Well start it at x y cameraz and at each step query the SDF to get the distance from the rays current point to the scene in any direction. If the distance is less than some threshold we hit the scene Otherwise we can safely march the ray forward by whatever distance is returned since we know the scene is at least that distance away in the forward direction. It may be moreimagine the ray passes close to the scene but never hits it while the ray is near the scene the distance queried from the SDF will be small so the ray will be forced to move slowly but eventually after some large number of steps it will move past the scene and start moving quickly again. Well arbitrarily limit the number of marches steps to 30 and return the background character if a ray doesnt hit something by then. With all of that this is what our new 3D sample function looks like def samplex float y float str start z far back from the scene which is centered at 0 0 0 so nothing clips z 10 well step at most 30 steps before assuming we missed the scene for step in range30 get the distance just like in 2D d spherex y z test against 0. 01 not 0 were a little more forgiving with the distance in 3D for faster convergence if d 0. 01 we hit the sphere return else didnt hit anything yet move the ray forward we can safely move forward by d without hitting anything since we know thats the distance to the scene z d we didnt hit anything after 30 steps return the background return And putting it all together heres the code for rendering a sphere import math time def spherex float y float z float float radius 0. 4 return math. sqrtx2 y2 z2 radius def samplex float y float str radius 0. 4 z 10 for step in range30 d spherex y z if d 0. 01 return else z d return this is unchanged while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 OK this isnt very impressive I admit it. If you didnt know any better youd probably accuse me of just reusing the circle image from before But I promise this is a sphere reallylets keep moving and trust for now that its 3D. A 3D donut To move towards our 3D donut next well need to replace the simple sphere SDF with the more complex torus donut SDF. The rest of the code stays the same import math time def donutx float y float z float float radius 0. 4 thickness 0. 3 first we get the distance from the center and subtract the radius just like the 2d donut. this value is the distance from the edge of the xy circle along a line drawn between x y 0 and 0 0 0 the center of the donut. xyd math. sqrtx2 y2 radius now we need to consider z which since were evaluating the donut at 0 0 0 is the distance orthogonal on the z axis to that x y 0.. 0 0 0 line. we can use these two values in the usual euclidean distance function to get the 3D version of our 2D donut distance from edge value. d math. sqrtxyd2 z2 then we subtract thickness 2 as before to get the signed distance just like in 2D. return d thickness 2 unchanged from before except for sspheredonutg def samplex float y float str z 10 for step in range30 d donutx y z if d 0. 01 return else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 As before this donut isnt very impressive but since its not symmetrical we can now add some motion to prove its 3D. A 3D donut that really spins All we need to do to make the donut spin is transform the points in sample before we evaluate the SDF def samplex float y float str... for step in range30 calculate the angle based on time to animate the donut spinning time. time 2 rotate the input coordinates which is equivalent to rotating the sdf tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz... This will rotate the donut around the y axis which is why y is unchanged. We calculate 6 persample 7 and then calculate a rotation matrix by hand because Im too lazy to import numpy real programmers dont use dependencies. Here it is incontext import math time def donutx float y float z float float radius 0. 4 thickness 0. 3 return math. sqrtmath. sqrtx2 y2 radius2 z2 thickness 2 def samplex float y float str z 10 for step in range30 time. time 2 tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz if d 0. 01 return else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 See thats definitely 3D no nits about it But since were only at 32 lines count em lets kick it up a notch and add some simple lighting and texturing with a normal vector estimator. Lighting it up and adding frosting To add lighting and a frosting texture well need to calculate normal vectors for our scene. A normal 8 vector is defined for an object at every point on that objects surface and is the vector that sticks straight out from that pointimagine the spines on a cactus or someones hair after touching a staticy balloon. Most surfaces have an analytic way to calculate the normal vector but that can get difficult when you start combining multiple SDFs into a complex scene. Plus who wants to write out a normal function for each SDF they write That sucks So we can cheat and instead estimate the normal by sampling the SDF on each axis around a target point Sdf typing. Callablefloat float float float def normalsdf Sdf x float y float z float tuplefloat float float an arbitrary small amount to offset around the point 0. 001 calculate each axis independently nx sdfx y z sdfx y z ny sdfx y z sdfx y z nz sdfx y z sdfx y z normalize the result to length 1 norm math. sqrtnx2 ny2 nz2 return nx norm ny norm nz norm To make sense of how this function works imagine the case where a component of the normal say x is 0. That means the SDF at that point is flat on the x axis so sdfx y z sdfx y z. As those values diverge the x component of the normal will grow either more positive or negative rotating it around. Its just an estimation but for rendering its usually good enough and even advanced demos will often use it. The downside is it can be quite slow since it needs to sample the SDF six times for every call As the scene SDF grows more complicated that can become a big performance drain. But for us its good enough Well calculate the normal in sample if the ray hits and use it to compute some lighting and texturing if d 0. 01 nty ntz normaldonut tx y tz islit nty 0. 15 isfrosted ntz 0. 5 if isfrosted return if islit else else return if islit else. We dont care about the x component of the normal just y and z. Well use y to calculate the lighting assuming that if a surface is facing up the normals y is close to 1 it should be lit. Well use z to show the frostingby thresholding against different values we can make the donut more or less frosted. The best way to gain an intuition for what these values mean is to play with them so try changing them around in a local copy of the code below and watch what happens import math time typing def donutx float y float z float float radius 0. 4 thickness 0. 3 return math. sqrtmath. sqrtx2 y2 radius2 z2 thickness 2 Sdf typing. Callablefloat float float float def normalsdf Sdf x float y float z float tuplefloat float float 0. 001 nx sdfx y z sdfx y z ny sdfx y z sdfx y z nz sdfx y z sdfx y z norm math. sqrtnx2 ny2 nz2 return nx norm ny norm nz norm def samplex float y float str z 10 for step in range30 time. time 2 tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz if d 0. 01 nty ntz normaldonut tx y tz islit nty 0. 15 isfrosted ntz 0. 5 if isfrosted return if islit else else return if islit else. else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 Theres our final 3D donut lit textured and spinning as promised in only 46 lines of code Thanks for sticking through and I hope this article is inspiring to try and make your own SDF creations. If youre interested in learning more about SDFs there are many resources online but none I could recommend more highly than Inigio Quilezs websitehe is an SDF master and has made some truly mindblowing art using SDFs. He also has a Youtube channel where he posts videos of his work and tutorials. I recommend this video where he makes a landscape using SDFs. Enjoy and thanks for reading to the end 1 Though a cool one thats worth learning 2 Technically a 2D donut is a circle minus a circle so a donut circle circle 0. And a donut is shaped like a 0... coincidence I think not 3 If you instead for example defined the donut as two radii where a point is filled if innerradius distance from center outerradius you permit a nonsense construction where innerradius outerradius. 4 vgel. me is a bastion of the Yup that is to say correct way of rendering. 5 This is how many 4D games work they render a 3D slice of a 4D world and then confusingly render that 3D slice to a 2D screen.... That slicing has always sort of bothered me since it allows you to see inside objects in a way a real 4D being couldnt. Ive experimented with and one day want to publish or see made a 4D game that does 4D raymarching to generate a 3D mesh and then renders that mesh so you can only see the surfaces of 4D objects not their interiors. 6 Yes I could have just written theta but I installed a polytonic Greek IME for language study and the installation process was sufficiently annoying that I intend to get as much use out of it as possible 7... which means if sample was sufficiently slow there would be an interesting warp effect. You can emulate this without making things actually slow with time. time2 xy8015 which looks really cool. Try... xy8015math. sintime. time for some even trippier visuals 8 normal in this context being based on the original meaning of the Latin word normlis rightangled from norma a carpenters square. The more common use of normal to mean regular or as it ought to be was a metaphorical extension of that older meaningsort of like the much newer phrase squared away. Its like poetry it rhymes Previous entrymmap1Tb A Rust arena allocator abusing Linux overcommit Next entry2022 A Mitigated Success,2022-12-20T22:00:30Z
13,Show HN: Self Hosted OpenStreetMap using only Sqlite3 and Python,Reverse Geocode for OpenStreetmap. Contribute to punnerud/rgcosm development by creating an account on GitHub.,https://github.com/punnerud/rgcosm,RGCosm Reverse Geocode for OpenStreetmap Locally hosted OpenStreetmap using sqlite3 for reverse geocode. So you easily can find adresses based on coordinates. Download the pbf file from httpsdownload. geofabrik. de Then use convert. py to create the database python3 convert. py You have to change norwaylatest. osm. pbf in convert. py into your filename. The norwaylatest. osm. pbf is about 1GB and the sqlite3 end up 10GB. With indexes 16GB. So dont try with the biggest areas for starting. Takes about 15minutes for the norway file. To speed up your queries it is highly recommended to add indexes. This increase the size around 50 and takes a couple of minutes to create CREATE INDEX nodes index lat ON nodes lat CREATE INDEX nodes index lon ON nodes lon Adding indexes change the search time for my Norway file from 10 to 0. 15 seconds. Changing the lookaround query can also reduce the search time at the risk that you miss an adress if the closest adress is more far away. Mac users I found this to work for installation of osmium for Python brew install cmake brew install wheel brew install osmiumtool python3 m pip install osmium Premade sqlite3 database for Norway with indexes if you just want to try it httpswww. dropbox. coms4rrhxpfzulqbvxrosm. dbdl0,2022-12-30T22:45:50Z
14,Rijndael S-box in 512 bytes of Python,"GitHub Gist: instantly share code, notes, and snippets.",https://gist.github.com/juliusgeo/969c722b2152e53e4f6bb94ca2696c7a,Author juliusgeo commented Jan 4 2023 juliusgeocommented Author juliusgeo commented Jan 4 2023 juliusgeocommented def pprintboxbox print axis 0x02x i for i in range161 print. joinaxis1 while box printaxis1. join0x02x i for i in box16 del box16 axis. pop1 pprintboxsb pprintboxib The addition of this helper code allows you to print out these boxes yielding 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0x0a 0x0b 0x0c 0x0d 0x0e 0x0f 0x00 0x63 0x7c 0x77 0x7b 0xf2 0x6b 0x6f 0xc5 0x30 0x01 0x67 0x2b 0xfe 0xd7 0xab 0x76 0x01 0xca 0x82 0xc9 0x7d 0xfa 0x59 0x47 0xf0 0xad 0xd4 0xa2 0xaf 0x9c 0xa4 0x72 0xc0 0x02 0xb7 0xfd 0x93 0x26 0x36 0x3f 0xf7 0xcc 0x34 0xa5 0xe5 0xf1 0x71 0xd8 0x31 0x15 0x03 0x04 0xc7 0x23 0xc3 0x18 0x96 0x05 0x9a 0x07 0x12 0x80 0xe2 0xeb 0x27 0xb2 0x75 0x04 0x09 0x83 0x2c 0x1a 0x1b 0x6e 0x5a 0xa0 0x52 0x3b 0xd6 0xb3 0x29 0xe3 0x2f 0x84 0x05 0x53 0xd1 0x00 0xed 0x20 0xfc 0xb1 0x5b 0x6a 0xcb 0xbe 0x39 0x4a 0x4c 0x58 0xcf 0x06 0xd0 0xef 0xaa 0xfb 0x43 0x4d 0x33 0x85 0x45 0xf9 0x02 0x7f 0x50 0x3c 0x9f 0xa8 0x07 0x51 0xa3 0x40 0x8f 0x92 0x9d 0x38 0xf5 0xbc 0xb6 0xda 0x21 0x10 0xff 0xf3 0xd2 0x08 0xcd 0x0c 0x13 0xec 0x5f 0x97 0x44 0x17 0xc4 0xa7 0x7e 0x3d 0x64 0x5d 0x19 0x73 0x09 0x60 0x81 0x4f 0xdc 0x22 0x2a 0x90 0x88 0x46 0xee 0xb8 0x14 0xde 0x5e 0x0b 0xdb 0x0a 0xe0 0x32 0x3a 0x0a 0x49 0x06 0x24 0x5c 0xc2 0xd3 0xac 0x62 0x91 0x95 0xe4 0x79 0x0b 0xe7 0xc8 0x37 0x6d 0x8d 0xd5 0x4e 0xa9 0x6c 0x56 0xf4 0xea 0x65 0x7a 0xae 0x08 0x0c 0xba 0x78 0x25 0x2e 0x1c 0xa6 0xb4 0xc6 0xe8 0xdd 0x74 0x1f 0x4b 0xbd 0x8b 0x8a 0x0d 0x70 0x3e 0xb5 0x66 0x48 0x03 0xf6 0x0e 0x61 0x35 0x57 0xb9 0x86 0xc1 0x1d 0x9e 0x0e 0xe1 0xf8 0x98 0x11 0x69 0xd9 0x8e 0x94 0x9b 0x1e 0x87 0xe9 0xce 0x55 0x28 0xdf 0x0f 0x8c 0xa1 0x89 0x0d 0xbf 0xe6 0x42 0x68 0x41 0x99 0x2d 0x0f 0xb0 0x54 0xbb 0x16 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0x0a 0x0b 0x0c 0x0d 0x0e 0x0f 0x00 0x52 0x09 0x6a 0xd5 0x30 0x36 0xa5 0x38 0xbf 0x40 0xa3 0x9e 0x81 0xf3 0xd7 0xfb 0x01 0x7c 0xe3 0x39 0x82 0x9b 0x2f 0xff 0x87 0x34 0x8e 0x43 0x44 0xc4 0xde 0xe9 0xcb 0x02 0x54 0x7b 0x94 0x32 0xa6 0xc2 0x23 0x3d 0xee 0x4c 0x95 0x0b 0x42 0xfa 0xc3 0x4e 0x03 0x08 0x2e 0xa1 0x66 0x28 0xd9 0x24 0xb2 0x76 0x5b 0xa2 0x49 0x6d 0x8b 0xd1 0x25 0x04 0x72 0xf8 0xf6 0x64 0x86 0x68 0x98 0x16 0xd4 0xa4 0x5c 0xcc 0x5d 0x65 0xb6 0x92 0x05 0x6c 0x70 0x48 0x50 0xfd 0xed 0xb9 0xda 0x5e 0x15 0x46 0x57 0xa7 0x8d 0x9d 0x84 0x06 0x90 0xd8 0xab 0x00 0x8c 0xbc 0xd3 0x0a 0xf7 0xe4 0x58 0x05 0xb8 0xb3 0x45 0x06 0x07 0xd0 0x2c 0x1e 0x8f 0xca 0x3f 0x0f 0x02 0xc1 0xaf 0xbd 0x03 0x01 0x13 0x8a 0x6b 0x08 0x3a 0x91 0x11 0x41 0x4f 0x67 0xdc 0xea 0x97 0xf2 0xcf 0xce 0xf0 0xb4 0xe6 0x73 0x09 0x96 0xac 0x74 0x22 0xe7 0xad 0x35 0x85 0xe2 0xf9 0x37 0xe8 0x1c 0x75 0xdf 0x6e 0x0a 0x47 0xf1 0x1a 0x71 0x1d 0x29 0xc5 0x89 0x6f 0xb7 0x62 0x0e 0xaa 0x18 0xbe 0x1b 0x0b 0xfc 0x56 0x3e 0x4b 0xc6 0xd2 0x79 0x20 0x9a 0xdb 0xc0 0xfe 0x78 0xcd 0x5a 0xf4 0x0c 0x1f 0xdd 0xa8 0x33 0x88 0x07 0xc7 0x31 0xb1 0x12 0x10 0x59 0x27 0x80 0xec 0x5f 0x0d 0x60 0x51 0x7f 0xa9 0x19 0xb5 0x4a 0x0d 0x2d 0xe5 0x7a 0x9f 0x93 0xc9 0x9c 0xef 0x0e 0xa0 0xe0 0x3b 0x4d 0xae 0x2a 0xf5 0xb0 0xc8 0xeb 0xbb 0x3c 0x83 0x53 0x99 0x61 0x0f 0x17 0x2b 0x04 0x7e 0xba 0x77 0xd6 0x26 0xe1 0x69 0x14 0x63 0x55 0x21 0x0c 0x7d Sign up for free to join this conversation on GitHub. Already have an account Sign in to comment,2023-01-04T16:12:29Z
15,"A former Times Square street vendor has helped 3,000 Black New Yorkers learn to code, changing their lives","We Build Black offers free programming like coding socials, tech conferences, and coding bootcamps to help tech newcomers land a job in the field.",https://www.businessinsider.com/software-engineer-devin-jackson-we-build-black-diversity-tech-2022-9,A former Times Square street vendor has helped 3000 Black New Yorkers learn to code changing their lives Streetvendorturnedsoftwareengineer Devin Jackson used free resources to break into tech. He started We Build Black to help Black New Yorkers transition into tech and land highpaying jobs. Kris Lee a network engineer at a law firm learned how to transition into a softwareengineer role for free. And he credits a community of Black tech workers in New York City with his success in making the switch. Lee stumbled upon a warehouse meeting that Devin Jackson a software engineer who taught others the basics of the coding language Python led. Today Lee works as a senior cybersecurity engineer for a software company. I experienced firsthand what having a community does for you Lee said. Those small meetings were just the start of the workforcedevelopment organization We Build Blacks efforts to help people learn how to code. It stems from Jacksons belief that free careerdevelopment programs are more effective than informative events like diversity panels at breaking down institutional barriers that keep Black people out of tech he said. His drive to enact socioeconomic change fueled his efforts to create an organization that aims to help Black people especially atrisk youth and women build community and land highpaying jobs in tech. I used to really feel annoyed about the fact that certain opportunities are not afforded to us Jackson told Insider. And I wanted to complain. But I also wanted to concretely do something. After Jackson founded We Build Black in 2017 the Brooklynbased nonprofit has evolved from a oneman operation into a volunteerrun group that has raised millions of dollars to support more than 3000 members in their career advancement Jackson said. But before starting We Build Black Jackson first broke into software engineering himself from a nontechnical background. Unlike most software engineers Jackson never went to high school or college he sold graffiti tshirts and comedy tickets on the streets of Times Square. He left the streets to attend coding workshops received scholarships to enroll in Python and dataanalysis bootcamps and studied for multiple professional ITskill certifications. He started his tech career in 2013 as an IT helpdesk intern at the consulting firm Accenture and about a year later he got his first job as a fullstack developer at a boutiquesoftware firm in Brooklyn according to LinkedIn. Today Jackson works as a technicalproduct manager at the cannabistech firm LeafLink. It was nuts he said. I was like Im never doing anything else except for this. We Build Black hosts coding meetups for Black tech workers to network and learn new skills Similar to organizations like Black Women Talk Tech Black Boys Code and Black Tech Pipeline We Build Black offers coding classes mentorships and networking events to Black workers looking to pursue tech careers. After all less than 5 of the 3. 2 million software engineers employed in the US are Black according to data from the careerbuilding platform Zippia. But Jackson said We Build Black looks to differentiate itself by taking a community approach to its programming. A member comes in with a goal for example to fix a bug or to learn how to code with Python and attendees with that skill can offer their help. Nine times out of ten you talk to somebody in our community they just say they dont have the money for that or they cant do it Jackson said. When you knock down those barriers and provide those resources theres no excuse. In the past We Build Black has hosted hackathons conferences coding competitions and even a recidivism program to help teens with criminal records learn webdevelopment skills. Due to timing and financial constraints these programs have ended indefinitely according to Jackson. The next step for We Build Black will be building out its job pipeline We Build Black is now focused on building out its workforcetraining programs Jackson said. The program chose ten Black students out of hundreds of applicants to learn userexperience design data analytics or Androidapp building through its fast track program. The students are incentivized to complete the training with perks like computers smart phones and cash prizes according to Jackson. Once they finish the course students work on a capstonecoding project for their portfolio and interview for internships at Shopify. Students that dont get the internship are matched with jobs at other tech companies Jackson said. New programs will continue to roll out. In 2023 We Build Black plans to launch Crowns and Code to match highschool seniors with mentors from Bungie a gamedeveloper firm according to Jackson. And former students like Lee have stuck around to help the organization grow through partnerships funding and other efforts. Jackson said he wants to ensure that people dont have to go through the same financial obstacles that he had to face. It sucks but you gotta keep trying to find a way to break through that door if thats the door you want to walk through he said. Jack Ma the billionaire founder of Alibaba who disappeared from public view in 2020 appears to resurface in Thailand as he prepares to give up control of his company The average ChickfilA franchise exceeds 8 million in sales and has one of the lowest fastfood franchise startup fee heres how to open one Chinas government is buying Alibaba and Tencent shares that give the Communist Party special rights over certain business decisions report says Amazon Great Republic Day Sale best deals on phones iPhone 13 OnePlus Nord 2T 5G Redmi A1 and more Wipro sees FY23 revenue growth at 11. 512 Q3 profit growth beats analyst expectations Rupee falls 4 paise to close at 81. 34 against US dollar 13th Gen Intel Core i913900KS brings unprecedented speed to desktop users Credit expansion will taper down next year if deposits growth doesnt rise Axis Bank MD,2022-12-28T15:33:42Z
16,Show HN: GPT-3 Powered Shell,Use `gpt-do` to have GPT-3 write all your bash commands for you!,https://musings.yasyf.com/never-write-a-bash-command-again-with-gpt-3/,Never write a Bash command again with the help of GPT3 I was inspired after seeing NaderLikeLadders post Using ChatGPT to make Bash palatable. He correctly pointed out that AI is finally the tool we need to win the fight against bash or fish or zsh... once and for all. While the interactive scripting experience with ChatGPT is pretty sweet my qualms normally come from the daytoday how do I do that again. Whether its rebasing while deferring to local changes or figuring out which process is locking up my TPU far too much time is spent diving into man pages. Enter gptdo With this handydandy command I never have to grep through docs again. I simply call do whatever I cant to get done in my shell and the commands magically appear. do kill the process which is locking up my GPU This command will kill the process which is locking up your GPU. kill 9 lsof t devnvidia0 Do you want to continue yN dois a reserved keyword in bashand zshso youll have to use ddoinstead. Well maybe not so magically. I figured if ChatGPT could do fullyinteractive sessions GPT3 should be able to handle oneoffs no problem. And it looks like I was right Check out this awesome demo I threw together Sometimes GPT3 doesnt quite get it right. do kill the orphaned python process These commands will list all processes and kill the orphaned python process. ps A grep python kill 9 PID Do you want to continue yN N So I added the ability to query a different model do kill the orphaned python process modelcodex This command will kill the orphaned python process. kill 9 ps aux grep python grep v grep awk print 2 Do you want to continue yN That did it With a little Python magic you can do too. Just brew install yasyfdodo or pipx install gptdo and check out the repo for more details. Youll need an OpenAI API key the default model is GPT3 and remember to use ddo if youre on bash or zsh,2022-12-23T04:34:35Z
17,A not so unfortunate sharp edge in Pipenv,"I’ve been a proponent of pipenv for several years now, particularly for application development (rather than library development). While the features around virtual environment management and the integration with pyenv to automatically install the version of …",https://0xda.de/blog/2022/12/a-not-so-unfortunate-sharp-edge-in-pipenv/,Ive been a proponent of pipenv for several years now particularly for application development rather than library development. While the features around virtual environment management and the integration with pyenv to automatically install the version of python necessary for an application are nice the features that Ive really advocated for are the separation of direct dependencies and transient dependencies via Pipfile and Pipfile. lock and the hash validation provided by Pipfile. lock. I find it helpful in improving the deterministic nature of builds not solving mind you but improving making sure everyone in the engineering organization is using the same versions of packages as everyone else. Its also a minor reassurance against supply chain attacks which is sort of what I want to write about today. When you install a package with pipenv install Django for instance pipenv will automatically add Django to your Pipfile as a direct dependency and then add Djangos dependencies as transient dependencies in Pipfile. lock. Say I install Django today and the latest version is 4. 1. 4 and then tomorrow Django releases 4. 1. 5. Pipfile. lock ensures that when my coworkers run pipenv sync or when our Dockerfile does they get 4. 1. 4 the version that I originally installed. Of course we can update this automatically with pipenv update but for the most part it is easy to install the same versions of the same packages I have installed. But because Pipfile. lock also contains the hashes of the distribution files for Django4. 1. 4 if someone were to try to publish new distribution files for Django4. 1. 4 then pipenv sync would fail because the hashes on pypi are not the same as the hashes your Pipfile. lock is expecting. Therein lies a small problem though. While Django was just an example using a popular package Im going to switch gears to a real life scenario. Real world example pythoncrontab Weve got an application and that application relies on a package called pythoncrontab. From its own description on PyPI Crontab module for reading and writing crontab files and accessing the system cron automatically and simply using a direct API. We installed pythoncrontab back in late October of 2021 and we installed the latest version at the time 2. 6. 0. pipenv install pythoncrontab gives us an entry in our Pipfile. lock that looks like this along with some dependencies not shown pythoncrontab hashes sha2561e35ed7a3cdc3100545b43e196d34754e6551e7f95e4caebbe0e1c0ca41c2f1b version 2. 6. 0 Fast forward to today our build is breaking and we arent really sure why. We automatically update dependencies every Monday morning but after that happened the build was still working just fine. We also happened to update to the latest version of pipenv yesterday but that still passed through the build fine. Something else had to have happened. Someone on our team attempted to rebuild our container environment with no cache and managed to get this output error from our pipenv sync step. Installing dependencies from Pipfile. lock 2cdb99... ERROR THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions please update the hashes. Otherwise examine the package contents carefully someone may have tampered with them. pythoncrontab2. 6. 0 from httpsfiles. pythonhosted. orgpackages8a65ee4f4db956d14b42aa6cf0dbd0b77217a206484b99f1d4aa11326cd3952apythoncrontab2. 6. 0py3noneany. whl from r tmppipenvuqhotzzrequirementspipenv2hctl0klhashedreqs. txt line 183 Expected sha256 1e35ed7a3cdc3100545b43e196d34754e6551e7f95e4caebbe0e1c0ca41c2f1b Got f308a64b8b1d072da4a235e9320398a242e92d080c1d8143bd0c600b24e160f8 Installing initially failed dependencies... ERROR Disabling PEP 517 processing is invalid project specifies a build backend of setuptools. buildmeta in pyproject. toml If we install the same version of pythoncrontab today into a new virtual environment with pipenv install pythoncrontab2. 6. 0 then we get this entry pythoncrontab hashes sha2561e35ed7a3cdc3100545b43e196d34754e6551e7f95e4caebbe0e1c0ca41c2f1b sha256f308a64b8b1d072da4a235e9320398a242e92d080c1d8143bd0c600b24e160f8 index pypi version 2. 6. 0 Investigating Interesting. It looks like the pythoncrontab wheel changed. Did my annoying insistence on using pipenv to help us combat supply chain attacks finally yield some fruit Is this what vindication feels like Well sorta. The hash that pipenv was expecting actually corresponds to the hash for the sdist from the initial release October 19th 2021 the. tar. gz file. But a wheel is available now as of December 19th 2022 which apparently wasnt available before. This caused the hash validation to fail which caused our build to fail which caused us to get a hundred messages deep in a slack thread trying to understand what had happened. I pulled down the source for pythoncrontab2. 6. 0. tar. gz and the source for the new wheel pythoncrontab2. 6. 0py3noneany. whl. I unzipped the wheel and then I went through each of the files in the wheel and diffd them against the files available from the sdist. No differences in any of the files in the wheel compared to the versions that we were running from the sdist for the past year. We can dig deeper and look at the issue tracker for pythoncrontab where we see an issue created yesterday asking for python wheels to be published to PyPI. This corresponds to when the new wheel showed up. We can also see a ticket opened earlier today complaining about the same thing we noticed in our build system the hash changed for version 2. 6. 0. Overall this is pipenv working exactly as it should. This could have very easily been a wheel containing different code than what wed previously been installing and pipenv tried its hardest to make sure we investigated that before running the new code. If this had been an actual supply chain attack we would have avoided deploying the malicious code into production. Hooray And thankfully it was just a developer trying to be helpful by providing a prebuilt wheel for an old package version. Just a minor pain point that those of us doing hash validation in our build pipelines had to investigate. Resolving In resolving this incident we opted for the smallest change that would not break the build. In this case we simply changed the hash in our Pipfile. lock by hand to the one pipenv was expecting. Note that we didnt add a hash we just changed the hash to the wheel hash instead of the sdist hash. pipenv accepts this fine which seems to mean that if a wheel matching your environment in this case any is present it doesnt actually care if there is an sdist hash at all or not. Which means that it is possible to but certainly not ergonomic to go through all of your dependencies and forbid pipenv from installing from source distributions provided you have a fairly tightly scoped developmentdeployment environment which is pretty neat. Perhaps Ill write a little utility to strip out sdist hashes from my Pipfile. lock files. But now to the sharp edge. Because this developer released a wheel for an old version and because we are tied to using pipenv to setup our application we can no longer do a build from scratch using any version of our repo between October 19th 2021 and December 19th 2022 without first making modifications. pipenv sync will fail for all of those commits because it will prefer the wheel over the sdist rightfully so mind you setup. py considered harmful and the wheel hash doesnt match the hash that was available for over a year of commits to our application. This will make bisecting problems difficult to say the least. As far as I can tell pipenv sync does not provide a flag that will avoid this but if Im wrong then please me pipenv sync help Usage pipenv sync OPTIONS Installs all packages specified in Pipfile. lock. Options system System pip management. env var PIPENVSYSTEM bare Minimal output. sequential Install dependencies oneatatime instead of concurrently. env var PIPENVSEQUENTIAL d dev Install both develop and default packages env var PIPENVDEV keepoutdated Keep outdated dependencies from being updated in Pipfile. lock. env var PIPENVKEEPOUTDATED pre Allow prereleases. python TEXT Specify which version of Python virtualenv should use. three two Use Python 32 when creating virtualenv. clear Clears caches pipenv pip and piptools. env var PIPENVCLEAR v verbose Verbose mode. pypimirror TEXT Specify a PyPI mirror. h help Show this message and exit. I suppose the behavior Id like to see here is that if my Pipfile. lock has hashes in it for a distribution then even if additional distributions are available pipenv sync should be allowed to continue to install from the distribution corresponding to the trusted hash. Emit a warning sure but Im not sure that it warrants failing a build because a developer added a new wheel to an old version. If the hash you have is for an sdist then youre already fine installing from that sdist with that hash. Adding a wheel is nice to have sure but were already installing with the sdist. Even if a new sdist was available for the same version of the same package such as 2. 6. 01 which I was informed about in a video by anthonywritescode as long as the old dist is still available with the same hash it should still be able to install. The existence of other distributions for a package version does not negate the validity of the build for the hashed versions I already know about. Even if those have bugs were bad releases whatever the hash about the contents is still correct Please reach out to me dadecrime. st if you have insight into a change we can make in our system to not get inconvenienced by this or if youre aware of efforts to make this change in pipenv already. Or maybe you have a different way of ensuring repeatability that youd like to share. Id love to chat about it. Just please dont try to convince me to use poetry.,2022-12-21T06:00:34Z
18,‘We don’t want to proceed to the route!’ See a giant python cross the road in Florida,"It’s not every day you see an enormous Burmese python cross the road. But your chances increase, because Florida.",https://www.miamiherald.com/news/local/community/miami-dade/homestead/article270958202.html,,2023-01-09T21:24:47Z
19,"PhotoRoom Is Hiring a Fullstack API Developer (OpenAPI, Python, React) in Paris","TL;DR  PhotoRoom is looking for their first API Engineer. You will work on a public API used by dozens of companies, leveraging PhotoRoom's technology to power their apps. Our API currently removes background from images using our award-winning algorithm an…",https://jobs.lever.co/photoroom/29260c53-a84f-4d4f-a322-3bdc8540cdb7?lever-origin=applied&lever-source%5B%5D=yc,Senior Fullstack Engineer PhotoRoom API Paris Remote Engineering Fulltime Remote About us PhotoRoom develops cuttingedge technology that empowers entrepreneurs small businesses and merchants to easily create images that sell leveraging deep learning. Our ambition power the internets commerce images. Our first consumer product the PhotoRoom app is already a leader in mobile photo editing we serve 7M users across more than 180 countries monthly and our app was awarded as the Best App of 2021 by Android and named Editors Choice by Apple. Our company is profitable and our lean team is made of worldclass experts in deep learning product and marketing with backgrounds at Apple Algolia Google or Bumble. We are a diverse team of entrepreneurs building for entrepreneurs. TLDR PhotoRoom is looking for their first API Engineer. You will work on a public API used by dozens of companies leveraging PhotoRooms technology to power their apps. Our API currently removes background from images using our awardwinning algorithm and we are about to add outstanding features leveraging Stable Diffusion. 65k100k depending on experience StockOptionsBSPCE We are flexible you can work from anywhere in Europe and come once a month in Paris fully reimbursed or come to the office more often. We offer substantial support for relocation 10k signing bonus including finding an apartment in Paris and supporting you with the visa procedure. Technology new MacBook Pro monitor keyboard etc. Socials Biannual company retreats weekly Happy Hour Game Time PhotoRoom is an international team and we work in English. We offer language lessons for those who need them English French. We can go higher for outstanding profiles and adapt to the cost of living if it is higher than in Paris. About the role You will be responsible for the design and development of our public API product photoroom. comapi used by dozens of companies to integrate PhotoRooms technology in their products. It currently removes background from images using our awardwinning algorithm and we are about to add outstanding features leveraging Stable Diffusion. You will have full ownershipon the API adding new features ensuring robustness of existing ones minimizing downtime and ensuring our documentation is topnotch You will get fast feedbackfrom API users and will iterate quickly we update the backend several times a week. You will follow the usage of the API and make decisions based on that. You will work within a small multicultural teamcomposed of 30 passionate friendly committed folks. You might thrive in this role if You have 3 yearsof experience in designing and implementing RESTAPIs used by external partners not just internal APIs You have a thorough experience with the OpenAPIstandard You have expertise in both Python ideally Django and React You are always the most pragmaticperson in the room. You favor speed of iteration over perfect always leverage frameworks and libraries to avoid reinventing the wheel. You have a hacker spiritand launched at least an APIrelated product in the past You have a strong sense of ownership. You take initiative and you are at ease to make product technical decisions. You have worked as part of a talented team and have experience in a fastgrowing startup. You are fluent in English French is not required. Hiring Process Screening call Home technical assignment followed by a review with the team can be compensated and does not take more than 3 hours Culture fit interview and meet the team Reference check Offer Support If you have a medical condition or an individual need for an adjustment to our process and you believe this may affect your ability to be at your best please let us know so we can talk about how we can best support you and make any adjustments that may be needed. Diversity Equity Inclusion and Belonging We are committed to enabling everyone to feel included and valued at the workplace. We believe both the company and its culture are strongest when composed of diverse experiences and backgrounds. Thats why We have flexible working hours We trust people to work remotely We extended the length of the parental leave All qualified applicants will receive consideration for employment without regard to age color family gender identity marital status national origin physical or mental disability sex including pregnancy sexual orientation or any other characteristic protected by applicable laws.,2023-01-08T07:00:02Z
20,2023 HackerRank Developer Skills Report,Discover which developer skills and programming languages are the most important in 2023. View the free report backed by exclusive HackerRank platform data.,https://www.hackerrank.com/research/developer-skills/2023,2023 HackerRank Developer Skills Report HackerRank Developer Skills Report Innovation never sleeps The past few years have seen plenty of uncertainty. Yet through a global pandemic geopolitical conflict supply chain woes and inflation innovation keeps pushing inexorably forward. Even as the broader economic outlook appears to be softening the demand and competition for skilled tech talent has only increased. At HackerRank weve seen this firsthand. In the last five years the number of unique candidates invited to our assessments has tripled. Today we have over 21 million developers in our community. Thats about 40 of the worlds developers using HackerRank to build their skills earn certifications and find rewarding jobs. To understand the state of developer skills heading into 2023 were taking a different approach with this years Developer Skills Report. Instead of relying on survey responses were going directly to the source our own platform data. Using this data we get an unbiased unparalleled view into the programming languages and technical skills employers are looking for and which roles and assessments drive the most engagement. We hope you enjoy our findings. Want to share some insights or need help Tweet us hackerrank or contact us on LinkedIn with comments or questions. LANGUAGE Measuring language demand Theres a lot of information and some strong opinions about what specific programming languages developers are using. But popularity doesnt necessarily translate to demand by employers. To determine demand in the workplace we explored the sum of monthly active tests mandating a specific language. Only about 1 in 5 assessments require a specific language. The remainder were left to the candidates preferences. Of the 20 or so of assessments that mandated a specific programming language we noticed a few data points of interest. Volume is heavily concentrated in the top five languages The top five languages by volume Java Python SQL C and JavaScript dominate the list. Fifthranked JavaScript has nearly four times the volume of the sixthranked Bash. For 2022 SQL has been growing at a steady rate and managed to surpass C but were unlikely to see any further placetrading among the top five in 2023. Were also unlikely to see any of the lowerranked languages break into the top five. The volume gap is simply too great to overcome. Most languages are growing but not equally In 2022 demand for most languages rose but not equally. To get a sense of where the various languages stand in relation to each other we indexed individual language growth against total market growth. In terms of growth relative to the market topranked Java is actually growing slightly slower than the market. As are C and JavaScript. Meanwhile Python and SQL demand grew. While Pythons growth may mean that it narrows the gap on Java again its unlikely to lead to any placetrading. Go and TypeScript show impressive gains Down the list two newer languages caught our attention. Go and TypeScript have been gaining popularity for years and rank 5th and 6th in the latest GitHub language rankings Q1 2022. Now were seeing that popularity translate into hiring demand. Go assessments grew by 301 and TypeScript by a whopping 392 or 282 compared to trend. Their growth may also affect the growth of larger languages. TypeScript for example is tightly linked with JavaScript. TypeScripts growth could come at the expense of JavaScript or it could serve to amplify it. Rising tide lifts most boats While Go and TypeScript are booming Swift and Ruby assessment volumes declined. For 2022 Swifts volume is only 80 of what it was in 2021. Ruby has fallen even further behind with a volume only 66 of what it posted in 2021. Developers have gradually shifted away from Ruby over the past several years and its not surprising to find it fading a bit more. Swift meanwhile never quite caught on and while its by no means dead is any language ever truly dead its demand seems limited. Developers prefer Java Python and C Mandatory language assessments reveal what languages employers are recruiting for but they only account for about 20 of all assessments. So what about the other 80 When developers have the freedom to choose their language which one do they choose For the most part they choose Java Python or C. Similar volume gaps appear when ranking by preference as by demand. The data reveals three distinct tiers. Java Python and C all claim well over 350000 developers. JavaScript C SQL and C hover closer to 100150000 developers. And then theres the of the rest of the range from the 13374 developers choosing PHP to the 1239 braving their assessments with R. Go and TypeScript climb Ruby and Swift fall R and Scala both fell in rankings in 2022 experiencing a loss of developers. Swift and Ruby also slipped in the rankings reinforcing their sluggish employer demand. Both gained developers but their growth trailed well behind the overall trend. The two brightest spots in the long tail are again Go and TypeScript. Both showed big gains in developers racking up the two highest growth rates of the entire list. Go clocked an absolute increase of 190 and TypeScript posted a doubletakeinducing 2788 gain. 2023 Language Forecast What does 2023 hold for languages Considering market uncertainty were keeping our forecast conservative and sticking to the lower end of the 95 confidence interval in our models. Overall our forecast predicts consistency through the first months of 2023. The languages that grew in 2021 and 2022 will continue to grow. Mostly. We expect to see a seasonal dip in December before they resume climbing. At this point we arent anticipating any major placetrading among the topranking languages. Java Python and SQL are likely to extend their lead in 2023 The top three languages of 2022 Java Python and SQL are poised to extend their lead over other languages in 2023 with a combination of higher growth and higher volume. C overtaken by Python in 2020 and SQL in 2022 will likely maintain consistent slightly slower growth. It wont overtake SQL but it wont be overtaken in turn. JavaScript demand has stayed comparatively steady since early 2021. Some of that may be due to TypeScript which is built on top of JavaScript. Its growing adoption may mean JavaScripts growth isnt lagging so much as it is being dispersed. Skills What skills are in demand Every year a stream of reports claim to have the answers. Maybe theyre taken from survey data. Or cobbled from job listings. Whatever the case theyre often a bit on the general side. If youre already working in tech or hiring in tech is it really all that useful to know that coding or cloud are in demand To get a sense of which technical skills employers are actively looking for we turned to our platform data and specifically to mandatory skills tests. These assessments test for specific skills which in our definition involve a combination of frameworks and more disciplinelevel skillsets. For example. NET and Data Visualization. Problem Solving and Machine Learning dominate skills demand Problem Solving is the most popular skill by demand by a healthy margin. It is also more foundational. Its about understanding data structures and algorithms so its not surprising to see it up top. Think of it more as a general competency check especially for more junior talent. Machine Learning remains the leading discipline by demand and its growth outpaces the overall market. Machine learning has been an indemand skill for several years and we dont see that changing. REST API shows sustained dramafree demand growth Indexed against market growth REST API grew by 179 outpacing any other skill in the top 5. It also posted steadier growth than Problem Solving or Machine Learning both of which have exhibited some monthtomonth volatility. While most languages and skills experienced a dip in mid2022 REST API kept on chugging. There seems to be a sustained and growing demand for developers who know how to connect all the things. Data science so hot right now Another bright point for 2022 has been data science and all data sciencerelated skills. Data Wrangling Data Visualization and Data Modeling are all recent additions to our assessments and theyve proven quite popular. While their growth needs to be taken with a grain of salt they are new after all it does indicate significant demand and their raw numbers are enough to land all of them in the top 10. We will be watching these skills closely over the next year to see whether this growth is a flash in the pan that will soon level out or something poised to increase further. Kubernetes rises Docker falls Even though theyre both vying toward the lower end of our skill demand rankings its worth noting the divergence between Kubernetes and Docker. Over the last year K8s surpassed Docker and grew at 185 of trend. Docker meanwhile lagged behind the market and barely grew at all with active tests only increasing by single digits. This is likely ongoing fallout from Dockers attempt at monetization and the growth in Kubernetes and containerization services from other providers see Amazon Kubernetes Service Google Kubernetes Service Azure Kubernetes Service etc. Developers seek out Problem Solving above all other skills combined Measuring skills based on questions from our library we find that Problem Solving remains far and away the most sought after skill. Problem Solving not only outnumbers all other skills combined its nearly double the next 14 skills beneath it. That may seem excessive but the gap has narrowed significantly from 2021 when Problem Solving outnumbered the next 14 skills by more than 3x. Foundational skills like Problem Solving will always rank highly but were seeing more specialized skills gaining ground and thats reflected in year over year growth. Employer demand for data sciencerelated skills is taking off and were seeing the same pattern among developers. Machine Learning and Data Wrangling both vaulted several ranks to take third and fourth places respectively. We can attribute some of the growth and the rapid rise up the ranks in 2022 to these skills being newer to the HackerRank ecosystem. But the volume speaks well volumes. It will be interesting to see if this growth carries into 2023. Lower down we found Docker and Kubernetes engaged in the pattern weve observed elsewhere. Both are growing but K8s is growing much faster. In 2021 Docker boasted twice as many developers while in 2022 that advantage is down to 10. We expect Kubernetes to pull ahead in 2023. 2023 Skills Forecast Looking ahead to 2023 where do we see the demand for tech skills shifting As with our language forecast were holding this forecast toward the lower end of the 95 confidence interval. At a glance prevailing trends will carry into 2023. Problem Solving Machine Learning and REST API continue to occupy the top three spots well above the skills clustered in the longer tail. And theyre poised to open the gap even further in 2023. Careful observers may notice that the fastgrowing data science skills arent represented in this forecast. Thats because theyre too new to reliably forecast. As we gain additional data were curious to see how these newer skills grow throughout 2023. STATE OF THE INDUSTRY Seeing through our growth In the last five years the number of unique candidates weve assessed has almost tripled. That doesnt mean job interviews tripled. It just means HackerRank grew. If we want to assess the broader state of tech hiring not just HackerRank we need to find a way to see through our own growth. Looking at the growth index you can see the curves flatten out in 2020 with the pandemic only to resume their growth in 2021. From January to December our 12month moving average increased 175. But HackerRank growth is still clouding the picture. Tech hiring and everything else hit headwinds in 2022 To see past HackerRanks growth and study the broader market weve indexed test invites against their trendline. This lets us take a quick temperature check where anything over 100 is performing above trend and anything under 100 is below trend. Compared to a similar index calculated from the Dow Jones Industrial Average a general pattern emerges from the pandemicrelated slowdown in 2020 to the slip back into underindexing in 2022. This underindexing doesnt mean a decline so much as it means slowing growth. With all the volatility and uncertainty in the broader global economy its difficult to predict what 2023 holds but we will continue to revisit this index. EMEA and Latin America are outpacing other regions in 2022. While theres been a pattern to the last two years it has played out slightly differently across various regions. All regions saw performance fall sharply in 2020 but the North America and APAC regions experienced a shallower drop and more rapid recovery. Latin America and EMEA both underindexed further and took longer to recover. EMEA didnt cross the index line until late 2021. For 2022 both regions are overindexing while North America and APAC have been tracking right along the index line. Data scientist candidates are most likely to take assessments While not a perfect metric attempt rates can be used as a loose proxy for interest or motivation. Higher assessment attempt rates indicate higher interest more engaged candidates and better targeting by the talent acquisition team. When looking by role data scientists lead with an attempt rate 8 percentage points above the average. Theyre followed by software engineer interns cloud security and cybersecurity engineers and site reliability engineers SREs. The lowest attempt rates are associated fullstack engineers data analysts and cloud engineers. Longer assessments are more popular Long assessments achieved a notably higher attempt rate than either short or regular assessments. This can seem bewildering until you consider a candidates place in the hiring process. A candidate being offered a longer assessment is probably further along and therefore more engaged and willing to take an assessment. We also found that mid and seniorlevel candidates are more likely to attempt longer duration assessments than short and regular duration tests. Test invites and attempt rates show an inverse correlation Indexing mapping data against its trendline lets us compare different data sets. Basically we can see when data points climb above or dip below their trendline. And by comparing the indexes of test invites and attempt rates were seeing what looks like an inverse relationship. When test invites overindex i. e. when the hiring market is hotter we see attempt rates underindex. And when test invites underindex we see attempt rates overindex. When there are fewer jobs or interviews candidates may pursue more opportunities. Or hiring teams may be more selective. For now well just call this correlation interesting. We want need to analyze more data over time to see if it holds. Key findings The top languages remain strong. Java Python and SQL are gaining popularity based on both employer demand and developer preference. Niche languages arent going anywhere. Innovation moves fast but languages can linger on almost indefinitely. Almost every language grew over the last year. Keep an eye on Go and TypeScript. Both languages showed substantial gains in popularity in 2022. They each grew over 300 in our monthly active tests indicating increasing demand from employers. Data science skills are seeing surging demand. Data sciencerelated skills like Machine Learning and Data Wrangling are gaining increasing interest from employers and developers alike. While many of these skills are too new to our platform to reliably forecast were watching closely over the next year to see how they continue to grow. REST API is a reliable safe choice. Ranking third among Skills REST API has exhibited steady growth since the beginning of 2021 with little of the volatility seen in other topranking skills. Candidates are more likely to engage with longer tests. Developers are nearly 7 percentage points more likely to engage with longer duration assessments than with regular tests. Overall the tech industry continues growing. Growth has slowed down in 2022 compared to 2021 but its still growth and our conservative forecast points to a recovery in 2023. Even if overall hiring slides innovation never sleeps and companies cant afford to fall behind in terms of tech talent. Nothing is pulling back. The market for developer talent is incredibly competitive and even in a bear market its unlikely to feel significant downward pressure. If your business is positioned for it now is a great time to invest in tech talent. 2023 DEVELOPER SKILLS REPORT Methodology For the 2023 Developer Skills Report we used exclusive data from the HackerRank platform to understand employer demand developer preference and candidate engagement. To estimate employer demand for specific programming languages we looked at HackerRank Work tests where a specific programming language was required or where specific library questions were asked. Skill demand was estimated by the number of HackerRank Work tests using specific library questions. On the preference or supply side we tracked submitted languages by candidates when they have multiple options available as well as their proficiency in those languages. We also included HankerRank Community practice data to get a full spectrum of skill preferences. To estimate the state of the industry we analyzed the number of unique candidates being invited to take HackerRank tests. We also looked at attempt rates as a proxy for candidate interest or engagement. In order to highlight trends protect sensitive data and compare different metrics we have used trendlines and indexing to those trendlines. Youre viewing a limited version of this report. Fill out the form to see all insights. Well also send you a PDF copy of the report via email. By submitting this form you agree to our privacy policy. Dont worry well never sell or share your data with a third party. Skills speak louder than words We help candidates sharpen their tech skills and pursue opportunities. We help companies develop the strongest tech teams around.,2023-01-01T12:34:12Z
21,Why enterprises can’t afford to overlook API security in 2023,"With API cybersecurity incidents on the rise, enterprises need a more proactive approach to protecting apps from threat actors.",https://venturebeat.com/security/api-security-2023/,Application and API security is vital for protecting modern enterprise environments. Yet most organizations are failing to implement it. According to Salt Security not only did 94 of organizations experience security problems in production APIs last year but one in five actually suffered a data breach as a result of security gaps in APIs. Wellknown organizations including Experian Peloton and most recently the FBI all suffered APIrelated breaches. In the most recent API attack on the FBI hackers gained access to a vetted database of executives called InfraGuard where members of the private sector can collaborate with the FBI to share threat data. To access InfraGuard the fraudster submitted an application for an account using the personal data of an unknown CEO. Once the FBI approved the application the hacker then used a Python script to retrieve user data through an exposed API. Event Intelligent Security Summit OnDemand Learn the critical role of AI ML in cybersecurity and industry specific case studies. Watch ondemand sessions today. The result was the exfiltration and leakage on a hacking forum of over 80000 cybersecurity and private sector stakeholders data including their names email addresses industry of employment and social media user IDs. APIs A gateway to interconnectivity and data theft This incident highlighted that while APIs play a critical role in enabling data exchange among applications microservices and services they can also provide cybercriminals with a gateway to user data if theyre left unprotected. Of course protecting this infrastructure is easier said than done given that organizations have an average of 15564 APIs to secure and a growing expertise gap. Hackers see APIs as an easy target for maninthemiddle attacks or API key and token theft to gain access to highvalue information including personally identifiable information PII and intellectual property IP. APIs are the common thread that connects all devices and microservices gaining access to the pipeline that carries soughtafter information can prove profitable. In todays drive towards digital transformation the popularity and use of APIs increases as does the cyberrisk landscape associated with it said Filip Verloy field CTO EMA at API provider Noname Security. The problem isnt that APIs are insecure but that there are so many APIs in use in modern enterprise environments that these vulnerabilities go unnoticed and unaddressed. In fact according to Gartner by 2025 less than 50 of enterprise APIs will be managed as the growth in APIs surpasses the capabilities of API management tools. As the number of APIs in use increases it becomes harder for organizations to secure and track them Verloy said. If attackers are trying their luck in industries and businesses they know are full of APIs its likely they will find an unauthenticated API similar to what occurred during the Optus breach. API security challenges The weaknesses of tokens When looking to exploit an API threat actors will often try to harvest client credentials and API keys to obtain access to the underlying data. Many API authentication measures are easily exploitable. For example some APIs use API keys or tokens to authorize client access to datasets. A client calls the API and uses a unique authentication key or credential to authenticate the clients identity and exchange data with the service. The problem with this is that if the information isnt encrypted with HTTPS during the call then a hacker can eavesdrop on the communication harvest the token from the client and use it to gather data from the API. Multifactor authentication is now the default for human user authentication but APIs typically rely on a single credential which is often hardcoded as an API key said Faiyaz Shahpurwala chief product and strategy officer at Fortanix. This issue along with the systemic access and intelligence i. e. what actions are supported for authenticated users and what system components are accessible via the API provided makes APIs a suitable target for attackers looking to compromise networks Shahpurwala said. Enterprises thus need to implement increased authentication controls such as multifactor authentication for token access to verify the identity of clients before allowing the connection. Want to secure APIs Start with visibility move to controls When looking to secure APIs at a high level organizations need to have a full perspective on external and internal APIs that exist throughout the environment. This means using tools from providers like Salt Security and Noname Security to automatically discover and create an inventory of APIs and to identify potential security risks. In addition organizations will need open collaboration between developers and security teams. Security teams will want to work with their dev counterparts to have a process for deploying and updating APIs said Sandy Carielli principal analyst security and risk at Forrester. Security leaders should make use of API discovery and inventory tools to have an accurate view of what APIs are deployed in their environment. Carielli suggests that organizations implement API gateways for authentication authorization and rate limiting while using WAF and bot management tools to manage and mitigate malicious traffic. Other actions like deactivating zombie APIs deprecated APIs that havent been disabled and implementing rolebased or policybased identity and access management controls for creating accessing and managing APIs can help to mitigate other risks. VentureBeats mission is to be a digital town square for technical decisionmakers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings.,2023-01-05T16:07:00Z
22,"GitHub ESP32 OTA Updates, Now in MicroPython Flavor","Wouldn’t it be great if you could keep all of your small Internet-connected hacks up to date with a single codebase? A couple of weeks ago, we wrote up a project that automagically pulls down OTA u…",https://hackaday.com/2022/12/23/github-esp32-ota-updates-now-in-micropython-flavor/,Wouldnt it be great if you could keep all of your small Internetconnected hacks up to date with a single codebase A couple of weeks ago we wrote up a project that automagically pulls down OTA updates to an ESP32 from GitHub using the ESP32 C SDK. Pascal asked in the comments but what about MicroPython Gauntlet thrown TURFPTAx wrote ugit. py a simple library that mirrors all of the code from a public GitHub Python repo straight to your gizmo running Micropython. Damped wrote in about Senko another library that does something very similar but by then TURFPTAx was already done. Bam Part of the speed is that MicroPython includes everything you need to get the job done parsing streamed JSON was the hard part with the original hack. MicroPython makes those sorts of things easy. This is one of those ideas thats just brilliant for a hacker with a small flock of independent devices to herd. And because ugit. py itself is fairly simple and readable if you need to customize it to do your own bidding thats no problem either. Just be sure that when youre storing your WiFi authentication info its not publicly displayed. TURFPTAx could I log into your home WiFi Whats TURFPTAx going to be using this for Were guessing its going to be deploying code to his awesome Open Muscle sensing rigs. What will we be using it for Blinky Christmas decorations for the inlaws now remotely updatable without them having to even learn what a repo is.,2022-12-23T12:00:50Z
23,NanoGPT,"The simplest, fastest repository for training/finetuning medium-sized GPTs. - GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs.",https://github.com/karpathy/nanoGPT,nanoGPT The simplest fastest repository for trainingfinetuning mediumsized GPTs. It is a rewrite of minGPT that prioritizes teeth over education. Still under active development but currently the file train. py reproduces GPT2 124M on OpenWebText running on a single 8XA100 40GB node in 38 hours of training. The code itself is plain and readable train. py is a 300line boilerplate training loop and model. py a 300line GPT model definition which can optionally load the GPT2 weights from OpenAI. Thats it. Because the code is so simple it is very easy to hack to your needs train new models from scratch or finetune pretrained checkpoints e. g. biggest one currently available as a starting point would be the GPT2 1. 3B model from OpenAI. install Dependencies pytorch 3 numpy 3 pip install transformersfor huggingface transformers 3 to load GPT2 checkpoints pip install datasetsfor huggingface datasets 3 if you want to download preprocess OpenWebText pip install tiktokenfor OpenAIs fast BPE code 3 pip install wandbfor optional logging 3 pip install tqdm usage To render a dataset we first tokenize some documents into one simple long 1D array of token indices. E. g. for OpenWebText run cd dataopenwebtext python prepare. py To download and tokenize the OpenWebText dataset. This will create a train. bin and val. bin which holds the GPT2 BPE token ids in one sequence stored as raw uint16 bytes. Then were ready to kick off training. The training script currently by default tries to reproduce the smallest GPT2 released by OpenAI i. e. the 124M version of GPT2. We can train as follows on a single device though I encourage you to read the code and see all of the settings and paths up top in the file python train. py To train using PyTorch Distributed Data Parallel DDP run the script with torchrun. For example to train on a node with 4 GPUs run torchrun standalone nprocpernode4 train. py Once some checkpoints are written to the output directory e. g.. out by default we can sample from the model python sample. py Training on 1 A100 40GB GPU overnight currently gets loss 3. 74 training on 4 gets 3. 60. Training on an 8 x A100 40GB node for 500000 iters 1 day atm gets down to 3. 1. Random chance at init is ln150257 10. 82. Which brings us to baselines. baselines OpenAI GPT2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows python train. py evalgpt2 python train. py evalgpt2medium python train. py evalgpt2large python train. py evalgpt2xl and observe the following losses on train and val I briefly tried finetuning gpt2 a bit more on our OWT and didnt notice dramatic improvements suggesting that OWT is not much much different from WT in terms of the data distribution but this needs a bit more thorough attempt once the code is in a better place. finetuning For an example of how to finetune a GPT on new text go to datashakespeare and look at prepare. py to download the tiny shakespeare dataset and render it into a train. bin and val. bin. Unlike OpenWebText this will run in seconds. Finetuning takes very little time e. g. on a single GPU just a few minutes. Run an example finetuning like python train. py configfinetuneshakespeare. py This will load the config parameter overrides in configfinetuneshakespeare. py I didnt tune them much though. Basically we initialize from a GPT2 checkpoint with initfrom and train as normal except shorter and with a small learning rate. The best checkpoint lowest validation loss will be in the outdir directory e. g. in outshakespeare by default per the config file. You can then run the code in sample. py to generate infinite Shakespeare. Note that youll have to edit it to point to the correct outdir. i only have a macbook Its possible to play with the code if you only have a macbook or some other cheap computer. In this case its much easier to just work with the Shakespeare dataset. Step 1 render the training data cd datashakespeare python prepare. py Then launch the training script with a baby network here is an example cd.... python train. py datasetshakespeare nlayer4 nhead4 nembd64 devicecpu compileFalse evaliters1 blocksize64 batchsize8 This creates a much smaller Transformer 4 layers 4 heads 64 embedding size runs only on CPU does not torch. compile the model torch seems to give an error if you try only evaluates for one iteration so you can see the training loop at work immediately and also makes sure the context length is much smaller e. g. 64 tokens and the batch size is reduced to 8. On my MacBook Air M1 this takes about 400ms per iteration. The network is still pretty expensive because the current vocabulary is hardcoded to be the GPT2 BPE encodings of vocabsize50257. So the embeddings table and the last layer are still massive. You can now also work with tiny shakespeare on the character level see datashakespearechar and run prepare. py to tokenize it on the character level. If you have a GPU you can use the decent starter settings in a provided config file train as follows python train. py configtrainshakespearechar. py But if all you have is a CPU you may want to further override the settings down another notch e. g. python train. py configtrainshakespearechar. py devicecpu compileFalse evaliters20 loginterval1 blocksize64 batchsize8 Where we decrease the context length to just 64 characters and only use a batch size of 8. benchmarking For model benchmarking bench. py might be useful. Its identical to what happens in the meat of the training loop of train. py but omits much of the other complexities. efficiency notes Code by default now uses PyTorch 2. 0. At the time of writing Dec 29 2022 this makes torch. compile available in the nightly release. The improvement from the one line of code is noticeable e. g. cutting down iteration time from 250ms iter to 135ms iter. Nice work PyTorch team todos A few todos Im aware of Optimizations Additional optimizations to the running time Investigate need for an actual Data Loader with a dedicated worker process for data Look into more efficient fused optimizers e. g. apex Reevaluate use of flash attention previously I wasnt able to get the forward pass to match up so I took it out CUDA Graphs Investigate potential speedups from Lightning or huggingface Accelerate Features APIs Add back fp16 support would need to also add back gradient scaler Finetune the finetuning script I think the hyperparams are not great Report and track other metrics e. g. perplexity numtokens MFU... Eval zeroshot perplexities on PTB WikiText other related benchmarks Suspiciousness Current initialization PyTorch default departs from GPT2. In a very quick experiment I found it to be superior to the one suggested in the papers but that cant be right I dont currently seem to need gradient clipping but it is very often used. Nothing is exploding so far at these scales but maybe Im leaving performance on the table. Evaluate withwithout. I am still not 100 confident that my GPT2 small reproduction hyperparameters are good if someone has reproduced GPT2 Id be eager to exchange notes ty I keep seeing different values cited for weight decay and AdamW betas look into I cant exactly reproduce Chinchilla paper results see scalinglaws. ipynb notebook Results Actually reproduce GPT2 results and have clean configs that reproduce the result. It was estimated 3 years ago that the training cost of 1. 5B model was 50K. Sounds a bit too high. troubleshooting Note that by default this repo uses PyTorch 2. 0 i. e. torch. compile. This is fairly new and experimental and not yet available on all platforms e. g. Windows. If youre running into related error messages try to disable this by adding compileFalseflag. This will slow down the code but at least it will run. acknowledgements All nanoGPT experiments are powered by GPUs on Lambda labs the best Cloud GPU provider thank you,2023-01-11T08:34:25Z
24,"Compromised PyTorch-nightly dependency chain December 30th, 2022",An open source machine learning framework that accelerates the path from research prototyping to production deployment.,https://pytorch.org/blog/compromised-nightly-dependency/,If you installed PyTorchnightly on Linux via pip between December 25 2022 and December 30 2022 please uninstall it and torchtriton immediately and use the latest nightly binaries newer than Dec 30th 2022. pip3 uninstall y torch torchvision torchaudio torchtriton pip3 cache purge PyTorchnightly Linux packages installed via pip during that time installed a dependency torchtriton which was compromised on the Python Package Index PyPI code repository and ran a malicious binary. This is what is known as a supply chain attack and directly affects dependencies for packages that are hosted on public package indices. NOTE Users of the PyTorch stable packages are not affected by this issue. How to check if your Python environment is affected The following command searches for the malicious binary in the torchtriton package PYTHONSITEPACKAGEStritonruntimetriton and prints out whether your current Python environment is affected or not. python3 c import pathlibimport importlib. utilsimportlib. util. findspectriton affectedanyx. name triton for x in pathlib. Paths. submodulesearchlocations0 if s is not None else runtime. globprintYou are affected. format if affected else not The malicious binary is executed when the triton package is imported which requires explicit code to do and is not PyTorchs default behavior. The Background At around 440pm GMT on December 30 Friday we learned about a malicious dependency package torchtriton that was uploaded to the Python Package Index PyPI code repository with the same package name as the one we ship on the PyTorch nightly package index. Since the PyPI index takes precedence this malicious package was being installed instead of the version from our official repository. This design enables somebody to register a package by the same name as one that exists in a third party index and pip will install their version by default. This malicious package has the same name torchtriton but added in code that uploads sensitive data from the machine. What we know torchtriton on PyPI contains a malicious triton binary which is installed at PYTHONSITEPACKAGEStritonruntimetriton. Its SHA256 hash is listed below. SHA256triton 2385b29489cd9e35f92c072780f903ae2e517ed422eae67246ae50a5cc738a0e The binarys main function does the following Get system information nameservers from etcresolv. conf hostname from gethostname current username from getlogin current working directory name from getcwd environment variables nameservers from Read the following files etchosts etcpasswd The first 1000 files in HOME HOME. gitconfig HOME. ssh Upload all of this information including file contents via encrypted DNS queries to the domain. h4ck. cfd using the DNS server wheezy. io The binarys file upload functionality is limited to files less than 99999 bytes in size. It also uploads only the first 1000 files in HOME but all files 99999 bytes in the. ssh directory. Steps taken towards mitigation torchtriton has been removed as a dependency for our nightly packages and replaced with pytorchtriton pytorchpytorch91539 and a dummy package registered on PyPI so that this issue doesnt repeat All nightly packages that depend on torchtriton have been removed from our package indices at httpsdownload. pytorch. org until further notice We have reached out to the PyPI security team to get proper ownership of the torchtritonpackage on PyPI and to delete the malicious version,2023-01-01T02:04:39Z
25,Arc Overhangs Make “Impossible” 3D Prints,An accidental discovery by [3DQue] allows overhangs on FDM printers that seem impossible at first glance. The key is to build the overhang area with concentric arcs. It also helps to print at a coo…,https://hackaday.com/2022/12/21/arc-overhangs-make-impossible-3d-prints/,An accidental discovery by 3DQue allows overhangs on FDM printers that seem impossible at first glance. The key is to build the overhang area with concentric arcs. It also helps to print at a cool temperature with plenty of fan and a slow print speed. In addition to the video from 3DQue theres also a video from CNC Kitchen below that covers the technique. If you want a quick overview you might want to start with the CNC Kitchen video first. The basic idea is that you build surfaces in the air by making small arcs that overlap and get further and further away from the main body of the part. Because the arcs overlap they support the next arc. The results are spectacular. Theres a third video below that shows some recent updates to the tool. Weve seen a similar technique handcrafted with fullcontrol. xyz but this is a Python script that semiautomatically generates the necessary arcs that overlap. We admit the surface looks a little odd but depending on why you need to print overhangs this might be just the ticket. There can also be a bit of warping if features are on top of the overhang. You dont need any special hardware other than good cooling. Like CNC Kitchen we hope this gets picked up by mainstream slicers. It probably will never be a default setting but it would be a nice option for parts that can benefit from the technique. Since the code is on GitHub maybe people familiar with the mainstream slicers will jump in and help make the algorithm more widely available and automatic. What will you build with this tool If you dont like arcs check out conical slicing or nonplanar slicing instead.,2022-12-21T12:00:59Z
26,"How To Access Google Analytics API Via Python via @sejournal, @rvtheverett","From building custom dashboards that display GA data to automating complex reporting tasks, you can set it up using Python. Here's how to do it.
The post How To Access Google Analytics API Via Python appeared first on Search Engine Journal.",https://www.searchenginejournal.com/how-to-access-google-analytics-api-via-python/474458/,The Google Analytics API provides access to Google Analytics GA report data such as pageviews sessions traffic source and bounce rate. The official Google documentation explains that it can be used to Build custom dashboards to display GA data. Automate complex reporting tasks. Integrate with other applications. You can access the API response using several different methods including Java PHP and JavaScript but this article in particular will focus on accessing and exporting data using Python. This article will just cover some of the methods that can be used to access different subsets of data using different metrics and dimensions. I hope to write a followup guide exploring different ways you can analyze visualize and combine the data. Setting Up The API Creating A Google Service Account The first step is to create a project or select one within your Google Service Account. Once this has been created the next step is to select the Create Service Account button. You will then be promoted to add some details such as a name ID and description. Once the service account has been created navigate to the KEYS section and add a new key. This will prompt you to create and download a private key. In this instance select JSON and then create and wait for the file to download. Add To Google Analytics Account You will also want to take a copy of the email that has been generated for the service account this can be found on the main account page. The next step is to add that email as a user in Google Analytics with Analyst permissions. Enabling The API The final and arguably most important step is ensuring you have enabled access to the API. To do this ensure you are in the correct project and follow this link to enable access. Then follow the steps to enable it when promoted. This is needed in order to access the API. If you miss this step you will be prompted to complete it when first running the script. Accessing The Google Analytics API With Python Now everything is set up in our service account we can start writing the script to export the data. I chose Jupyter Notebooks to create this but you can also use other integrated developer environments IDEs including PyCharm or VSCode. Installing Libraries The first step is to install the libraries that are needed to run the rest of the code. Some are unique to the analytics API and others are useful for future sections of the code. pip install upgrade googleapipythonclient pip3 install upgrade oauth2client from apiclient. discovery import build from oauth2client. serviceaccount import ServiceAccountCredentials pip install connect pip install functions import connect Note When using pip in a Jupyter notebook add the if running in the command line or another IDE the isnt needed. Creating A Service Build The next step is to set up our scope which is the readonly analytics API authentication link. This is followed by the client secrets JSON download that was generated when creating the private key. This is used in a similar way to an API key. To easily access this file within your code ensure you have saved the JSON file in the same folder as the code file. This can then easily be called with the KEYFILELOCATION function. Finally add the view ID from the analytics account with which you would like to access the data. Altogether this will look like the following. We will reference these functions throughout our code. SCOPES httpswww. googleapis. comauthanalytics. readonly KEYFILELOCATION clientsecrets. json VIEWID XXXXX Once we have added our private key file we can add this to the credentials function by calling the file and setting it up through the ServiceAccountCredentials step. Then set up the build report calling the analytics reporting API V4 and our already defined credentials from above. credentials ServiceAccountCredentials. fromjsonkeyfilenameKEYFILELOCATION SCOPES service buildanalyticsreporting v4 credentialscredentials Writing The Request Body Once we have everything set up and defined the real fun begins. From the API service build there is the ability to select the elements from the response that we want to access. This is called a ReportRequest object and requires the following as a minimum A valid view ID for the viewId field. At least one valid entry in the dateRanges field. At least one valid entry in the metrics field. View ID As mentioned there are a few things that are needed during this build stage starting with our viewId. As we have already defined previously we just need to call that function name VIEWID rather than adding the whole view ID again. If you wanted to collect data from a different analytics view in the future you would just need to change the ID in the initial code block rather than both. Date Range Then we can add the date range for the dates that we want to collect the data for. This consists of a start date and an end date. There are a couple of ways to write this within the build request. You can select defined dates for example between two dates by adding the date in a yearmonthdate format startDate 20221027 endDate 20221127. Or if you want to view data from the last 30 days you can set the start date as 30daysAgo and the end date as today. Metrics And Dimensions The final step of the basic response call is setting the metrics and dimensions. Metrics are the quantitative measurements from Google Analytics such as session count session duration and bounce rate. Dimensions are the characteristics of users their sessions and their actions. For example page path traffic source and keywords used. There are a lot of different metrics and dimensions that can be accessed. I wont go through all of them in this article but they can all be found together with additional information and attributes here. Anything you can access in Google Analytics you can access in the API. This includes goal conversions starts and values the browser device used to access the website landing page secondpage path tracking and internal search site speed and audience metrics. Both the metrics and dimensions are added in a dictionary format using keyvalue pairs. For metrics the key will be expression followed by the colon and then the value of our metric which will have a specific format. For example if we wanted to get a count of all sessions we would add expression gasessions. Or expression ganewUsers if we wanted to see a count of all new users. With dimensions the key will be name followed by the colon again and the value of the dimension. For example if we wanted to extract the different page paths it would be name gapagePath. Or name gamedium to see the different traffic source referrals to the site. Combining Dimensions And Metrics The real value is in combining metrics and dimensions to extract the key insights we are most interested in. For example to see a count of all sessions that have been created from different traffic sources we can set our metric to be gasessions and our dimension to be gamedium. response service. reports. batchGet body reportRequests viewId VIEWID dateRanges startDate 30daysAgo endDate today metrics expression gasessions dimensions name gamedium. execute Creating A DataFrame The response we get from the API is in the form of a dictionary with all of the data in keyvalue pairs. To make the data easier to view and analyze we can turn it into a Pandas dataframe. To turn our response into a dataframe we first need to create some empty lists to hold the metrics and dimensions. Then calling the response output we will append the data from the dimensions into the empty dimensions list and a count of the metrics into the metrics list. This will extract the data and add it to our previously empty lists. dim metric for report in response. getreports columnHeader report. getcolumnHeader dimensionHeaders columnHeader. getdimensions metricHeaders columnHeader. getmetricHeader. getmetricHeaderEntries rows report. getdata. getrows for row in rows dimensions row. getdimensions dateRangeValues row. getmetrics for header dimension in zipdimensionHeaders dimensions dim. appenddimension for i values in enumeratedateRangeValues for metricHeader value in zipmetricHeaders values. getvalues metric. appendintvalue Adding The Response Data Once the data is in those lists we can easily turn them into a dataframe by defining the column names in square brackets and assigning the list values to each column. df pd. DataFrame dfSessions metric dfMedium dim df dfMediumSessions df. head More Response Request Examples Multiple Metrics There is also the ability to combine multiple metrics with each pair added in curly brackets and separated by a comma. metrics expression gapageviews expression gasessions Filtering You can also request the API response only returns metrics that return certain criteria by adding metric filters. It uses the following format if metricName operator comparisonValue return the metric For example if you only wanted to extract pageviews with more than ten views. response service. reports. batchGet body reportRequests viewId VIEWID dateRanges startDate 30daysAgo endDate today metrics expression gapageviews dimensions name gapagePath metricFilterClauses filters metricName gapageviews operator GREATERTHAN comparisonValue 10. execute Filters also work for dimensions in a similar way but the filter expressions will be slightly different due to the characteristic nature of dimensions. For example if you only want to extract pageviews from users who have visited the site using the Chrome browser you can set an EXTRACT operator and use Chrome as the expression. response service. reports. batchGet body reportRequests viewId VIEWID dateRanges startDate 30daysAgo endDate today metrics expression gapageviews dimensions name gabrowser dimensionFilterClauses filters dimensionName gabrowser operator EXACT expressions Chrome. execute Expressions As metrics are quantitative measures there is also the ability to write expressions which work similarly to calculated metrics. This involves defining an alias to represent the expression and completing a mathematical function on two metrics. For example you can calculate completions per user by dividing the number of completions by the number of users. response service. reports. batchGet body reportRequests viewId VIEWID dateRanges startDate 30daysAgo endDate today metrics expression gagoal1completionsgausers alias completions per user. execute Histograms The API also lets you bucket dimensions with an integer numeric value into ranges using histogram buckets. For example bucketing the sessions count dimension into four buckets of 19 1099 100199 and 200399 you can use the HISTOGRAMBUCKET order type and define the ranges in histogramBuckets. response service. reports. batchGet body reportRequests viewId VIEWID dateRanges startDate 30daysAgo endDate today metrics expression gasessions dimensions name gasessionCount histogramBuckets 110100200400 orderBys fieldName gasessionCount orderType HISTOGRAMBUCKET. execute In Conclusion I hope this has provided you with a basic guide to accessing the Google Analytics API writing some different requests and collecting some meaningful insights in an easytoview format. I have added the build and request code and the snippets shared to this GitHub file. I will love to hear if you try any of these and your plans for exploring the data further. More resources A Guide To Googles Knowledge Graph Search API For SEO How To Build A Google Analytics 4 Landing Page Report In Less Than 4 Minutes Google Analytics 4 Three Metrics You Should Know To Optimize Google Ads Featured Image BestForBestShutterstock,2023-01-03T10:00:09Z
27,Prompt engineering DaVinci-003 on our own docs for automated support (Part I),Tech support as an LLM Slack bot,https://www.patterns.app//blog/2022/12/21/finetune-llm-tech-support,One problem were working on at Patterns is how to scale our technical support. We have technical documentation Slack channels emails and support tickets that all provide a way for us to interface with our customers. Like many folks weve been playing around with the power and potential of new Large Language Models like ChatGPT so we decided to see if we could help tackle our support problem with an LLM support bot. We came in with somewhat low expectations we know these models are prone to common failure modes youd expect from a nexttoken optimizer so we were shocked when we saw the end result. Read on to learn how we did it and our experience building with LLMs. By the end of the post well have built a full app that you can clone and customize to have your own version of our bot if youd like. Game plan Our plan was to build an automated slack bot that would respond to tech support questions with knowledge from our technical documentation community slack channels and previous support tickets. Normally with LLMs you put all the necessary context in the prompt. We had far too much content to fit in the 2048 token limit for the normal prompt though. Luckily many LLM providers like OpenAI provide a finetuning api where you can submit labeled example completions to finetune your own version of their LLM. This finetuning workflow seemed like a good fit for our problem it could take our rich text corpus finetune a model and then provide completions via a Slack bot. To serve this experience as a Patterns app endtoend we need to Generate training data from our text corpus Finetune our model OpenAIs GPT3 davinci003 engine Serve the finetuned model as a Slack bot Lets build it Prerequisites OpenAI account API key A text corpus to train your bot e. g. technical documentation or something similar Generating training data Immediately we ran into a problem to finetune an OpenAI model requires a specific format of promptcompletion pairs prompt prompt text completion ideal generated text prompt prompt text completion ideal generated text prompt prompt text completion ideal generated text Our freeform text corpus wouldnt work out of the box. We could manually generate the training data but thats a lot of work. So we remembered the Golden Rule of LLMs If it at first you dont succeed send it through again with a new prompt. So instead of manually labeling we come up with a series of prompts to automatically generate our labeled pairs from our corpus Feed GPT a chunk of our corpus and ask it to generate three relevant questions for the chunk Feed those questions back in with the chunk and ask GPT to specifically answer the generated question using the chunk Take this output and pair with the original generated question for a labeled training example to feed to the finetuner Import training data into Patterns There are many ways to import our documentation into Patterns. To keep it simple for our example we loaded ours into an Airtable base that we imported via the standard Airtable component in Patterns. An example of the technical documentation text contained Webhooks and Streaming Patterns has native support for stream processing. A common automation is to have a webhook ingesting realtime events from some external system processing these events with a series of python nodes and then sending a response either to a communication channel like Slack or Discord or to trigger an action in another external system. Generate questions to use as prompts from our docs With our text data in Patterns we wrote a prompt to tease out three questions that would act as our prompts in training data. From the marketplace we cloned the OpenAI Completions component added the following prompt and ran it to generate completions an example is shown in green below. You are reviewing the documentation for a data company called Patterns. Write three questions based on the documentation below. Documentation Patterns has native support for stream processing. A common automation is to have a webhook ingesting realtime events from some external system processing these events with a series of python nodes and then sending a response either to a communication channel like Slack or Discord or to trigger an action in another external system. Questions 1. What types of realtime events are supported by Patterns webhooks 2. How are python nodes used to process events 3. How are responses sent to external systems This is what it looks like in the UI. Generate answers from the questions in the prior step With the three generated questions prompts that are relevant to our task we next generated answers for these using the documentation text. Again we cloned the OpenAI Completions component and edited the prompt You are a smart and witty technical support agent for a data company called Patterns. Please generate answers to the following questions given the documentation. Documentation Patterns has native support for stream processing. A common automation is to have a webhook ingesting realtime events from some external system processing these events with a series of python nodes and then sending a response either to a communication channel like Slack or Discord or to trigger an action in another external system. Questions 1. What types of realtime events are supported by Patterns webhooks 2. How are python nodes used to process events 3. How are responses sent to external systems Answers 1. Patterns webhooks support a wide range of realtime events including webhooks for Slack Discord and other external systems. 2. Python nodes are used to process the realtime events received via webhooks and transform them into the desired output format. 3. Responses can be sent to external systems through a communication channel like Slack or Discord or by triggering an action in the external system. Next we prepped the data and formatted it for finetuning. There are many tricks here and OpenAI has a CLI data preparation tool to ensure the structure of your data prior to upload. Upload training data to OpenAI and start the finetuning job This resulted in promptcompletion pairs that we could structure and upload to OpenAI for finetuning. This involved first uploading our completions which took a few seconds and then kicking off the finetuning job on OpenAI. Depending on the size of training data and OpenAIs resource availability finetuning can take anywhere from a few minutes to a few hours for that reason we kept a state object in our finetuning script and check for updates until its finished. When the finetuning job completes successfully we write out the model to another table named finetunedmodels. Configure finetuned model as a Slack Bot With the finetuned model ready to go on OpenAIs servers we could tackle the next step building a Slackbot to receive and respond to customers questions. To configure a Slack bot in Patterns that uses GPT3 we set up the following flow A webhook configured to receive messages from a Slack channel A Python node to detect when the bot is mentioned An OpenAI node parameterized with the modelnamefrom the finetuning process that provides a completion to any prompt A Python node that serves completion back to the Slack channel To do this we replicated Engineering Advice GPT3 Slack Bot into the same App and followed the configuration in that link. Learnings and next steps Overall we were impressed with the bots ability to generate answers to relevant questions that its trained on. However a lot of the time the bot just makes stuff up this occurs for both prompts that do and dont have relevant samples in the training data. Making stuff up and being confidently wrong are well known sideeffects of LLMs and there are many techniques to change this behavior. totally made up URL not even our domain In Part II of this post coming soon we will explore several avenues for improvement including expanding the corpus via automated ingestion using embeddings to encode documentation semantics to provide more robust links directly to documentation and labeling some negative examples of when the bot should or should not respond. If you have any questions about setting up a use case similar to this please email me at chrispatterns. app or create a Slack channel from your dashboard and message us there.,2022-12-22T14:50:06Z
28,A Brief History of SPAM,"From Monty Python to Minnesota's SPAM Museum, this little packaged luncheon meat has a big cultural footprint.",https://www.mentalfloss.com/posts/spam-food-history,A Brief History of SPAM Even if youve never tried SPAM youre probably familiar with its iconic packaging and oneofakind name. From Monty Python to Minnesotas SPAM Museum this little packaged luncheon meat has a big cultural footprint. In the latest episode of Food History host Justin Dodd discusses SPAMs history from army base stew to SPAM musubi a Hawaiian take on sushi that features fried SPAM in place of fish. Whether you love SPAM or hate it you probably dont know where the word comes from. Thats because its manufacturer Hormel guards the true meaning if there is one from the public. According to Life magazine Jay Hormel hosted a New Years Eve party in which the price of each drink was a possible name for the new product written on a slip of paper. He offered a 100 prize to whomever could come up with the winning name. As Hormel recalled Along about the third or fourth drink they began showing some imagination. An actor named Ken Daigneau brother of a Hormel Foods vice president received the 100 prize for his shortandsweet moniker. Many theories have been floated over the decades to explain the name with some saying its short for Shoulder of Pork And Ham. Others offer a lesspleasant option Scientifically Processed Animal Matter. The most common belief is that SPAM is a portmanteau of spiced and ham despite the fact that the product is neither spiced nor a ham. Watch the full video to learn some fun facts about SPAMs history from its complicated place in Korean culinary history to its surprisingly simple list of ingredients. You can also watch dozens of episodes of Food History on YouTube.,2023-01-06T20:00:00Z
29,What Is the Arduino MicroPython IDE?,How does Arduino Labs for MicroPython change how you use Arduino?,https://www.makeuseof.com/what-is-the-arduino-micropython-ide/,Arduino Labs for MicroPython is an experimental lightweight crossplatform editor for MicroPython. Arduino now offers support for the Python programming language as a means of interacting with its microcontroller boards. This means that you can now leverage the vast libraries and ecosystem of the language to build more powerful and complex projects. The power of Python coupled with the versatility of Arduino may just be the perfect combination for your next project Despite being in its experimental phase Arduino Labs for MicroPython has already garnered a lot of attention from the maker community with many people eager to try it out. So what prompted this move Arduino vs. MicroPython Arduino has long been a favorite among hobbyists and professionals alike for its range of cheap hardware options and active community of users who are always willing to lend a helping hand on the Arduino forum. There are many useful Arduino projects to try from home automation to robotics. One of the main advantages of Arduino is its simplicity. The hardware is easy to use and the programming language is based on CC which is widely known and easy to learn. You can check our exclusive guide on what an Arduino is and what you can do with it to learn more. MicroPython on the other hand is a lean and efficient implementation of the Python programming language that is designed for use on microcontrollers. It offers a powerful programming environment that is ripe for experimentation and allows you to take advantage of the rich ecosystem of Python libraries and tools. One of the main benefits of MicroPython is its flexibilityPython is a very expressive language that is wellsuited for prototyping. The Arduino IDE for MicroPython created in collaboration with Murilo Polese who wrote the initial version offers a robust development platform that combines the benefits of both worlds. This results in a more efficient and effective development experience. Its Not Another Arduino IDE Extension While you may think its just another extension built for the already existing Arduino IDE its an entirely separate development platform built from scratch by the team at Arduino specifically to support MicroPython. This IDE is similar to the original Arduino IDE but it allows users to write and upload Python code to their boards. One of the main features of the Arduino IDE for MicroPython is its support for boards with REPL over serial including the vulnerable ESP8266. This allows you to easily upload and run Python code on your microcontroller boards without the need for external tools or complicated setup procedures. The IDE also includes file manager capabilities which make it easy to organize and manage your Python scripts. Arduino IDE for MicroPython is available on Windows Linux and macOS download it free from the Arduino Lab for Micropython page. Since it uses Real Eval Print Loop REPL over serial the IDE supports other boards too like the Raspberry Pico. The Pythonspecific REPL shell also comes in store with the IDE which allows you to test out small pieces of your code when debugging. Additionally It lets you build your projects faster as you are using a highlevel language that is very easy to learna perfect fit for someone getting into microcontrollers. One of the main advantages of using MicroPython is its ability to save on the limited reprogramming counts of the program flash memory of the microcontroller. This is because the Python script is stored and run in the RAM rather than being written directly to the flash memory. This can be especially useful for projects that require frequent updates or changes to the code. However there is no syntax checking for errors at the moment which can make it more difficult to spot mistakes in your code. Arduino has not yet communicated any plans to add this feature in the future. What Is the Future of Arduino Lab for MicroPython This MicroPython IDE seems like a good fit for those who want more flexibility and the ability to take advantage of the rich ecosystem of Python libraries. With the growth in popularity of the Python programming language its set to gain more traction which may eventually prompt Arduino to expand its MicroPython library on the platform. Its reception in the Arduino community looks promising and already has all the signs of becoming a permanent product. As of now the MicroPython firmware is only available for the Arduino Nano 33 BLE Sense Nano RP2040 Connect and Portenta H7 boards. It is not clear when it will be available for other models. Ready for the GameChanger IDE for Every Maker Overall the Arduino Lab for MicroPython is a welcome addition to the Arduino ecosystem since it offers the best of both worldsthe rich hardware options and supportive community of Arduino combined with the powerful programming capabilities of MicroPython. Whether you are a seasoned pro or just getting started with microcontrollers this development is sure to have something for you.,2022-12-21T16:45:17Z
