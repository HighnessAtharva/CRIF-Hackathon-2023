Article,Title,Description,URL,Content,Headline,Headline Sentiment,Offense Rating,Negative Words,Offensive Words,Tags
1,"GitHub ESP32 OTA Updates, Now in MicroPython Flavor","Wouldn’t it be great if you could keep all of your small Internet-connected hacks up to date with a single codebase? A couple of weeks ago, we wrote up a project that automagically pulls down OTA u…",https://hackaday.com/2022/12/23/github-esp32-ota-updates-now-in-micropython-flavor/,Wouldnt it be great if you could keep all of your small Internetconnected hacks up to date with a single codebase A couple of weeks ago we wrote up a project that automagically pulls down OTA updates to an ESP32 from GitHub using the ESP32 C SDK. Pascal asked in the comments but what about MicroPython Gauntlet thrown TURFPTAx wrote ugit. py a simple library that mirrors all of the code from a public GitHub Python repo straight to your gizmo running Micropython. Damped wrote in about Senko another library that does something very similar but by then TURFPTAx was already done. Bam Part of the speed is that MicroPython includes everything you need to get the job done parsing streamed JSON was the hard part with the original hack. MicroPython makes those sorts of things easy. This is one of those ideas thats just brilliant for a hacker with a small flock of independent devices to herd. And because ugit. py itself is fairly simple and readable if you need to customize it to do your own bidding thats no problem either. Just be sure that when youre storing your WiFi authentication info its not publicly displayed. TURFPTAx could I log into your home WiFi Whats TURFPTAx going to be using this for Were guessing its going to be deploying code to his awesome Open Muscle sensing rigs. What will we be using it for Blinky Christmas decorations for the inlaws now remotely updatable without them having to even learn what a repo is.,No Headline,Extremely Negative,220,"['Unfortunately', 'dead', 'shame']",[],"[Utkarsh, Upadhyay, Samar, Dhwoj, Acharya]"
2,Second Life on GitHub,"Linden Lab open-sourced Second Life's client on January 8th, 2007. In the 15 years since, open source contributions have led to countless new features and bug fixes that have markedly improved the life of our virtual world. Without open source, SL would not b…",https://community.secondlife.com/blogs/entry/12081-second-life-on-github/,Second Life on GitHub Linden Lab opensourced Second Lifes client on January 8th 2007. In the 15 years since open source contributions have led to countless new features and bug fixes that have markedly improved the life of our virtual world. Without open source SL would not be as vibrant enduring and successful as it is today. As part of an effort to improve and modernize our tooling and better support the open source community Im happy to announce that Second Lifes source code is moving to GitHub the worlds most popular version control hosting platform. You can find our open source repositories in their new home at github. comsecondlife. As the de facto home of many open source projects GitHub likely needs no introduction to anyone who works with open source software. Its features and market position make it the best option for Second Life going forward. Improved Developer Experience While migrating code to GitHub is an important step towards improving Second Life open source contributors development experience it is not a complete picture. We will be making several changes to streamline and modernize our engineering processes Simpler Contributor License Agreements A Contributor License Agreement CLA is a commonly used legal contract between an individual or group and an organization accepting contributions. This process has been modernized adopting simpler more developerfriendly language and allowing contributors to sign the contract as part of pull requests. No PDF no fax. GitHub Actions CICD Second Life will be moving its continuous integration and deployment CICD from a selfhosted TeamCity instance to GitHub Actions. This change to be completed in 2023 will open up new possibilities for our engineering team and the open source community Our build tooling will be more transparent allowing OSS contributors to easily fork and build things like autobuild libraries Pull requests will feature automated merge checks such as linting and automated testing Services can be built using newer technology such as aarch64 servers A simple pipeline to test and publish autobuild. Summary While these changes will not impact most users they will make contributing to Second Life easier and allow the platform to evolve faster. Linden Lab is a uniquely open company holding regular inworld user groups and allowing its residents to contribute in meaningful ways. We are committed to this philosophy and would love to join you in helping make our shared virtual world even better. Do you want to contribute to Second Life Attend one of the user groups on our public calendar submit an idea for a new feature or of course check out our code on GitHub Signal Linden Engineer Second Life Server Edited by Signal Linden 16 9,No Headline,Extremely Negative,230,"['hacks', 'hard', 'hack', 'problem']",[],"[Pascal, TURFPTAx, Damped]"
3,"OpenAI releases Point-E, which is like DALL-E but for 3D modeling","OpenAI, the Elon Musk-founded artificial intelligence startup behind popular DALL-E text-to-image generator, announced on Tuesday the release of its newest picture-making machine POINT-E, which can produce 3D point clouds directly from text prompts. Whereas e…",https://www.engadget.com/openai-releases-point-e-dall-e-3d-text-modeling-210007892.html,OpenAI the Elon Muskfounded artificial intelligence startup behind popular DALLE texttoimage generator announced on Tuesday the release of its newest picturemaking machine POINTE which can produce 3D point clouds directly from text prompts. Whereas existing systems like Googles DreamFusion typically require multiple hours and GPUs to generate their images PointE only needs one GPU and a minute or two. 3D modeling is used across a variety industries and applications. The CGI effects of modern movie blockbusters video games VR and AR NASAs moon crater mapping missions Googles heritage site preservation projects and Metas vision for the Metaverse all hinge on 3D modeling capabilities. However creating photorealistic 3D images is still a resource and time consuming process despite NVIDIAs work to automate object generation and Epic Games RealityCapture mobile app which allows anyone with an iOS phone to scan realworld objects as 3D images. TexttoImage systems like OpenAIs DALLE 2 and Craiyon DeepAI Prisma Labs Lensa or HuggingFaces Stable Diffusion have rapidly gained popularity notoriety and infamy in recent years. Textto3D is an offshoot of that research. PointE unlike similar systems leverages a large corpus of text image pairs allowing it to follow diverse and complex prompts while our imageto3D model is trained on a smaller dataset of image 3D pairs the OpenAI research team led by Alex Nichol wrote in PointE A System for Generating 3D Point Clouds from Complex Prompts published last week. To produce a 3D object from a text prompt we first sample an image using the texttoimage model and then sample a 3D object conditioned on the sampled image. Both of these steps can be performed in a number of seconds and do not require expensive optimization procedures. If you were to input a text prompt say A cat eating a burrito PointE will first generate a synthetic view 3D rendering of said burritoeating cat. It will then run that generated image through a series of diffusion models to create the 3D RGB point cloud of the initial image first producing a coarse 1024point cloud model then a finer 4096point. In practice we assume that the image contains the relevant information from the text and do not explicitly condition the point clouds on the text the research team points out. These diffusion models were each trained on millions of 3d models all converted into a standardized format. While our method performs worse on this evaluation than stateoftheart techniques the team concedes it produces samples in a small fraction of the time. If youd like to try it out for yourself OpenAI has posted the projects opensource code on Github.,"

Tools and Technology

",Extremely Positive,30,['bug'],[],"['lawsuit', Signal, Linden]"
4,"This Week in Security: GitHub Actions, SHA-1 Retirement, and a Self-Worming Vulnerability","It should be no surprise that running untrusted code in a GitHub Actions workflow can have unintended consequences. It’s a killer feature, to automatically run through a code test suite …read more",https://hackaday.com/2022/12/23/this-week-in-security-github-actions-sha-1-retirement-and-a-self-worming-vulnerability/,It should be no surprise that running untrusted code in a GitHub Actions workflow can have unintended consequences. Its a killer feature to automatically run through a code test suite whenever a pull request is opened. But that pull request is run in some part of the targets development environment and theres been a few clever attacks found over the years that take advantage of that. Theres now another one what Legit Security calls Github Environment Injection and there were some bigname organizations vulnerable to it. The crux of the issue is the GITHUBENV file which contains environment variables to be set in the Actions environment. Individual variables get added to this file as part of the automated action and that process needs to include some sanitization of data. Otherwise an attacker can send an environment variable that includes a newline and completely unintended environment variable. And an unintended arbitrary environment variable is game over for the security of the workflow. The example uses the NODEOPTIONS variable to dump the entire environment to an accessible output. Any API keys or other secrets are revealed. This particular attack was reported to GitHub but there isnt a practical way to fix it architecturally. So its up to individual projects to be very careful about writing untrusted data into the GITHUBENV file. Your Tires Are Leaking Data Back a few years ago Mike Metzger gave a DEFCON talk about TPMS Tire Pressure Monitoring Systems. This nifty safety feature allows sensors in car tires to talk to the infotainment center and warn when a tire is low. Drew Griess decided to follow up on this bit of info and see just how practical it would be to use and abuse these gizmos. An RTLSDR and the very useful rtl433 project do the job quite nicely. Add an antenna and the signals are readable over fifty feet away. It really becomes interesting when you realize that each of those sensors have a unique ID sent in each ping. Need to track a vehicle Just follow its tires SHA is dead long live SHA NIST has formally announced the retirement of SHA1 at the end of 2030 with the recommendation to move to SHA2 or SHA3 as soon as is possible. Which seems a bit odd as SHA1 has been considered broken for quite some time most notably in the wake of the SHAttered demonstration from 2017 where two PDFs were generated with matching SHA1 hashes. The latest iteration of that attack puts the cost of generating a collision where the attacker controls both inputs at a measly 45000 of compute. The wheels of official change turn slowly at times. OpenAI Security Researcher One of the tedious bits of reverse engineering is to work through the various functions guess their purpose and rename everything to something useful. If only there was a way to automate the process. Enter Gepetto a project from Ivan Kwiatkowski that asks OpenAIs Davinci003 model to describe what a decompiled function does. Its packaged as an IDA Pro plugin but the concept should apply to other decompilers too. Step two is to fold that description back into the AI model and ask it to name the function and variables. The normal warning applies the AI chat engine will always generate a description that sounds good but it may be wildly inaccurate. Sovrin and Decentralized Vulnerabilities The folks at CyberArk took a look at the Decentralized IDentity DID landscape and found a spectacularly bad vulnerability in the open source Sovrin network. So first DID is an attempt to do something genuinely useful on the blockchain in this case storing identity information. Want to prove that your WordPress account is owned by the same person as your Twitter or Mastodon account DID can help. The version of this idea that really gets our open source juices flowing is SelfSovereign Identity a DID network that allows the end users to have ultimate control over their own data. And for all that goodness the network is still made up of servers running potentially vulnerable code. The POOLUPDGRADE command is limited to authorized administrators of the given pool but the code behind it uses a validatethenauthenticate paradigm. Lets chat about that for a moment. The order of operations can really matter. The first place I really had to think about this concept was while working on Single Packet Authorization in the Fwknop project. Those packets were a bit of request data both encrypted and then authenticated with a shared key. Which should happen first Did we want the data to get signed first and then encrypted Definitely no. The problem is when the message is received on the other side the decryption process would happen first on potentially untrusted data. If there was a vulnerability in the data parsing code it could be triggered by an unauthenticated user. Instead the Fwknop project intentionally used the encryptthenauthenticate approach. So when receiving the incoming packet the first step was to check the authentication and drop the packet if it wasnt from a known user. Back to Sovrin where the processing of an incoming command first went through a validation step before checking for an authorized source. Part of that validation is to look at the packages in the upgrade command and make a call to dpkg to verify that its a real package using a simple concatenation to generate the command. And using subprocess. run with shell set to True. So its trivially exploitable with a semicolon and whatever command you want to run. And to make matters way worse the upgrade command gets forwarded through the pool automatically all before the authentication check. Its not often that a vulnerability is selfworming. This one has a welldeserved 10. 0 CVSS score. This one was privately disclosed back in May and fixed less than a month later. Bits and Bytes Okta is having a rough year. After several breaches earlier this year Oktas private GitHub repositories were accessed and copied by an attacker. So far it appears that no customer data was accessed and to their credit Okta has a security posture that does not rely on the confidentiality of its source code as a means to secure its services. Its likely that this incident was a followon from the previous breach using credentials obtained in that data. And breaking just before we hit the presses Lastpass has revealed more information about the breach they suffered back in November. Its not good. We made an educated guess that the cause was an access token lost during a previous incident but the latest news indicates it was a social engineering attack using captured information. The data lost is troubling including encrypted data vaults metadata like URLs customer name address phone number IP Address etc. Thankfully this doesnt include credit card information and the Lastpass Zero Knowledge architecture does protect the actual passwords assuming your master password is sufficiently secure. This isnt quite a worstcase scenario as no malicious code was shipped to customers but its just about as bad as could be otherwise. Particularly be on the lookout for spearphishing and other social engineering attacks in an attempt to leverage the pilfered information.,"OpenAI releases Point-E, which is like DALL-E but for 3D modeling",Generally Neutral,60,"['object', 'notoriety', 'object', 'object', 'expensive', 'coarse', 'worse']",[],"[Elon, Muskfounded, Alex, Nichol]"
5,Rijndael S-box in 512 bytes of Python,"GitHub Gist: instantly share code, notes, and snippets.",https://gist.github.com/juliusgeo/969c722b2152e53e4f6bb94ca2696c7a,Author juliusgeo commented Jan 4 2023 juliusgeocommented Author juliusgeo commented Jan 4 2023 juliusgeocommented def pprintboxbox print axis 0x02x i for i in range161 print. joinaxis1 while box printaxis1. join0x02x i for i in box16 del box16 axis. pop1 pprintboxsb pprintboxib The addition of this helper code allows you to print out these boxes yielding 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0x0a 0x0b 0x0c 0x0d 0x0e 0x0f 0x00 0x63 0x7c 0x77 0x7b 0xf2 0x6b 0x6f 0xc5 0x30 0x01 0x67 0x2b 0xfe 0xd7 0xab 0x76 0x01 0xca 0x82 0xc9 0x7d 0xfa 0x59 0x47 0xf0 0xad 0xd4 0xa2 0xaf 0x9c 0xa4 0x72 0xc0 0x02 0xb7 0xfd 0x93 0x26 0x36 0x3f 0xf7 0xcc 0x34 0xa5 0xe5 0xf1 0x71 0xd8 0x31 0x15 0x03 0x04 0xc7 0x23 0xc3 0x18 0x96 0x05 0x9a 0x07 0x12 0x80 0xe2 0xeb 0x27 0xb2 0x75 0x04 0x09 0x83 0x2c 0x1a 0x1b 0x6e 0x5a 0xa0 0x52 0x3b 0xd6 0xb3 0x29 0xe3 0x2f 0x84 0x05 0x53 0xd1 0x00 0xed 0x20 0xfc 0xb1 0x5b 0x6a 0xcb 0xbe 0x39 0x4a 0x4c 0x58 0xcf 0x06 0xd0 0xef 0xaa 0xfb 0x43 0x4d 0x33 0x85 0x45 0xf9 0x02 0x7f 0x50 0x3c 0x9f 0xa8 0x07 0x51 0xa3 0x40 0x8f 0x92 0x9d 0x38 0xf5 0xbc 0xb6 0xda 0x21 0x10 0xff 0xf3 0xd2 0x08 0xcd 0x0c 0x13 0xec 0x5f 0x97 0x44 0x17 0xc4 0xa7 0x7e 0x3d 0x64 0x5d 0x19 0x73 0x09 0x60 0x81 0x4f 0xdc 0x22 0x2a 0x90 0x88 0x46 0xee 0xb8 0x14 0xde 0x5e 0x0b 0xdb 0x0a 0xe0 0x32 0x3a 0x0a 0x49 0x06 0x24 0x5c 0xc2 0xd3 0xac 0x62 0x91 0x95 0xe4 0x79 0x0b 0xe7 0xc8 0x37 0x6d 0x8d 0xd5 0x4e 0xa9 0x6c 0x56 0xf4 0xea 0x65 0x7a 0xae 0x08 0x0c 0xba 0x78 0x25 0x2e 0x1c 0xa6 0xb4 0xc6 0xe8 0xdd 0x74 0x1f 0x4b 0xbd 0x8b 0x8a 0x0d 0x70 0x3e 0xb5 0x66 0x48 0x03 0xf6 0x0e 0x61 0x35 0x57 0xb9 0x86 0xc1 0x1d 0x9e 0x0e 0xe1 0xf8 0x98 0x11 0x69 0xd9 0x8e 0x94 0x9b 0x1e 0x87 0xe9 0xce 0x55 0x28 0xdf 0x0f 0x8c 0xa1 0x89 0x0d 0xbf 0xe6 0x42 0x68 0x41 0x99 0x2d 0x0f 0xb0 0x54 0xbb 0x16 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0x0a 0x0b 0x0c 0x0d 0x0e 0x0f 0x00 0x52 0x09 0x6a 0xd5 0x30 0x36 0xa5 0x38 0xbf 0x40 0xa3 0x9e 0x81 0xf3 0xd7 0xfb 0x01 0x7c 0xe3 0x39 0x82 0x9b 0x2f 0xff 0x87 0x34 0x8e 0x43 0x44 0xc4 0xde 0xe9 0xcb 0x02 0x54 0x7b 0x94 0x32 0xa6 0xc2 0x23 0x3d 0xee 0x4c 0x95 0x0b 0x42 0xfa 0xc3 0x4e 0x03 0x08 0x2e 0xa1 0x66 0x28 0xd9 0x24 0xb2 0x76 0x5b 0xa2 0x49 0x6d 0x8b 0xd1 0x25 0x04 0x72 0xf8 0xf6 0x64 0x86 0x68 0x98 0x16 0xd4 0xa4 0x5c 0xcc 0x5d 0x65 0xb6 0x92 0x05 0x6c 0x70 0x48 0x50 0xfd 0xed 0xb9 0xda 0x5e 0x15 0x46 0x57 0xa7 0x8d 0x9d 0x84 0x06 0x90 0xd8 0xab 0x00 0x8c 0xbc 0xd3 0x0a 0xf7 0xe4 0x58 0x05 0xb8 0xb3 0x45 0x06 0x07 0xd0 0x2c 0x1e 0x8f 0xca 0x3f 0x0f 0x02 0xc1 0xaf 0xbd 0x03 0x01 0x13 0x8a 0x6b 0x08 0x3a 0x91 0x11 0x41 0x4f 0x67 0xdc 0xea 0x97 0xf2 0xcf 0xce 0xf0 0xb4 0xe6 0x73 0x09 0x96 0xac 0x74 0x22 0xe7 0xad 0x35 0x85 0xe2 0xf9 0x37 0xe8 0x1c 0x75 0xdf 0x6e 0x0a 0x47 0xf1 0x1a 0x71 0x1d 0x29 0xc5 0x89 0x6f 0xb7 0x62 0x0e 0xaa 0x18 0xbe 0x1b 0x0b 0xfc 0x56 0x3e 0x4b 0xc6 0xd2 0x79 0x20 0x9a 0xdb 0xc0 0xfe 0x78 0xcd 0x5a 0xf4 0x0c 0x1f 0xdd 0xa8 0x33 0x88 0x07 0xc7 0x31 0xb1 0x12 0x10 0x59 0x27 0x80 0xec 0x5f 0x0d 0x60 0x51 0x7f 0xa9 0x19 0xb5 0x4a 0x0d 0x2d 0xe5 0x7a 0x9f 0x93 0xc9 0x9c 0xef 0x0e 0xa0 0xe0 0x3b 0x4d 0xae 0x2a 0xf5 0xb0 0xc8 0xeb 0xbb 0x3c 0x83 0x53 0x99 0x61 0x0f 0x17 0x2b 0x04 0x7e 0xba 0x77 0xd6 0x26 0xe1 0x69 0x14 0x63 0x55 0x21 0x0c 0x7d Sign up for free to join this conversation on GitHub. Already have an account Sign in to comment,No Headline,Extremely Negative,580,"['attacks', 'vulnerable', 'issue', 'arbitrary', 'dump', 'Leaking', 'dead', 'odd', 'broken', 'measly', 'tedious', 'warning', 'inaccurate', 'bad', 'vulnerable', 'limited', 'problem', 'worse', 'rough', 'breach', 'breaking', 'breach', 'suffered', 'lost', 'lost', 'troubling', 'malicious', 'bad', 'attacks']","['killer', 'abuse']","[Mike, Metzger, Drew, Griess, Ivan, Kwiatkowski]"
6,Some Remarks on Large Language Models – Yoav Goldberg,"GitHub Gist: instantly share code, notes, and snippets.",https://gist.github.com/yoavg/59d174608e92e845c8994ac2e234c8a9,Some remarks on Large Language Models Yoav Goldberg January 2023 Audience I assume you heard of chatGPT maybe played with it a little and was imressed by it or tried very hard not to be. And that you also heard that it is a large language model. And maybe that it solved natural language understanding. Here is a short personal perspective of my thoughts of this and similar models and where we stand with respect to language understanding. Intro Around 20142017 right within the rise of neuralnetwork based methods for NLP I was giving a semiacademicsemipopsci lecture revolving around the story that achieving perfect language modeling is equivalent to being as intelligent as a human. Somewhere around the same time I was also asked in an academic panel what would you do if you were given infinite compute and no need to worry about labour costs to which I cockily responded I would train a really huge language model just to show that it doesnt solve everything. Well this response aged badly or did it And how does it coexist with my perfectlanguagemodelingasintelligence story I was also telling at the same time Perfect Language Modeling is AIComplete My popscimeetsintroNLP talk teaching computers to understand language was centered around Claude Shannons guessing game and the idea of language modeling. It started with AIforgames then quickly switched to a different kind of game invented by Shannon in 1951 the game of guessing the next letter. The game operator chooses some text and a cutting point within the text and hides the end. The players need to guess the first hidden letter in the smallest number of gueses. I gave a few examples of this game that demonstrate various kinds of linguistic knowledge that are needed in order to perform well in it at different levels of linguistic understanding from morphology through various levels of syntax smantics pragmatics and sociolinguistics. And then I said that humans are great at this game without even practicing and thats it is hard for them to get better at it which is why they find it to be a notgreat game. I then said that computers kinda suck at this game compared to humans but that by teaching them to play it we gain a lot of implicit knowledge of language. And that there is a long way to go but that there was some steady progress this is how machine translation works today I also said that computers are still not very good and that this is understandable the game is AIcomplete really playing the game at human level would mean solving every other problem of AI and exhibiting humanlike intelligence. To see why this is true consider that the game entails completing any text prefix including very long ones including dialogs including every possible conversation prefix including every description of experience that can be expressed in human language including every answer to every question that can be asked on any topic or situation including advanced mathmatics including philosophy and so on. In short to play it well you need to understand the text understand the situation described in the text imagine yourself in the situation and then to respond. It really mimicks the human experience and thought. Yes there could be several objections to this argument for example humans may also need to ask questions about images or scenes or other perceptual inputs that the model cannot see. But I think you get the point. So that was the the story I told about Shannons guessing game aka language modeling and how playing it at human level entails human level intelligence. Building a large language model wont solve everything anything Now if obtaining the ability of perfect language modeling entails intelligence AIcomplete why did I maintain that building the largest possible language model wont solve everything and was I wrong The answer is that I didnt think building a very large language model based on the thenexisting tech which was then just shifting between RNNsLSTMs and the Transformer will get us nowhere even close to having perfect language modeling. Was I wrong sort of. I was definitely surprised by the abilities demonstrated by large language models. There turned out to be a phase shift somewhere between 60B parameters and 175B parameters that made language models super impressive. They do a lot more than what I thought a language model trained on text and based on RNNsLSTMsTransformers could ever do. They certainly do all the things I had in mind when I cockily said they will not solve everything. Yes currentday language models first release of chatGPT did solve all of the things in the set of language understanding problems I was implicitly considering back then. So in that sense I was wrong. But in another sense no it did not solve everything. Not yet at least. Also the performance of current day languagemodels is not obtained only by language modeling in the sense I had in mind then. I think this is important and I will elaborate on it a bit soon. In what follows I will briefly discuss the difference I see between currentdayLMs and what was then perceived to be an LM and then briefly go through some of the things I think are not yet solved by the large LMs. I will also mention some arguments that I find to be correct but irrelevant uninteresting. Natural vs Currated Lanagueg Modeling What do I mean by The performance of current days language models are not obtained by language modeling The first demonstration of large language models lets say at the 170B parameters level ala GPT3 was to the best of our knowledge trained on naturally occuring text data text found in books crawled from the internet found in social networks etc. Later replications BLOOM OPT also used similar data. This is very close to Shannons game and also what most people in the past few decades thought of as language modeling. These models already brought remarkable performance. But chatGPT is different. Whats different in chatGPT There are three conceptual steps between GPT3 and chatGPT Instructions code RLHF. The last one is I think the least interesting despite getting the most attention but all are interesting. Heres my handwavy explanation. Maybe some day I will turn it into a more formal argument. I hope you get an intuition out of it though. Training on text alone like a traditional language model does have some clear theoretical limitations. Most notably it doesnt have a connection to anything external to the text and hence cannot have access to meaning or to communicative intent. Another way to say it is that the model is not grounded. The symbols the model operates on are just symbols and while they can stand in relation to one another they do not ground to any realworld item. So we can know the symbol blue but there is no realworld concept behind it. In instruction tuning the model trainers stopped training on just found data and started trainign on also on specific humancreated data this is known in machinelearning circles as supervised learning e. g. learning from annotated examples in addition to the found one. For example the human annotators would write something like please summarize this text followed by some text they got followed by a summary they produced of this text. Or they may write translate this text into a formal language followed by some text followed by formal language. They would create many instructions of these kind many summaries many translations etc for many different tasks. And then these will be added to the models training data. Why is this significant At the core the model is still doing language modeling right learning to predict the next word based on text alone Sure but here the human annotators inject some level of grounding to the text. Some symbols summarize translate formal are used in a consistent way together with the concepttask they denote. And they always appear in the beginning of the text. This make these symbols or the instructions in some loose sense external to the rest of the data making the act of producing a summary grounded to the human concept of summary. Or in other words this helps the model learn the communicative intent of the a user who asks for a summary in its instruction. An objection here would be that such cases likely naturally occur already in large text collections and the model already learned from them so what is new here I argue that it might be much easier to learn from direct instructions like these than it is to learn from noninstruction data think of a direct statement like this is a dog vs needing to infer from overhearing people talk about dogs. And that by shifting the distribution of the training data towards these annotated cases substantially alter how the model acts and the amount of grounding it has. And that maybe with explicit instructions data we can use much less training text compared to what was needed without them. I promised you hand waving didnt I Additionally the latest wave of models is also trained on programming language code data and specifically data that contains both natural language instructions or descriptions in the form of code comments and the corresponding programming language code. Why is this significant This produced another very direct form of grounding. Here we have two separate systems in the stream of text one of them is the human language and the other is the programming language. And we obsere the direct interaction between these two systems the human language describes concepts or intents which are then realized in the form of the corresponding programs. This is now a quite explicit form to meaning pairing. We can certainly learn more from it than what we could learn from form alone. Also I hypothesize that latest models are also trained on execution pairs of programs and their outputs. This is an even stronger form of grounding denotations. This is now very far from just language modeling. Finally RLHF or RL with Human Feedback. This is a fancy way of saying that the model now observes two humans in a conversation one playing the role of a user and another playing the role of the AI demonstrating how the AI should respond in different situations. This clearly helps the model learn how dialogs work and how to keep track of information across dialog states something that is very hard to learn from just found data. And the instructions to the humans are also the source of all the It is not appropriate to... and other formulaic templatic responses we observe from the model. It is a way to train to behave nicely by demonstration. ChatGPT has all three of these if not more. This is why I find it to be very different from traditional language models why it may not obey some of the limitations we or I expect to from language models and why it performs so much better on many tasks it is a supervised model with access to an external modality which is also trained explicitly by demonstration to follow a large set of instructions given in dialog form. Whats still missing Commonyetboring arguments There are a bunch of commonly occuring arguments about language models which I find to be true but uninspiring irrelevant to my discussion here They are wasetful training them is very expensive using them is very expensive. Yes this is certainly true today. But things get cheaper over time. Also lets put things in perspective yes it is enviromentally costly but we arent training that many of them and the total cost is miniscule compared to all the other energy consumptions we humans do. And I am also not sure what the environmental argument has to to with the questions of are these things interesting are these things useful etc. Its an economic question. The models encode many biasesand stereotypes. Well sure they do. They model observed humans language and we humans are terrible beings we are biased and are constantly stereotyping. This means we need to be careful when applying these models to realworld tasks but it doesnt make them less valid useful or interesting from a scientiic perspective. The models dontlanguage. reallyunderstand Sure. They dont. So what Lets focus on what they do manage to do and maybe try to improve where they dont These models will never reallyunderstand language. Again so what There are parts they clearly cover very well. Lets look at those Or dont look at them if you dont care about these aspects. Those who want to really understandlanguage may indeed prefer to look elsewhere. I am happy with approximate understanding. Again so what There are parts they clearly cover very well. Lets look at those Or dont look at them if you dont care about these aspects. Those who want to The models do not understand language like humans do. Duh they are not humans Of course they differ in some of their mechanisms. They still can tell us a lot about language structure. And for what they dont tell us we can look elsewhere. You cannot learn anything meaningful based only on form But it is not trained only on form see section above. It only connects pieces its seen beforeaccording to some statistics.... And isnt it magnificentthat statistics can get you so far The large models connect things in a very powerful way. Also consider how many terribly wrongways there are to connect words and phrases from the corpus according to statistics. And how many such ways the models manage to avoid and somehow choose meaningful ones. I find this utterly remarkable.... And isnt it We do not know the effects these things may have on society This is true about any new tech new discovery. Lets find out. We can try and be careful about it. But that doesnt make the thing less interesting less effective less worthy of study. It just adds one additional aspect worth studying. The models dont cite their sources Indeed they dont. But... so what I can see why you would like that in certain kinds of applications and you certainly want the models to not bulshit you and maybe you want to be able to verifythat they dont bulshit you but these are all not really related to the core of what language models are this is not the right question to ask in my opinion. After all humans dont really cite their sources in the real sense we rarely attribute our knowledge to a specific single source and if we do we very often do it as a rationalization or in a very deliberate process of first finding a source and then citing it. This can be replicated. From an application perspective say if we want to develop a search system or a paper writing system or a generalpurpose question answering system people can certainly work on linking utterances to sources either through the generation process or in a postprocessing step or on setups where you first retrieve and then generate. And many people do. But this is not really related to language understanding. What isinteresting though and what I find to be a more constructive thing to ask for is a how do we separate core knowledge about language and reasoning from specific factual knowledge about things and b how do we enable knowledge of knowledge see below. Indeed they dont. But... so what I can see why you would like that in certain kinds of applications and you certainly want the models to not bulshit you and maybe you want to be able to So whats missing what are some real limitations Here is an informal and incomplete list of things that I think are currently challenging in current large language models including the latest chatGPT and which hinders them from fully understanding language is some real sense. These are a bunch of things the models still cannot do or are at least very illequipped to do. Relating multiple texts to each other. In their training the models consume text either as one large stream or as independent pieces of information. They may pick up patterns of commonalities in the text but it has no notion of how the texts relate to events in the real world. In particular if the model is trained on multiple news stories about the same event it has no way of knowing that these texts all describe the same thing and it cannot differentiate it from several texts describing similar but unrelated events. In this sense the models cannot really form or are really not equipped to form a coherent and complete world view from all the text they read. A notion of time. Similarly the models dont have a notion of which events followother events in their training stream. They dont really have a notion of time at all besides maybe explicit mentions of time. So it may learn the local meaning of expressions like Obama became president in 2009 and reason about other explicitly dated things that happened before or after this. But it cannot understand the flow of time in the sense that if it reads in a separate text that Obama is the current president of the united state and yet in a third text that Obame is no longer the president which of these things follow one another and what is true now. It can concurrently believe that both Obame is the current president of the US Trump is the current president of the US and Biden is the current president of the US are all valid statements. Similarly it really has no practical way of interpreting statments like X is the latest album by Y and how they stand in relation to each other. Knowledge of KnowledgeThe models dont really know what they know. They dont even know what knowing is. All they do is guess the next token in a stream and this next token guess may be based on either well founded acuired knowledge or it may be a complete guess. The models training and training data have no explicit mechanism for distingushing these two cases and certainly dont have explicit mechanisms to act differently according to them. This is manifested in the well documented tendency to confidently make stuff up. The learningfromdemonstraiton RLHF made the models aware that some answers should be treated with caution and maybe the models even learned to associate this level of caution with the extent to which some fact entity or topic were covered in their training data or the extent to which the data is reflected in their internal weights. So in that sense they exhibit someknowledge of knowledge. But when they get over this initial stage of refuding to answer and go into text generation mode they lose all such knowledge of knowledge and very quickly tranistion into making stuff up mode also on things that it clearly stated in a different stage that is has no knowledge of. Numbers and mathThe models are really illequipped to perform math. Their basic building blocks are word pieces which dont really correspond to numbers in any convenient base. They also dont have any appropriate way of learning the relations between different numebrs such as the 1 or greater than relations in any meaningful and consistent way. LLMs manage to perform semiadequately on some questions involving numbers but really there are so much betterways to go about representing numbers and math than the mechanisms we gave LLM that it is surprising they can do anything at all. But I suspect they will not get very far without some more explicit modeling. Rare events high recall setups high coverage setupsBy their nature the models focus on the common and probable cases. This makes me immediately suspicious about their ability to learn from rare events in the data or to recall rare occurances or to recall all occurances. Here I am less certain than in the other points they might be able to do it. But I am currently skeptical. Data hungerThis is perhaps the biggest technical issueI see with current large language models they are extremely data hungry. To achieve their impressive performance they were trained on trillions of words. The obvious... and humans learn from a tiny fraction of this is of course true but not very interesting to me on its own so what we dont have to mimick humans to be useful. There are other implications though that I find to be very disturbing Most human languages dont have so much data certainly not data vailable in digital form. Why is this significantBecause it means that we will be hardpressed to replicate the incredible English understanding results that we have now for other languages such as my native language Hebrew or even to more common ones like German French or Arabic or even Chinese or Hindi I dont even consider so called low resource language like the many african and phillipinian ones. We can get a lot of data in these language but not so muchdata. Yes with instruct training we may need less data. But then the instruction data needs to be created this is a huge undertaking for each new language we want to add. Additionally if we believe and I do that training on code language is significant this is another hugebarrirer for achieving similar models for languages other than English. Cant this be solved by translationafter all we have great progress also in machine translation. We can translate to English run the model there and then translate back. Well yes we could. But this will work only at a very superficial level. Different languages come from different geographical regions and these regions have their local cultures norms stories events and so on. These differ from the cultures norms stories and events of English speaking geographies in various ways. Even a simple concept such as a city differs across communities and geographies not to mention concepts such as politeness or violence. Or just factual knowledge about certain people historic events significant places plants customs etc. These will not be reflected in the English training data and cannot be covered by translation. So data hunger isa real problem if we consider that we may want to have language understanding and AI technologies also outside of English. For those of us who want to worry about social implications this combination of datahunger and EnglishUScentrality is definitely a huge issue to consider. ModularityAt the end of the common yet boring arguments section above I asked how do we separate core knowledge about language and reasoning from specific factual knowledge about things. I think this is a major question to ask and that solving it will go a long way towards making progress if not solving many of the other issues. If we can modularize and separate the core language understanding and reasoning component from the knowledge component we may be able to do much better w. r. t to the datahunger problem and the resulting cultural knowledge gaps we may be able to better deal with and control biases and stereotypes we may get knowledgeofknowledge almost for free. Many people are working on retrieval augmented language models. This may or may not be the right way to approach this problem. I tend to suspect there is a more fundamental approach to be found. But history has proven I dont have great intuitions about such things. Conclusion Large language models are amazing. Language modeling is not enough but current language models are actually more than language models and they can do much more than we expected. This is still not enough though if we care about inclusive language understanding and also if we dont.,No Headline,Extremely Negative,190,[],[],"[juliusgeo, juliusgeo]"
7,Tencent WeChat is now a GitHub secret scanning partner,Tencent WeChat is now a GitHub secret scanning partner,https://github.blog/changelog/2022-12-19-tencent-wechat-is-now-a-github-secret-scanning-partner/,Secret scanning alerts for third party API key detections now include a link to relevant documentation provided by the service provider where available. These links are intended to help users better understand detections and take appropriate action. The links will appear in the alert view for all repositories with secret scanning enabled. You can enable secret scanning on your public repositories and any private repository with GitHub Advanced Security. If you have feedback on any provided links please write us a note in our code security discussion. For more information,No Headline,Extremely Negative,710,"['hard', 'worry', 'badly', 'hard', 'problem', 'objections', 'wrong', 'wrong', 'problems', 'wrong', 'irrelevant', 'limitations', 'loose', 'objection', 'hard', 'limitations', 'irrelevant', 'expensive', 'expensive', 'costly', 'terrible', 'biased', 'terribly', 'utterly', 'limitations', 'incomplete', 'challenging', 'lose', 'suspect', 'suspicious', 'skeptical', 'disturbing', 'superficial', 'problem', 'worry', 'issue', 'boring', 'issues', 'problem', 'biases', 'problem', 'suspect']","['suck', 'violence']","[Yoav, Goldberg, Claude, Shannons, Shannon, Obama, Obama, Obame, Obame, Trump, Biden, Knowledge, of]"
8,"Okta says its GitHub account hacked, source code stolen","In a 'confidential' email notification sent by Okta and seen by BleepingComputer, the company states that attackers gained access to its GitHub repositories this month and stole the company's source code.",https://www.bleepingcomputer.com/news/security/oktas-source-code-stolen-after-github-repositories-hacked/,Okta a leading provider of authentication services and Identity and Access Management IAM solutions says that its private GitHub repositories were hacked this month. According to a confidential email notification sent by Okta and seen by BleepingComputer the security incident involves threat actors stealing Oktas source code. BleepingComputer has obtained a confidential security incident notification that Okta has been emailing to its security contacts as of a few hours ago. We have confirmed that multiple sources including IT admins have been receiving this email notification. Earlier this month GitHub alerted Okta of suspicious access to Oktas code repositories states the notification. Upon investigation we have concluded that such access was used to copy Okta code repositories writes David Bradbury the companys Chief Security Officer CSO in the email. Despite stealing Oktas source code attackers did not gain unauthorized access to the Okta service or customer data says the company. Oktas HIPAA FedRAMP or DoD customers remain unaffected as the company does not rely on the confidentiality of its source code as a means to secure its services. As such no customer action is needed. At the time of writing our report the incident appears to be relevant to Okta Workforce Identity Cloud WIC code repositories but not Auth0 Customer Identity Cloud product given the email wording. An excerpt from the remainder of the notification reviewed by BleepingComputer is published below As soon as Okta learned of the possible suspicious access we promptly placed temporary restrictions on access to Okta GitHub repositories and suspended all GitHub integrations with thirdparty applications. We have since reviewed all recent access to Okta software repositories hosted by GitHub to understand the scope of the exposure reviewed all recent commits to Okta software repositories hosted with GitHub to validate the integrity of our code and rotated GitHub credentials. We have also notified law enforcement. Additionally we have taken steps to ensure that this code cannot be used to access company or customer environments. Okta does not anticipate any disruption to our business or our ability to service our customers as a result of this event. Note The security event pertains to Okta Workforce Identity Cloud WIC code repositories. It does not pertain to any Auth0 Customer Identity Cloud products. We have decided to share this information consistent with our commitment to transparency and partnership with our customers. While ending its confidential email that pledges a commitment to transparency Okta says it will publish a statement today on its blog. BleepingComputer reached out to Okta with questions in advance of publishing but a reply was not immediately available. Its been a difficult year for Okta with its series of security incidents and bumpy disclosures. September this year Oktaowned Auth0 disclosed a similarstyle incident. According to the authentication service provider older Auth0 source code repositories were obtained by a thirdparty individual from its environment via unknown means. But Oktas problems began long before amid the irregularity surrounding the disclosure of its January hack. March this year data extortion group Lapsus claimed it had access to Oktas administrative consoles and customer data as it began posting screenshots of the stolen data on Telegram. After stating that it was investigating these claims Okta shortly acknowledged that the hack being referred to had in fact occurred late January 2022 and potentially affected 2. 5 of its customers. This figure was estimated to be roughly 375 organizations at the time given Oktas 15000 customer base back then. The same week Okta admitted that it had made a mistake in delaying the disclosure of this hack that the firm said had originated at its thirdparty contractor Sitel Sykes. In April Okta clarified that the January breach had lasted 25 consecutive minutes and the impact was significantly smaller than what was originally anticipated limited to just two customers. A security company keeping their critical code connected to the web... stooopid. Not a member yet Register Now,Tencent Weixin now partners with GitHub to notify users if their credentials are exposed in a public repository,Extremely Positive,0,[],[],[]
9,Automatic1111 is back on GitHub after removing Embedding Links,Stable Diffusion web UI. Contribute to AUTOMATIC1111/stable-diffusion-webui development by creating an account on GitHub.,https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion,Textual Inversion Pages 27 What is Textual Inversion Using pretrained embeddings Training embeddings Textual inversion tab Explanation for parameters Creating an embedding Preprocess Training an embedding filewords Third party repos Finding embeddings online Hypernetworks Dum Dum Guide Unload VAE and CLIP from VRAM when training Clone this wiki locally What is Textual Inversion Textual Inversion allows you to train a tiny part of the neural network on your own pictures and use results when generating new ones. In this context embedding is the name of the tiny bit of the neural network you trained. The result of the training is a. pt or a. bin file former is the format used by original author latter is by the diffusers library with the embedding in it. See original site for more details about what textual inversion is httpstextualinversion. github. io. Using pretrained embeddings Put the embedding into the embeddings directory and use its filename in the prompt. You dont have to restart the program for this to work. As an example here is an embedding of Usada Pekora I trained on WD1. 2 model on 53 pictures 119 augmented for 19500 steps with 8 vectors per token setting. Pictures it generates portrait of usada pekora Steps 20 Sampler Euler a CFG scale 7 Seed 4077357776 Size 512x512 Model hash 45dee52b You can combine multiple embeddings in one prompt portrait of usada pekora mignon Steps 20 Sampler Euler a CFG scale 7 Seed 4077357776 Size 512x512 Model hash 45dee52b Be very careful about which model you are using with your embeddings they work well with the model you used during training and not so well on different models. For example here is the above embedding and vanilla 1. 4 stable diffusion model portrait of usada pekora Steps 20 Sampler Euler a CFG scale 7 Seed 4077357776 Size 512x512 Model hash 7460a6fa Training embeddings Textual inversion tab Experimental support for training embeddings in user interface. create a new empty embedding select directory with images train the embedding on it the feature is very raw use at own risk i was able to reproduce results I got with other repos in training anime artists as styles after few tens of thousands steps works with half precision floats but needs experimentation to see if results will be just as good if you have enough memory safer to run with nohalf precision full Section for UI to run preprocessing for images automatically. you can interrupt and resume training without any loss of data except for AdamW optimization parameters but it seems none of existing repos save those anyway so the general opinion is they are not important no support for batch sizes or gradient accumulation it should not be possible to run this with lowvramand medvramflags. Explanation for parameters Creating an embedding Name filename for the created embedding. You will also use this text in prompts when referring to the embedding. Initialization text the embedding you create will initially be filled with vectors of this text. If you create a one vector embedding named zzzz1234 with tree as initialization text and use it in prompt without training then prompt a zzzz1234 by monet will produce same pictures as a tree by monet. Number of vectors per token the size of embedding. The larger this value the more information about subject you can fit into the embedding but also the more words it will take away from your prompt allowance. With stable diffusion you have a limit of 75 tokens in the prompt. If you use an embedding with 16 vectors in a prompt that will leave you with space for 75 16 59. Also from my experience the larger the number of vectors the more pictures you need to obtain good results. Preprocess This takes images from a directory processes them to be ready for textual inversion and writes results to another directory. This is a convenience feature and you can preprocess pictures yourself if you wish. Source directory directory with images Destination directory directory where the results will be written Create flipped copies for each image also write its mirrored copy Split oversized images into two if the image is too tall or wide resize it to have the short side match the desired resolution and create two possibly intersecting pictures out of it. Use BLIP caption as filename use BLIP model from the interrogator to add a caption to the filename. Training an embedding Embedding select the embedding you want to train from this dropdown. Learning rate how fast should the training go. The danger of setting this parameter to a high value is that you may break the embedding if you set it too high. If you see Loss nanin the training info textbox that means you failed and the embedding is dead. With the default value this should not happen. Its possible to specify multiple learning rates in this setting using the following syntax 0. 005100 1e31000 1e5 this will train with lr of 0. 005for first 100 steps then 1e3until 1000 steps then 1e5until the end. Dataset directory directory with images for training. They all must be square. Log directory sample images and copies of partially trained embeddings will be written to this directory. Prompt template file text file with prompts one per line for training the model on. See files in directory textualinversiontemplatesfor what you can do with those. Use style. txtwhen training styles and subject. txtwhen training object embeddings. Following tags can be used in the file name the name of embedding filewords words from the file name of the image from the dataset. See below for more info. Max steps training will stop after this many steps have been completed. A step is when one picture or one batch of pictures but batches are currently not supported is shown to the model and is used to improve embedding. if you interrupt training and resume it at a later date the number of steps is preserved. Save images with embedding in PNG chunks every time an image is generated it is combined with the most recently logged embedding and saved to imageembeddings in a format that can be both shared as an image and placed into your embeddings folder and loaded. Preview prompt if not empty this prompt will be used to generate preview pictures. If empty the prompt from training will be used. filewords filewords is a tag for prompt template file that allows you to insert text from filename into the prompt. By default files extension is removed as well as all numbers and dashes at the start of filename. So this filename 0000011a man in suit. png will become this text for prompt a man in suit. Formatting of the text in the filename is left as it is. Its possible to use options Filename word regex and Filename join string to alter the text from filename for example with word regex w and join string the file from above will produce this text a man in suit. regex is used to extract words from text and they are a man in suit and join string is placed between those words to create one text a man in suit. Its also possible to make a text file with same filename as image 0000011a man in suit. txt and just put the prompt text there. The filename and regex options will not be used. Third party repos I successfully trained embeddings using those repositories Other options are to train on colabs andor using diffusers library which I know nothing about. Finding embeddings online Github has kindly asked me to remove all the links here. Hypernetworks Hypernetworks is a novel get it concept for fine tuning a model without touching any of its weights. The current way to train hypernets is in the textual inversion tab. Training works the same way as with textual inversion. The only requirement is to use a very very low learning rate something like 0. 000005 or 0. 0000005. Dum Dum Guide An anonymous user has written a guide with pictures for using hypernetworks httpsrentry. orghypernetwork4dumdums Unload VAE and CLIP from VRAM when training This option on settings tab allows you to save some memoryat the cost of slower preview picture generation.,No Headline,Extremely Negative,460,"['threat', 'stealing', 'suspicious', 'stealing', 'suspicious', 'disruption', 'difficult', 'bumpy', 'unknown', 'problems', 'irregularity', 'hack', 'extortion', 'stolen', 'hack', 'mistake', 'delaying', 'hack', 'breach', 'limited', 'critical']",[],"['lawsuit', David, Bradbury, 'lawsuit']"
10,Slack's private GitHub code repositories stolen over holidays,Slack suffered a security incident over the holidays affecting some of its private GitHub code repositories.,https://www.bleepingcomputer.com/news/security/slacks-private-github-code-repositories-stolen-over-holidays/,Slack suffered a security incident over the holidays affecting some of its private GitHub code repositories. The immensely popular Salesforceowned IM app is used by an estimated 18 million users at workplaces and digital communities around the world. BleepingComputer has come across a security incident notice issued by Slack on December 31st 2022. The incident involves threat actors gaining access to Slacks externally hosted GitHub repositories via a limited number of Slack employee tokens that were stolen. While some of Slacks private code repositories were breached Slacks primary codebase and customer data remain unaffected according to the company. The wording from the notice 1 2 published on New Years eve is as follows On December 29 2022 we were notified of suspicious activity on our GitHub account. Upon investigation we discovered that a limited number of Slack employee tokens were stolen and misused to gain access to our externally hosted GitHub repository. Our investigation also revealed that the threat actor downloaded private code repositories on December 27. No downloaded repositories contained customer data means to access customer data or Slacks primary codebase. Slack has since invalidated the stolen tokens and says it is investigating potential impact to customers. At this time there is no indication that sensitive areas of Slacks environment including production were accessed. Out of caution however the company has rotated the relevant secrets. Based on currently available information the unauthorized access did not result from a vulnerability inherent to Slack. We will continue to investigate and monitor for further exposure states Slacks security team. Ironically the security update speaks of Slack taking your security privacy and transparency very seriously and yet comes with some caveats. For starters this news item doesnt appear on the companys international news blog aside other articles at the time of writing. Additionally contrary to Slacks earlier blog posts this update when accessed in some regions e. g. UK is marked with noindexan HTML feature that is used to exclude a webpage from search engine results thereby making it harder to discover the page. BleepingComputer further observed that the meta tag containing the noindex attribute was itself placed towards the bottom within the pages HTML code in an elongated line that overflows without breaking. This means those viewing the source code like us wouldnt readily get to see the buried tag unless they actively searched CtrlF the source code for it. Per convention HTML head and meta tags are typically placed at the top of a page. We noticed though Google has already indexed the U. S. advisory published without the tag. Other techniques employed by businesses looking to limit the visibility of uncanny news may include the use of geofencing and tailoring the robots. txt file. Such techniques including the use of noindex in important announcements are typically frowned upon. In some cases though noindex attribute may be erroneously applied when the aim was to achieve generating canonical links. Last year infosec reporter and editor Zack Whittaker called out LastPass and GoTo for employing similar tactics with LastPass 2022 security breach disclosure. In August 2022 Slack reset user passwords after accidentally exposing the password hashes in a separate incident. Unsurprisingly that particular notice is also marked with a noindex both the U. S. and international versions. In 2019 Slack announced it had reset passwords for about 1 of users impacted by the 2015 data breach who additionally met a set criteria. The good news with regards to the most recent security update is that no action needs to be taken by customers for now. Not a member yet Register Now,No Headline,Extremely Negative,310,"['risk', 'interrupt', 'loss', 'limit', 'Split', 'danger', 'break', 'Loss', 'dead', 'object', 'interrupt', 'slower']",[],"[Usada, Pekora, usada, pekora, usada, pekora, mignon, usada, pekora]"
11,Github degraded Copilot for students with no warning,Select Topic Area Question Body I am a verified teacher with free access to Copilot. I keep getting the error rate limit exceeded for plan FREE_FACULTY today. It was working fine yesterday. Is ther...,https://github.com/orgs/community/discussions/43673,Beta Was this translation helpful Give feedback. I got an answer from the GitHub support team. I hope this becomes true. Beta Was this translation helpful Give feedback. Beta Was this translation helpful Give feedback. Beta Was this translation helpful Give feedback.,No Headline,Extremely Negative,430,"['Slack', 'suffered', 'Slack', 'threat', 'limited', 'Slack', 'stolen', 'suspicious', 'limited', 'Slack', 'stolen', 'threat', 'Slack', 'stolen', 'Slack', 'Ironically', 'Slack', 'breaking', 'limit', 'erroneously', 'breach', 'Slack', 'Slack', 'breach']",[],"[Zack, Whittaker]"
12,Querying the GitHub archive with the ClickHouse playground,Via [this comment](https://news.ycombinator.com/item?id=34197637) on Hacker News I started exploring the [ClickHouse Playground](https://clickhouse.com/docs/en/getting-started/playground/). It's reall,https://til.simonwillison.net/clickhouse/github-explorer,Via this comment on Hacker News I started exploring the ClickHouse Playground. Its really cool and among other things it allows CORSenabled API hits that can query a decade of history from the GitHub events archive in less than a second. ClickHouse is an open source columnoriented database originally developed at Yandex but spun out into a separate VCfunded company in 2021. Its designed for big data analytical queries in a similar space to HBase BigQuery and DuckDB. It turns out it can do that trick with HTTP range queries where you can point it at the URL to a Parquet or. native. zst file ClickHouse native format optionally compressed using Facebook Zstandard and run queries without downloading the entire file first. The ClickHouse Playground is a free hosted environment for trying out ClickHouse. You can access it here httpsplay. clickhouse. complayuserplay Try this query taken from the ClickHouse Everything You Always Wanted To Know About GitHub But Were Afraid To Ask tutorial SELECT count FROM githubevents WHERE eventtype WatchEvent The githubevents table contains a copy of the GH Archive a project that archives and makes available the public GitHub timeline I think using data from the public events API. GH Archive then makes that data available as compressed newlinedelimited JSON in this bucket. The archive stretches back to February 2011 and is constantly updated. The ClickHouse demo table is continually updated with the latest archived data by this script which runs every 10 minutes. You can do all sorts of fun stuff with it. Heres my recent activity acrosss all of GitHub SELECT createdat actorlogin reponame eventtype title FROM githubevents WHERE actorlogin simonw AND reponame simonwdisasterdata ORDER BY createdat DESC LIMIT 100 This link executes that query note how it includes a base64 encoded copy of the SQL query following the in the URL. There are 77 tables total in the Playground instance you can get a list of them like this SELECT database name FROM system. tables You can access the API via curl like this curl httpsplay. clickhouse. com X POST H Authorization Basic cGxheTo dataraw SELECT createdat actorlogin reponame FROM githubevents WHERE eventtype WatchEvent ORDER BY createdat DESC LIMIT 100 This defaults to returning TSV without column headers like this 20230101 035959 Willmac16 nlohmannjson 20230101 035959 SamroseAhmed Stebalienstashrs 20230101 035957 CodePromoter aplusframeworkimage To get back data in JSON instead add defaultformatJSON to the URL. Here Im piping that through jq to pretty print it curl httpsplay. clickhouse. comdefaultformatJSON X POST H Authorization Basic cGxheTo dataraw SELECT createdat actorlogin reponame FROM githubevents WHERE eventtype WatchEvent ORDER BY createdat DESC LIMIT 1 jq Output meta name createdat type DateTime name actorlogin type LowCardinalityString name reponame type LowCardinalityString data createdat 20230101 035959 actorlogin Willmac16 reponame nlohmannjson rows 1 rowsbeforelimitatleast 341429632 statistics elapsed 0. 925636889 rowsread 341540363 bytesread 10567093585 More format options are documented here. This pattern works for running queries from JavaScript. CORS is enabled I pasted this into the Firefox DevTools console on httpswww. example. com and it returned the results I expected r await fetchhttpsplay. clickhouse. comuserplay method POST body SELECT createdat eventtype actorlogin reponame number title body FROM githubevents WHERE actorlogin simonw ORDER BY createdat desc LIMIT 100 FORMAT JSON d await r. json Here Im using FORMAT JSON at the end of the query itself and passing the requested user as userplay rather than sending an Authorization header. Created 20221231T2106140800 updated 20230101T1037420800 History Edit,No Headline,Extremely Negative,190,[],[],[]
13,Embeddedipsec: IPsec for Embedded Systems,embedded IPsec - a lightweight IPsec implementation - GitHub - tinytux/embeddedipsec: embedded IPsec - a lightweight IPsec implementation,https://github.com/tinytux/embeddedipsec,This commit does not belong to any branch on this repository and may belong to a fork outside of the repository. master Switch branchestags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names so creating this branch may cause unexpected behavior. Are you sure you want to create this branch,Querying the GitHub archive with the ClickHouse playground,Somewhat Positive,50,"['trick', 'Afraid', 'LIMIT', 'LIMIT', 'LIMIT', 'LIMIT']",[],[]
14,Bash retry function with exponential backoff,"Bash retry function. GitHub Gist: instantly share code, notes, and snippets.",https://gist.github.com/sj26/88e1c6584397bb7c13bd11108a579746,This is free and unencumbered software released into the public domain. Anyone is free to copy modify publish use compile sell or distribute this software either in source code form or as a compiled binary for any purpose commercial or noncommercial and by any means. In jurisdictions that recognize copyright laws the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For more information please refer to httpsunlicense. org,No Headline,Extremely Negative,200,['unexpected'],[],[]
15,Show HN: Self Hosted OpenStreetMap using only Sqlite3 and Python,Reverse Geocode for OpenStreetmap. Contribute to punnerud/rgcosm development by creating an account on GitHub.,https://github.com/punnerud/rgcosm,RGCosm Reverse Geocode for OpenStreetmap Locally hosted OpenStreetmap using sqlite3 for reverse geocode. So you easily can find adresses based on coordinates. Download the pbf file from httpsdownload. geofabrik. de Then use convert. py to create the database python3 convert. py You have to change norwaylatest. osm. pbf in convert. py into your filename. The norwaylatest. osm. pbf is about 1GB and the sqlite3 end up 10GB. With indexes 16GB. So dont try with the biggest areas for starting. Takes about 15minutes for the norway file. To speed up your queries it is highly recommended to add indexes. This increase the size around 50 and takes a couple of minutes to create CREATE INDEX nodes index lat ON nodes lat CREATE INDEX nodes index lon ON nodes lon Adding indexes change the search time for my Norway file from 10 to 0. 15 seconds. Changing the lookaround query can also reduce the search time at the risk that you miss an adress if the closest adress is more far away. Mac users I found this to work for installation of osmium for Python brew install cmake brew install wheel brew install osmiumtool python3 m pip install osmium Premade sqlite3 database for Norway with indexes if you just want to try it httpswww. dropbox. coms4rrhxpfzulqbvxrosm. dbdl0,No Headline,Extremely Negative,300,"['detriment', 'LIMITED', 'LIABLE', 'DAMAGES', 'LIABILITY']",[],"['lawsuit', 'lawsuit']"
16,Stable Diffusion macOS native app,Stable Diffusion macOS native app. Contribute to justjake/Gauss development by creating an account on GitHub.,https://github.com/justjake/Gauss,Gauss A Stable Diffusion app for macOS built with SwiftUI and Apples mlstablediffusion CoreML models. Usage Write prompt text and adjust parameters in the composer view at the bottom. To export an image just drag it to Finder or any other image editor. You can always generate more images from an existing prompt. Project Status This software is under development and is prealpha quality there is no release for endusers yet. If youd like to contribute you can build the project by following the instructions for developers below. See DiffusionBee for a Stable Diffusion UI built with Electron Python but that works out of the box today. System requirements macOS 13. 1 Recommended an Apple Silicon CPU. Intel hardware may work but is untested by the primary developer and could be slow. Developer setup System requirements macOS 13. 1 Xcode 14. 2 gitlfsis used to fetch prebuilt models from HuggingFace. If you have brew brew install gitlfs then git lfs install. Otherwise download the installer here then git lfs install. If you have At least 10gb of free disk space. Recommended set up Xcode to format code when you save. Building from source Clone this repo git clone httpsgithub. comjustjakeGauss Inside the repo run make. This will download prebuild models from HuggingFace into. compiledmodels. You can run make j 3to download the models in parallel if you have a fast connetion. Eg cd Gauss make Open the project file Gauss. xcodeproj with Xcode open Gauss. xcodeproj You should be able to build CmdB and run the project. Packing zip files for release The current plan for releasing Gauss is to pack the models it needs into zips split the zips into parts no larger than 2gb and then publish everything via Github release. 2gb is the file size limit for Github releases. Well teach Gauss itself how to find download and reassemble the zip files from Github directly. To create the zip files make zips Publishing releases to Github TODO figure this out,No Headline,Extremely Negative,200,['risk'],[],[]
17,Advanced and easy YouTube archiver now stable,YouTube archiving made simple. Contribute to Owez/yark development by creating an account on GitHub.,https://github.com/Owez/yark,Yark YouTube archiving made simple. Installation Managing your Archive Viewing your Archive Yark lets you continuously archive all videos and metadata for YouTube channels and playlists. You can also view your archive as a seamless offline website Installation To install Yark simply download Python 3. 9 and FFmpeg optional then run the following pip3 install yark Managing your Archive Once youve installed Yark think of a name for your archive e. g. foobar and copy the channelplaylist url yark new foobar httpswww. youtube. comchannelUCSMdm6bUYIBN0KfS2CVuEPA Now that youve created the archive you can tell Yark to download all videos and metadata using the refresh command yark refresh foobar Once everything has been downloaded Yark will automatically give you a status report of whats changed since the last refresh Viewing your Archive Viewing you archive is easy just type view with your archives name yark view foobar This will pop up an offline website in your browser letting you watch all videos Under each video is a rich history report filled with timelines and graphs as well as a noting feature which lets you add timestamped and permalinked comments Light and dark modes are both available and automatically apply based on the systems theme. Details Here are some things to keep in mind when using Yark the good and the bad Dont create a new archive again if you just want to update it Yark accumulates all new metadata for you via timestamps Feel free to suggest new features via the issues tab on this repository Scheduling isnt a feature just yet please use cronor something similar Archive Format The archive format itself is simple and consists of a directorybased structure with a core metadata file and all thumbnailvideo data in their own directories as typical files name Your selfcontained archive yark. json Archive file with all metadata yark. bak Backup archive file to protect against data damage videos Directory containing all known videos id. Files containing video data for YouTube videos images Directory containing all known images e. g. thumbnails icons hash. png Files containing images with their BLAKE2 hash Its best to take a few minutes to familiarize yourself with your archive by looking at files which look interesting to you in it everything is quite readable.,No Headline,Extremely Negative,230,"['untested', 'slow', 'split', 'limit']",[],[Gauss]
18,Solving a Dungeons and Dragons riddle using Prolog,"Solving a Dungeons & Dragons riddle using prolog. GitHub Gist: instantly share code, notes, and snippets.",https://gist.github.com/Spuffynism/446c7c2d498477491d8137e8f234d4a9,Solving a Dungeons Dragons riddle using prolog Bringing back the magic of Christmas using the magic of prolog As part of a holiday DD oneshot session where Santa Clauss toy factory had been sabotaged our dungeon master presented to us a group of Christmas elves a riddle to solve. 9 cards labeled with the names of Santas reindeer were presented to us. The instructions indicated that we had to find the order reindeer were in according to this riddle Vixen should be behind Rudolph Prancer and Dasher whilst Vixen should be in front of Dancer and Comet. Dancer should be behind Donder Blitzen and Rudolph. Comet should be behind Cupid Prancer and Rudolph. Donder should be behind Comet Vixen Dasher Prancer and Cupid. Cupid should be in front of Comet Blitzen Vixen Dancer and Rudolph. Prancer should be in front of Blitzen Donder and Cupid. Blitzen should be behind Cupid but in front of Dancer Vixen and Donder. Rudolph should be behind Prancer but in front of Dasher Dancer and Donder. Finally Dasher should be behind Prancer but in front of Blitzen Dancer and Vixen. These sentences seem a lot like prolog facts to me Lets translate them to prolog facts and then use said facts to solve the riddle. Solution with the least knowledge about the problem First we declare facts for all relationships between reindeer as described in the riddle. Well use isbehindSecond First to describe when a Second reindeer is behind the First reindeer and the opposite for when a reindeer is in front of the other one Second is in front of First becomes isbehindFirst Second. Vixen should be behind Rudolph Prancer and Dasher isbehindvixen rudolph. isbehindvixen prancer. isbehindvixen dasher. whilst Vixen should be in front of Dancer and Comet. isbehinddancer vixen. isbehindcomet vixen. Dancer should be behind Donder Blitzen and Rudolph. isbehinddancer donder. isbehinddancer blitzen. isbehinddancer rudolph. Comet should be behind Cupid Prancer and Rudolph. isbehindcomet cupid. isbehindcomet prancer. isbehindcomet rudolph. Donder should be behind Comet Vixen Dasher Prancer and Cupid. isbehinddonder comet. isbehinddonder vixen. isbehinddonder dasher. isbehinddonder prancer. isbehinddonder cupid. Cupid should be in front of Comet Blitzen Vixen Dancer and Rudolph. isbehindcomet cupid. isbehindblitzen cupid. isbehindvixen cupid. isbehinddancer cupid. isbehindrudolph cupid. Prancer should be in front of Blitzen Donder and Cupid. isbehindblitzen prancer. isbehinddonder prancer. isbehindcupid prancer. Blitzen should be behind Cupid but in front of Dancer Vixen and Donder. isbehindblitzen cupid. isbehinddancer blitzen. isbehindvixen blitzen. isbehinddonder blitzen. Rudolph should be behind Prancer but in front of Dasher Dancer and Donder. isbehindrudolph prancer. isbehinddasher rudolph. isbehinddancer rudolph. isbehinddonder rudolph. Finally Dasher should be behind Prancer but in front of Blitzen Dancer and Vixen. isbehinddasher prancer. isbehindblitzen dasher. isbehinddancer dasher. isbehindvixen dasher. Using a rule we declare that if Last is behind Middle and Middle is behind First then Last is behind First. In other words by transitivity if x y and y z then x z. Well need it when testing if a sequence respects the reindeer order described by the facts. followsLast First isbehindLast First. followsLast First isbehindMiddle First followsLast Middle. A sequence respects the order if reindeer follow each other according to the facts. Lets specify that in a rule. respectsorderFirstSecond followsSecond First. respectsorderFirstSecondRest followsSecond First respectsorderSecondRest. Finally lets declare a rule that will find a valid solution. We first list all unique known reindeer. Then we compute possible reindeer sequence permutations. Lastly we find the first permutation thats a valid solution the one that respects the reindeer order. solutionPermutation findallReindeer isbehindReindeer isbehind Reindeer List listtosetList UniqueReindeer permutationUniqueReindeer Permutation oncerespectsorderPermutation. We can now query solution to find the reindeer sequence... solutionSequence Sequence prancer cupid rudolph dasher blitzen vixen comet donder dancer And there you have it the answer to our riddle We only needed a few elements Facts about the reindeer Rules to confirm that a sequence is valid A rule to test possible sequences Although this works great we are not taking full advantage of the power of prolog nor the knowledge we have about what a valid solution consists of. Other solution knowing there are 9 reindeer We can combine the knowledge that there are exactly 9 unique reindeer with this Prolog wiki solution for a similar logic puzzle to create a more succinct solution. Furthermore whereas in the previous solution we had to manually make the isbehind rule transitive by declaring the follows rule well depend on the transitivity of comparison operators less than and greater than in this solution. Well generate permutations by associating every reindeer to a unique number. Then well test the permutations against the riddle. First we list the reindeer with a list of possible positions for them 1 through 9. We use the permutation predicate to have prolog generate reindeer sequences permutation Vixen Rudolph Prancer Dasher Dancer Comet Donder Blitzen Cupid 1 2 3 4 5 6 7 8 9 Then we declare the riddle sentences as comparisons on free variables. Second First means that Second is behind First. First Second means that First is in front of Second. Vixen should be behind Rudolph Prancer and Dasher Vixen Rudolph Vixen Prancer Vixen Dasher whilst Vixen should be in front of Dancer and Comet. Vixen Dancer Vixen Comet Dancer should be behind Donder Blitzen and Rudolph. Dancer Donder Dancer Blitzen Dancer Comet Comet should be behind Cupid Prancer and Rudolph. Comet Cupid Comet Prancer Comet Rudolph Donder should be behind Comet Vixen Dasher Prancer and Cupid. Donder Comet Donder Vixen Donder Dasher Donder Prancer Donder Cupid Cupid should be in front of Comet Blitzen Vixen Dancer and Rudolph. Cupid Comet Cupid Blitzen Cupid Vixen Cupid Dancer Cupid Rudolph Prancer should be in front of Blitzen Donder and Cupid. Prancer Blitzen Prancer Donder Prancer Cupid Blitzen should be behind Cupid but in front of Dancer Vixen and Donder. Blitzen Cupid Blitzen Dancer Blitzen Vixen Blitzen Donder Rudolph should be behind Prancer but in front of Dasher Dancer and Donder. Rudolph Prancer Rudolph Dasher Rudolph Dancer Rudolph Donder Finally Dasher should be behind Prancer but in front of Blitzen Dancer and Vixen. Dasher Prancer Dasher Blitzen Dasher Dancer Dasher Vixen. Finally we put it together under a solution rule... solutionVixen Rudolph Prancer Dasher Dancer Comet Donder Blitzen Cupid permutationVixen Rudolph Prancer Dasher Dancer Comet Donder Blitzen Cupid 1 2 3 4 5 6 7 8 9 Vixen should be behind Rudolph Prancer and Dasher Vixen Rudolph Vixen Prancer Vixen Dasher... more comparison rules... and we query it solutionVixen Rudolph Prancer Dasher Dancer Comet Donder Blitzen Cupid Blitzen 5 Comet 7 Cupid 2 Dancer 9 Dasher 4 Donder 8 Prancer 1 Rudolph 3 Vixen 6 Sorting them by hand we get the answer to the riddle Prancer 1 Cupid 2 Rudolph 3 Dasher 4 Blitzen 5 Vixen 6 Comet 7 Donder 8 Dancer 9 In the first solution we defined transitivity of the isbehind rule using the follows rule. In this solution we equate reindeer to numbers and rely on the transitivity of comparison operators to arrive at the solution We didnt even need to write an algorithm to solve the riddle we only needed to declare the riddle constraints and then let prolog find a sequence that fits within said constraints,No Headline,Extremely Negative,230,"['dark', 'bad', 'issues', 'damage']",[],[]
19,"VanadiumOS: Portable, multi-user Unix-like OS",Vanadium OS. Contribute to p-durlej/newsys development by creating an account on GitHub.,https://github.com/p-durlej/newsys,Vanadium OS This is the complete source code of Vanadium OS v1. 4. The system is released under a BSD 2clause license. Running If you want to run the operating system without building it there are USB stick CDROM ISO and PXE images available for download at GitHub. You can install the operating system if you want but you dont have to. There is a live USBCDPXE mode. Building The current version of the operating system was tested to build successfully on Ubuntu 17. 10. Before the system can be compiled from source you should make sure that you have gmake cc and mkisofs in your PATH. If you intend to run the operating system using the scripts in the s directory provide a qemu command in your search path. Symbolic links to appropriate programs will suffice. Also zlib is is needed. If the host OS makes a distinction it should be the development version. Additionally to compile the GCC toolchain you must have the mpfr and gmp libraries. Again these should be the development versions. The source code should be placed at HOMEos386src. Other locations may work but were not tested recently. On Ubuntu 17. 10 you may install the required packages by running sinsthostpkgs as root. Building everything To start the build change the current working directory to HOMEos386src and run the smkall script. This should download the gcc and binutils source tarballs build and install the toolchain in HOMEos386tools then build i386 and amd64 releases and finally install the library and header files in the toolchain directories. The resultant release tarballs are placed in HOMEos386src. Building a single platform release Building the entire toolchain and the two platforms can be time consuming it takes 10 40 minutes on my machines so if you just want to build a single platform release you can run ARCHamd64 smkrelse in HOMEos386src. If you want to build the i386 release you can substitute i386 for amd64. Amd64 is the default so you can also omit ARCHamd64. Rebuilding after making changes If you just made a change in the source code and want to recompile just run the gmake command in HOMEos386src. It is no longer required to firstly source senv. sh. Of course the toolchain is required. Again amd64 is built by default. If you want to build i386 instead set ARCHi386 in the environment or run gmake ARCHi386. Program development API There is currently no API documentation available. Therefore the kernel driver and library source code is the documentation and the bundled applications are the examples Standard library functions such as printf act much like their standard counterparts from big operating systems. Nonetheless this is a hobby OS and some parts of the implementation are simplified. Some standard functions are reduced in functionality some do not exist at all. There is a race condition in open iirc. Vfprintf and the other printf functions do not support floating point conversions and math. h is missing from this version. Some functions are Vanadium OSspecific these can be figured out by reading the source code. The GUI API is completely made by up me there isnt any similar API documented anywhere. Again the source code is the documentation. Compiling programs Programs for Vanadium OS are built using the standard GNU tools. The executable prefix is either amd64os386elf or i386os386elf depending on the platform. So if you want to compile a singlefile program you can run amd64os386elfgcc o hello hello. c Just make sure the HOMEos386toolsbin directory is in your PATH.,No Headline,Extremely Negative,220,"['Dungeons', 'dungeon', 'problem']",[],"[Santa, Clauss, Vixen, Rudolph, Prancer, Dasher, Vixen, Dancer, Donder, Blitzen, Rudolph, Comet, Cupid, Prancer, Rudolph, Donder, Cupid, Cupid, Rudolph, Prancer, Blitzen, Cupid, Rudolph, Prancer, Dasher, Vixen, Dasher, Donder, Blitzen, Rudolph, Blitzen, Donder, Blitzen, Cupid, Donder, isbehindblitzen, cupid, blitzen, blitzen, isbehinddonder, blitzen, ., Rudolph, Dasher, Dancer, Donder, rudolph, rudolph, isbehinddonder, rudolph, ., Dasher, Prancer, Rudolph, Prancer, Vixen, Dasher, Vixen, Donder, Blitzen, Rudolph, Rudolph, Rudolph, Donder, Rudolph, Prancer, Cupid, Rudolph, Dasher, Rudolph, Donder, Rudolph, Prancer, Vixen, Dasher]"
20,Dweb: Offline Internet Archive,Offline Internet Archive project. Contribute to internetarchive/dweb-mirror development by creating an account on GitHub.,https://github.com/internetarchive/dweb-mirror,Offline Internet Archive Introduction to the Offline Internet Archive project The internet now seems like a utility available everywhere from our homes and offices to trains and planes. But utilitylevel access is not yet a reality for more than half of the worlds population who lack consistent or indeed any access to the Internet. Why Cost Internet access is unaffordable to people with low or no income. Connectivity In many developing countries and rural areas the infrastructure that enables internet access is unreliable slow or simply unavailable. Natural disasters uprisings and war compound the challenge. Censorship Some countries limit internet access for political reasons. Several countries block the Internet Archive. In some countries Facebook has become synonymous with the internet but it is hardly a substitute for free and open World Wide Web. The Internet Archive offers perhaps the worlds largest online store of open content. The wisdom of the ages just a few clicks away. As Wikipedia has become the worlds encyclopedia the Internet Archive has become its library. Central to our mission is establishing Universal Access to All Knowledge. Access to our library of millions of books journals audio and video recordings and beyond is free to anyone with one caveat the need for a reliable internet connection. Lack of access to todays internet is a significant factor in poorer educational outcomes intergenerational poverty and disempowerment as identified by the UN in their Sustainable Development Goal 9. The Offline Archive project works towards making online collections available regardless of internet availability. Part of the challenge is that those of us who live where the Internet works well are adding graphics video and other demands on bandwidth faster than access is being improved in many parts of the world. An evolving ecosystem is emerging to enable access over poorer internet. Typically the approaches build around low cost low power devices that can be installed in communities and schools for example and deliver content either offline or through better usage of a narrow pipe to the net. We have built an offline server that Crawls Internet Archive collections to a local server Serves that content locally Caches content while browsing Moves content between servers by sneakernet on disks USB sticks and SD cards Delivers mostly the Internet Archive UI offline in javascript in the browser Is open source And is being made available in other languages. The server is integrated into the InternetInABox IIAB platform and can be installed on top of the Rachel platform or hopefully any linux based platform. Our approach should improve access for anything from a US20 Raspberry Pi up to a server holding terabytes of data for an institution. We are also collaborating with other parts of the ecosystem integrating the Archives APIs with those of other partners to make it easier for them to incorporate Archive content. Contributing Wed love to have you contribute please email mitraarchive. org or interact with the rest of this repo and Ill figure out how to help you get started. TODO setup a better channel for this Installation If you would like to run the offline archive server then see INSTALLATION. md and the documents it points to. If you want to fix bugs develop code or contribute in other ways then see INSTALLATIONdev. md. Note this document was written for Mac OSX users a useful task would be for someone with a Linux machine to make any edits to it if required or just confirms it is correct. Also see these documents to update an existing installation Or to troubleshoot an existing installation. Using it starting the server. See the Installation docs but on most platforms except currently on Mac OSX the server should start at reboot. If not then assuming youve got it installed in your home directory... cd nodemodulesdwebmirror. internetarchive server Or a slightly different location for the developers. The startup is a little slow but youll see some debugging when its live On platforms where it starts automatically e. g. IIAB Rachel it can be turned on or off at a terminal window with service internetarchive start or service internetarchive stop Browsing Open the web page the address depends on the platform. httparchive. local4244 or httparchive4244 should work on any platform but this depends on the configuration of your LAN. If you know the IP address then http4244 will work On MacOSX or if using a browser on the RaspberryPiOrangePi httplocalhost4244 On Rachel try httprachel. local4244 or httprachel4244 or via the main interface at httprachel. local and click Internet Archive On IIAB The server can be accessed at httpbox4244 or httpbox. lan4244 try httpbox. local4244 via mDNS over a local network if you dont have name resolution set up to reach your InternetinaBox. Try walking through. USING. md to get a tour of the system and you can click Home or the Internet Archive logo if you just want to explore the Internet Archives resources. Administration Administration is carried out mostly through the same User Interface as browsing. Select local from any of the pages to access a display of local content. Administration tools are under Settings. Click on the Archive logo in the centertop to get the Internet Archive main interface if connected to the net. While viewing an item or collection the Crawl button in the top bar indicates whether the item is being crawled or not. Clicking it will cycle through three levels No crawling Details sufficient information will be crawled to display the page for a collection this also means getting the thumbnails and metadata for the top items. Full crawls everything on the item this can be a LOT of data including full size videos etc so use with care if bandwidthdisk is limited. Disk storage The server checks for caches of content in directories called archiveorg in all the likely places in particular it looks for any inserted USB drives on most systems and if none are found it uses archiveorg. The list of places it checks in an unmodified installation can be seen at httpsgithub. cominternetarchivedwebmirrorblobmasterconfigDefaults. yamlL7. You can override this in dwebmirror. config. yaml in the home directory of the user that runs the server. Note on IIAB this is currently in rootdwebmirror. config. yaml see Advanced below Archives Items are stored in subdirectories of the first of these directories found but are read from any of the locations. If your disk space is getting full its perfectly safe to delete any subdirectories except archiveorg. hashstore and the server will refetch anything else it needs next time you browse to the item while connected to the internet. It is also safe to move directories to an attached USB underneath a archiveorg directory at the top level of the disk It is also safe to move attached USBs from one device to another. Some of this functionality for handling disks is still under active development but most of it works now. Maintenance If you are worried about corruption or after for example handediting or moving cached items around. Run everything as root sudo su cd into location for your installation cd nodemodulesinternetarchivedwebmirror. internetarchive m This will usually take about 510 minutes depending on the amount of material cached just to rebuild a table of checksums. Advanced Most functionality of the tool is controlled by two YAML files the second of which you can edit if you have access to the shell. You can view the current configuration by going to info on your server. The default and user configurations are displayed as the 0 and 1 item in the info call. In the Repo is a default YAML file which is commented. You really should never need to edit this file as anything in it can be overridden by lines in dwebmirror. config. yaml. Make sure you understand how yaml works before editing this file if you break it you can copy a new default from dwebmirror. config. yaml on the repo Note that this file is also edited automatically when the Crawl button described above is clicked. As the project develops this file will be more and more editable via a UI. Crawling The Crawler runs automatically at startup and when you add something to the crawl but it can also be configurable through the YAML file described above or run at a command line for access to more functionality. In a shell sudo sh cd into the location for your installation on most platforms it is cd nodemodulesinternetarchivedwebmirror Or on IIAB it would be cd optiiabinternetarchivenodemodulesinternetarchivedwebmirror Perform a standard crawl. internetarchive crawl To fetch the foobar item from IA. internetarchive crawl foobar To crawl top 10 items in the prelinger collection sufficiently to display and put them on a disk plugged into the mediapixyz. internetarchive copydirectory mediapixyzarchiveorg crawl rows 10 level details prelinger To get a full list of possible arguments and some more examples. internetarchive help More info I recommend following through the tour in USING. md DwebMirror lives on GitHub at dwebmirror the server source and issues tracker dwebarchive the UI source and issues tracker This project is part of the Internet Archives larger Dweb project see also dwebuniversal info about others working to bring access offline. dwebtransports for our transport library to IPFS WEBTORRENT WOLK GUN etc dwebarchivecontroller for an object oriented wrapper around our APIs,No Headline,Extremely Negative,200,['omit'],[],[]
21,Clozure Common Lisp Wiki,Clozure Common Lisp. Contribute to Clozure/ccl development by creating an account on GitHub.,https://github.com/Clozure/ccl/wiki,The mailing list openmcldevelclozure. com is for general and technical discussion of Clozure CL. You dont need to be a developer if you are interested in CCL you are welcome to participate on the mailing list. There is also a ccl IRC channel on irc. freenode. net. As with the mailing list everyone interested in CCL is welcome to join the channel. You cant perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.,No Headline,Extremely Negative,510,"['lack', 'unaffordable', 'unreliable', 'slow', 'unavailable', 'limit', 'Lack', 'poorer', 'poorer', 'bugs', 'slow', 'limited', 'worried', 'break', 'issues', 'issues', 'object']","['poverty', 'corruption', 'GUN']","[foobar, foobar]"
22,Writing a Python SQL engine from scratch,Python SQL Parser and Transpiler. Contribute to tobymao/sqlglot development by creating an account on GitHub.,https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md,Writing a Python SQL engine from scratch Introduction When I first started writing SQLGlot in early 2021 my goal was just to translate SQL queries from SparkSQL to Presto and vice versa. However over the last year and a half Ive ended up with a fullfledged SQL engine. SQLGlot can now parse and transpile between 18 SQL dialects and can execute all 24 TPCH SQL queries. The parser and engine are all written from scratch using Python. This post will cover why I went through the effort of creating a Python SQL engine and how a simple query goes from a string to actually transforming data. The following steps are briefly summarized Why I started working on SQLGlot because of my work on the experimentation and metrics platform at Netflix where I built tools that allowed data scientists to define and compute SQLbased metrics. Netflix relied on multiple engines to query data Spark Presto and Druid so my team built the metrics platform around PyPika a Python SQL query builder. This way definitions could be reused across multiple engines. However it became quickly apparent that writing python code to programmatically generate SQL was challenging for data scientists especially those with academic backgrounds since they were mostly familiar with R and SQL. At the time the only Python SQL parser was sqlparse which is not actually a parser but a tokenizer so having users write raw SQL into the platform wasnt really an option. Some time later I randomly stumbled across Crafting Interpreters and realized that I could use it as a guide towards creating my own SQL parsertranspiler. Why did I do this Isnt a Python SQL engine going to be extremely slow The main reason why I ended up building a SQL engine was... just for entertainment. Its been fun learning about all the things required to actually run a SQL query and seeing it actually work is extremely rewarding. Before SQLGlot I had zero experience with lexers parsers or compilers. In terms of practical use cases I planned to use the Python SQL engine for unit testing SQL pipelines. Big data pipelines are tough to test because many of the engines are not open source and cannot be run locally. With SQLGlot you can take a SQL query targeting a warehouse such as Snowflake and seamlessly run it in CI on mock Python data. Its easy to mock data and create arbitrary UDFs because everything is just Python. Although the implementation is slow and unsuitable for large amounts of data 1 million rows theres very little overheadstartup and you can run queries on test data in a couple of milliseconds. Finally the components that have been built to support execution can be used as a foundation for a faster engine. Im inspired by what Apache Calcite has done for the JVM world. Even though Python is commonly used for data there hasnt been a Calcite for Python. So you could say that SQLGlot aims to be that framework. For example it wouldnt take much work to replace the Python execution engine with numpypandasarrow to become a respectablyperforming query engine. The implementation would be able to leverage the parser optimizer and logical planner only needing to implement physical execution. There is a lot of work in the Python ecosystem around high performance vectorized computation which I think could benefit from a pure Pythonbased ASTplan. Parsing and planning doesnt have to be fast when the bottleneck of running queries is processing terabytes of data. So having a Pythonbased ecosystem around SQL is beneficial given the ease of development in Python despite not having bare metal performance. Parts of SQLGlots toolkit are being used today by the following Ibis A Python library that provides a lightweight universal interface for data wrangling. Uses the Python SQL expression builder and leverages the optimizerplanner to convert SQL into dataframe operations. mysqlmimic PurePython implementation of the MySQL server wire protocol Parses transforms SQL and executes INFORMATIONSCHEMA queries. Quokka Pushbased vectorized query engine Parse and optimizes SQL. Splink Fast accurate and scalable probabilistic data linkage using your choice of SQL backend. Transpiles queries. How There are many steps involved with actually running a simple query like SELECT bar. a b 1 AS b FROM bar JOIN baz ON bar. a baz. a WHERE bar. a 1 In this post Ill walk through all the steps SQLGlot takes to run this query over Python objects. Tokenizing The first step is to convert the sql string into a list of tokens. SQLGlots tokenizer is quite simple and can be found here. In a while loop it checks each character and either appends the character to the current token or makes a new token. Running the SQLGlot tokenizer shows the output. Each keyword has been converted to a SQLGlot Token object. Each token has some metadata associated with it like linecolumn information for error messages. Comments are also a part of the token so that comments can be preserved. Parsing Once a SQL statement is tokenized we dont need to worry about white space and other formatting so its easier to work with. We can now convert the list of tokens into an AST. The SQLGlot parser is a handwritten recursive descent parser. Similar to the tokenizer it consumes the tokens sequentially but it instead uses a recursive algorithm. The tokens are converted into a single AST node that presents the SQL query. The SQLGlot parser was designed to support various dialects so it contains many options for overriding parsing functionality. The AST is a generic representation of a given SQL query. Each dialect can override or implement its own generator which can convert an AST object into syntaticallycorrect SQL. Optimizing Once we have our AST we can transform it into an equivalent query that produces the same results more efficiently. When optimizing queries most engines first convert the AST into a logical plan and then optimize the plan. However I chose to optimize the AST directly for the following reasons Its easier to debug and validate the optimizations when the input and output are both SQL. Rules can be applied a la carte to transform SQL into a more desirable form. I wanted a way to generate canonical sql. Having a canonical representation of SQL is useful for understanding if two queries are semantically equivalent e. g. SELECT 1 1and SELECT 2. Ive yet to find another engine that takes this approach but Im quite happy with this decision. The optimizer currently does not perform any physical optimizations such as join reordering. Those are left to the execution layer as additional statistics and information could become relevant. The optimizer currently has 17 rules. Each of these rules is applied transforming the AST in place. The combination of these rules creates canonical sql that can then be more easily converted into a logical plan and executed. Some example rules are qualifytables and qualifycolumns Adds all dbcatalog qualifiers to tables and forces an alias. Ensure each column is unambiguous and expand stars. SELECT FROM x SELECT db. x AS x simplify Boolean and math simplification. Check out all the test cases. NOT FALSE AND x x AND TRUE OR 1 3 x x 1 1 2 normalize Attempts to convert all predicates into conjunctive normal form. DNF A AND B OR B AND C AND D CNF A OR C AND A OR D AND B unnestsubqueries Converts subqueries in predicates into joins. The subquery can be converted into a left join SELECT FROM x AS x WHERE SELECT y. a AS a FROM y AS y WHERE x. a y. a 1 SELECT FROM x AS x LEFT JOIN SELECT y. a AS a FROM y AS y WHERE TRUE GROUP BY y. a AS u0 ON x. a u0. a WHERE u0. a 1 AND NOT u0. a IS NULL pushdownpredicates Push down filters into the innermost query. SELECT FROM SELECT FROM x AS x AS y WHERE y. a 1 SELECT FROM SELECT FROM x AS x WHERE y. a 1 AS y WHERE TRUE annotatetypes Infer all types throughout the AST given schema information and function type definitions. Planning After the SQL AST has been optimized its much easier to convert into a logical plan. The AST is traversed and converted into a DAG consisting of one of five steps. The different steps are Scan Selects columns from a table applies projections and finally filters the table. Sort Sorts a table for order by expressions. Set Applies the operators unionunion allexceptintersect. Aggregate Applies an aggregationgroup by. Join Joins multiple tables together. The logical plan is quite simple and contains the information required to convert it into a physical plan execution. Executing Finally we can actually execute the SQL query. The Python engine is not fast but its very small 400 LOC It iterates the DAG with a queue and runs each step passing each intermediary table to the next step. In order to keep things simple it evaluates expressions with eval. Because SQLGlot was built primarily to be a transpiler it was simple to create a Python SQL dialect. So a SQL expression x 1 can just be converted into scopex 1. Whats next SQLGlots main focus will always be on parsingtranspiling but I plan to continue development on the execution engine. Id like to pass TPCDS. If someone doesnt beat me to it I may even take a stab at writing a PandasArrow execution engine. Im hoping that over time SQLGlot will spark the Python SQL ecosystem just like Calcite has for Java. Special thanks SQLGlot would not be what it is without its core contributors. In particular the execution engine would not exist without Barak Alon and George Sittas. Get in touch If youd like to chat more about SQLGlot please join my Slack Channel,No Headline,Extremely Negative,190,[],[],[]
23,Pbproxy – Send your clipboard anywhere you can ssh,Contribute to nikvdp/pbproxy development by creating an account on GitHub.,https://github.com/nikvdp/pbproxy,pbproxy send your clipboard anywhere you can ssh to TLDR pbcopy and pbpaste from any mac or linux machine you can ssh to This is a simple tool to make it easy to interact with clipboards on remote machines. The ui mimics macOSs pbcopy and pbpaste commands using the original pbcopy and pbpaste on macOS and wrapping xsel on Linux boxes. On linux boxes where xsel is not available eg headless servers that dont have X it falls back to storing clipboard contents in a temp file In other words you can pbcopy and pbpaste from any remote machines you have ssh access to. The special sauce is that you can put a server name from your ssh config on the end of a pbcopy or pbpaste command so you can do stuff like run echo hello pbcopy server2 on server1 after which doing pbpaste from server2 would print hello. You can also do this in the other direction with pbpaste eg from server2 you can run pbpaste server1 and itll print out the contents of server1s clipboard. Prereqs For best results make sure you have pbproxyinstalled at. pbproxyon all machines youll be copyingpasting tofrom. make sure you have keybased ssh authentication set upotherwise youll have to enter your password each time you copy or paste Usage Print the contents of another machines clipboard to your local machines stdout pbpaste someserver Insert data into another machines clipboard echo hello there pbcopy someserver You can also use this to send the contents of your current machines clipboard to another machine pbpaste pbcopy someserver Print the contents of your local clipboard to stdout pbpaste Insert data into your local clipboard echo some data pbcopy NOTE This tool works the same way regardless of whether its running on a macOS or a linux machine. On macs it wraps the native pbcopy and pbaste commands and on linux it uses xsel or a temp file Installation Clone this repo to your local machine Add the folder you cloned it to to your PATHthrough your. bashrc. zshrcfiles NOTE on macOS make sure it comes BEFORE usrbinon your PATH otherwise the native pbcopyand pbpastewill take precedence restart your shell or on zsh do hash r and try it out. For the lazy read efficient copypasting this quick and dirty shell snippet should get the job done git clone httpsgithub. comnikvdppbproxy. pbproxy ps grep grep q zsh echo export PATHHOME. pbproxyPATH tee a. zshrc ps grep grep q bash echo export PATHHOME. pbproxyPATH tee a. bashrc export PATHHOME. pbproxyPATH hash r Alternatives If you know of any other tools in this space feel free to open a PR to include them here marcopaganiniclipsync conceptually similar but runs as a background daemon. Unlike pbproxy this means it can sync automatically in realtime over the cloud whereas pbproxy only connects to other clipboards on demand. Linux only for now though.,Name already in use,Extremely Positive,170,"['scratch', 'vice', 'scratch', 'challenging', 'randomly', 'stumbled', 'slow', 'mock', 'mock', 'arbitrary', 'slow', 'object', 'error', 'worry', 'object', 'FALSE', 'stab', 'Slack']",[],"[Barak, Alon, George, Sittas]"
24,Show HN: Scripting language inspired by JavaScript and GLSL,Generalized Linear Model Language. Contribute to sebbekarlsson/glms development by creating an account on GitHub.,https://github.com/sebbekarlsson/glms,Generalized Linear Model Script A scripting language focused on linear algebra heavily inspired by GLSL and JavaScript. This language comes with types functions and structures commonly used when doing linear algebra such as vec2 vec3 vec4 dot cross lerp sin cos... etc There are more to come and everything is not implemented yet. Caution This is a work in progress Building using it mkdir build cd build cmake.. make j8. glmse inputfile. gs Extensions Its possible to create extensions for GLMS here is an example. This specific extension lets you draw on a canvas using OpenGL. import libglmscanvas. so as canvasLib typedef canvasLib. canvas canvas were making a promise to the interpreter that this type will exist later. canvas c canvas640 480 opens a window changes can be seen in realtime c. shadevec3 uv vec3 fragCoord vec3 resolution number time return vec40. 5 0. 5 cosuv. xtime 0. 5 0. 5 sinuv. y time costime 1. 0 Documentation To see documentation builtin functions etc Have a look at this Integration Want to integrate GLMSfor scripting in an existing project Have a look at this Some interesting features fetchapi similar to the one in Javascript jsonsupport file IO image manipulation vector math matrix math almost all functions youd see in GLSLexists in GLMSas well. extension support Extend the language by writing extensions... and more Some examples Shaderlike image manipulation number w 640 number h 480 image img image. makew h img. shadevec3 uv vec3 fragCoord vec3 resolution vec3 center resolution 0. 5 number d absdistancefragCoord center number g 255 d TAU 6. 0 1. 0 random vec3 color mixvec30. 1 0. 3 0. 9 vec31 g return vec4color. xyz 1. 0 img. savetest. png Functional programming array arr 1 2 3 array mapped arr. mapnumber v v 2 printmapped 2. 000000 4. 000000 6. 000000 Vectors vec3 a vec31 0 0 vec3 b vec30 1 0 vec3 c crossa b printc vec3 d vec3random random random vec3 e vec3random random random printdistanced e vec3 f vec3random printf number dp dotd f printdp Lerp number x 25. 012 number y 98. 241 number z lerpx y 0. 1 printz 32. 334900 Clamp number value 2312. 0 value clampvalue 0. 0 1. 0 printvalue 1. 000000 Structs typedef struct number age string name Person Person p Person33 John Doe printp. age 33. 000000 printp. name John Doe HTTP Requests response r fetchhttpsexample. org printr. status 200 printr. text prints the whole response text we can also request json APIs response r fetchhttpsjsonplaceholder. typicode. composts array p r. json object firstPost p0 printfirstPost. title Reading JSON file f file. openassetssomefile. json r string contents f. read f. close object data json. parsecontents printdata. firstName printdata. age Template strings string name John string x hello name printx hello John More examples For more examples see examples FAQ Q Why did you create this language I was looking for a scriptinglanguage to be used in a game I was developing and I wanted something that came with vector operatons right out of the box just like GLSL but I also wanted it to be expressive like Javascript. Q How do I integrate it into my application game Have a look at this If its still not clear feel free to create an issue or something with your question. Q What operating systems platforms can this language run on It will most likely only work on Linux right now maybe MacOS as well but youre always welcome to contribute to support more platforms Q Any other plans for this language It would be cool to add some frontends to the language heres some Ive had in mind GLSL Would be cool to use this as a transpiler for GLSL Javascript Emitting Javascript would allow for webapplications being created with GLMS WASM Same reason as the Javascript one 64bit assembly Because its cool That being said Im not sure Id ever implement any of these ideas. Im just using this for scripting a game Im developing at the moment. Q Is there some kind of community I just threw together a Discordserver for whoever is interested you can find it here Q Can I contribute Please do Simply fork and create pullrequests,No Headline,Extremely Negative,270,"['falls', 'lazy', 'bash']",['dirty'],[]
25,Just: A Command Runner,🤖 Just a command runner. Contribute to casey/just development by creating an account on GitHub.,https://github.com/casey/just,just just is a handy way to save and run projectspecific commands. This readme is also available as a book. Commands called recipes are stored in a file called justfile with syntax inspired by make You can then run them with just RECIPE just testall cc. c o main. test all Yay all your tests passed just has a ton of useful features and many improvements over make justis a command runner not a build system so it avoids much of makes complexity and idiosyncrasies. No need for. PHONYrecipes Linux MacOS and Windows are supported with no additional dependencies. Although if your system doesnt have an sh youll need to choose a different shell. Errors are specific and informative and syntax errors are reported along with their source context. Recipes can accept command line arguments. Wherever possible errors are resolved statically. Unknown recipes and circular dependencies are reported before anything runs. justloads. envfiles making it easy to populate environment variables. Recipes can be listed from the command line. Command line completion scripts are available for most popular shells. Recipes can be written in arbitrary languages like Python or NodeJS. justcan be invoked from any subdirectory not just the directory that contains the justfile. If you need help with just please feel free to open an issue or ping me on Discord. Feature requests and bug reports are always welcome Installation Prerequisites just should run on any system with a reasonable sh including Linux MacOS and the BSDs. On Windows just works with the sh provided by Git for Windows GitHub Desktop or Cygwin. If youd rather not install sh you can use the shell setting to use the shell of your choice. Like PowerShell use PowerShell instead of sh set shell powershell. exe c hello WriteHost Hello world or cmd. exe use cmd. exe instead of sh set shell cmd. exe c list dir You can also set the shell using commandline arguments. For example to use PowerShell launch just with shell powershell. exe shellarg c. PowerShell is installed by default on Windows 7 SP1 and Windows Server 2008 R2 S1 and later and cmd. exe is quite fiddly so PowerShell is recommended for most Windows users. Packages PreBuilt Binaries Prebuilt binaries for Linux MacOS and Windows can be found on the releases page. You can use the following command on Linux MacOS or Windows to download the latest release just replace DEST with the directory where youd like to put just curl proto https tlsv1. 3 sSf httpsjust. systemsinstall. sh bash s to DEST For example to install just to bin create bin mkdir p bin download and extract just to binjust curl proto https tlsv1. 3 sSf httpsjust. systemsinstall. sh bash s to bin add bin to the paths that your shell searches for executables this line should be added to your shells initialization file e. g.. bashrc or. zshrc export PATHPATHHOMEbin just should now be executable just help Note that install. sh may fail on GitHub actions or in other environments where many machines share IP addresses. install. sh calls GitHub APIs in order to determine the latest version of just to install and those API calls are ratelimited on a perIP basis. To make install. sh more reliable in such circumstances pass a specific tag to install with tag. GitHub Actions With extractionssetupjust uses extractionssetupjustv1 with justversion 0. 8 optional semver specification otherwise latest With taikieinstallaction uses taikieinstallactionjust Release RSS Feed An RSS feed of just releases is available here. Node. js Installation justinstall can be used to automate installation of just in Node. js applications. just is a great more robust alternative to npm scripts. If you want to include just in the dependencies of a Node. js application justinstall will install a local platformspecific binary as part of the npm install command. This removes the need for every developer to install just independently using one of the processes mentioned above. After installation the just command will work in npm scripts or with npx. Its great for teams who want to make the set up process for their project as easy as possible. For more information see the justinstall README file. Backwards Compatibility With the release of version 1. 0 just features a strong commitment to backwards compatibility and stability. Future releases will not introduce backwards incompatible changes that make existing justfiles stop working or break working invocations of the commandline interface. This does not however preclude fixing outright bugs even if doing so might break justfiles that rely on their behavior. There will never be a just 2. 0. Any desirable backwardsincompatible changes will be optin on a per justfile basis so users may migrate at their leisure. Features that arent yet ready for stabilization are gated behind the unstable flag. Features enabled by unstable may change in backwards incompatible ways at any time. Editor Support justfile syntax is close enough to make that you may want to tell your editor to use make syntax highlighting for just. Vim and Neovim vimjust The vimjust plugin provides syntax highlighting for justfiles. Install it with your favorite package manager like Plug call plugbegin Plug NoahTheDukevimjust call plugend Or with Vims builtin package support mkdir p. vimpackvendorstart cd. vimpackvendorstart git clone httpsgithub. comNoahTheDukevimjust. git vimjust is also available from vimpolyglot a multilanguage Vim plugin. treesitterjust treesitterjust is an Nvim Treesitter plugin for Neovim. Makefile Syntax Highlighting Vims builtin makefile syntax highlighting isnt perfect for justfiles but its better than nothing. You can put the following in. vimfiletype. vim if existsdidloadfiletypes finish endif augroup filetypedetect au BufNewFileBufRead justfile setf make augroup END Or add the following to an individual justfile to enable make mode on a perfile basis vim set ftmake Emacs justmode provides syntax highlighting and automatic indentation of justfiles. It is available on MELPA as justmode. justl provides commands for executing and listing recipes. You can add the following to an individual justfile to enable make mode on a perfile basis Local Variables mode makefile End Visual Studio Code An extension for VS Code by skellock is available here repository but is no longer actively developed. You can install it from the command line by running code installextension skellock. just An more recently active fork by sclu1034 is available here. JetBrains IDEs A plugin for JetBrains IDEs by linuxchina is available here. Kakoune Kakoune supports justfile syntax highlighting out of the box thanks to TeddyDD. Sublime Text The Just package by nk9 with just syntax and some other tools is available on PackageControl. Other Editors Feel free to send me the commands necessary to get syntax highlighting working in your editor of choice so that I may include them here. Quick Start See the installation section for how to install just on your computer. Try running just version to make sure that its installed correctly. For an overview of the syntax check out this cheatsheet. Once just is installed and working create a file named justfile in the root of your project with the following contents recipename echo This is a recipe this is a comment anotherrecipe echo This is another recipe. When you invoke just it looks for file justfile in the current directory and upwards so you can invoke it from any subdirectory of your project. The search for a justfile is case insensitive so any case like Justfile JUSTFILE or JuStFiLe will work. just will also look for files with the name. justfile in case youd like to hide a justfile. Running just with no arguments runs the first recipe in the justfile just echo This is a recipe This is a recipe One or more arguments specify the recipes to run just anotherrecipe This is another recipe. just prints each command to standard error before running it which is why echo This is a recipe was printed. This is suppressed for lines starting with which is why echo This is another recipe. was not printed. Recipes stop running if a command fails. Here cargo publish will only run if cargo test succeeds publish cargo test tests passed time to publish cargo publish Recipes can depend on other recipes. Here the test recipe depends on the build recipe so build will run before test build cc main. c foo. c bar. c o main test build. test sloc echo wc l. c lines of code just test cc main. c foo. c bar. c o main. test testing all tests passed Recipes without dependencies will run in the order theyre given on the command line just build sloc cc main. c foo. c bar. c o main 1337 lines of code Dependencies will always run first even if they are passed after a recipe that depends on them just test build cc main. c foo. c bar. c o main. test testing all tests passed Examples A variety of example justfiles can be found in the examples directory. Features The Default Recipe When just is invoked without a recipe it runs the first recipe in the justfile. This recipe might be the most frequently run command in the project like running the tests test cargo test You can also use dependencies to run multiple recipes by default default lint build test build echo Building test echo Testing lint echo Linting If no recipe makes sense as the default recipe you can add a recipe to the beginning of your justfile that lists the available recipes default just list Listing Available Recipes Recipes can be listed in alphabetical order with just list just list Available recipes build test deploy lint just summary is more concise just summary build test deploy lint Pass unsorted to print recipes in the order they appear in the justfile test echo Testing build echo Building just list unsorted Available recipes test build just summary unsorted test build If youd like just to default to listing the recipes in the justfile you can use this as your default recipe default just list Note that you may need to add justfile justfile to the line above above. Without it if you executed just f somedistantjustfile d. or just f. nonstandardjustfile the plain just list inside the recipe would not necessarily use the file you provided. It would try to find a justfile in your current path maybe even resulting in a No justfile found error. The heading text can be customized with listheading just list listheading Cool stuffn Cool stuff test build And the indentation can be customized with listprefix just list listprefix Available recipes test build The argument to listheading replaces both the heading and the newline following it so it should contain a newline if nonempty. It works this way so you can suppress the heading line entirely by passing the empty string just list listheading test build Aliases Aliases allow recipes to be invoked with alternative names alias b build build echo Building just b build echo Building Building Settings Settings control interpretation and execution. Each setting may be specified at most once anywhere in the justfile. For example set shell zsh cu foo this line will be run as zsh cu ls. txt ls. txt Table of Settings Boolean settings can be written as set NAME Which is equivalent to set NAME true Allow Duplicate Recipes If allowduplicaterecipes is set to true defining multiple recipes with the same name is not an error and the last definition is used. Defaults to false. set allowduplicaterecipes foo echo foo foo echo bar just foo bar Dotenv Load If dotenvload is true a. env file will be loaded if present. Defaults to false. Export The export setting causes all just variables to be exported as environment variables. Defaults to false. set export a hello foo b echo a echo b just foo goodbye hello goodbye Positional Arguments If positionalarguments is true recipe arguments will be passed as positional arguments to commands. For linewise recipes argument 0 will be the name of the recipe. For example running this recipe set positionalarguments foo bar echo 0 echo 1 Will produce the following output just foo hello foo hello When using an shcompatible shell such as bash or zsh expands to the positional arguments given to the recipe starting from one. When used within double quotes as arguments including whitespace will be passed on as if they were doublequoted. That is is equivalent to 1 2 When there are no positional parameters and expand to nothing i. e. they are removed. This example recipe will print arguments one by one on separate lines set positionalarguments test args bash c while do echo 1 shift done Running it with two arguments just test foo bar baz foo bar baz Shell The shell setting controls the command used to invoke recipe lines and backticks. Shebang recipes are unaffected. use python3 to execute recipe lines and backticks set shell python3 c use print to capture result of evaluation foos printfoo 4 foo printSnake snake snake snake. printfoos just passes the command to be executed as an argument. Many shells will need an additional flag often c to make them evaluate the first argument. Windows Shell just uses sh on Windows by default. To use a different shell on Windows use windowsshell set windowsshell powershell. exe NoLogo Command hello WriteHost Hello world See powershell. just for a justfile that uses PowerShell on all platforms. Windows PowerShell set windowspowershell uses the legacy powershell. exe binary and is no longer recommended. See the windowsshell setting above for a more flexible way to control which shell is used on Windows. just uses sh on Windows by default. To use powershell. exe instead set windowspowershell to true. set windowspowershell true hello WriteHost Hello world Python 3 set shell python3 c Bash set shell bash uc Z Shell set shell zsh uc Fish set shell fish c Nushell set shell nu c If you want to change the default table mode to light set shell nu m light c Nushell was written in Rust and has crossplatform support for Windows macOS and Linux. Documentation Comments Comments immediately preceding a recipe will appear in just list build stuff build. binbuild test stuff test. bintest just list Available recipes build build stuff test test stuff Dotenv Integration If dotenvload is set just will load environment variables from a file named. env. This file can be located in the same directory as your justfile or in a parent directory. These variables are environment variables not just variables and so must be accessed using VARIABLENAME in recipes and backticks. For example if your. env file contains a comment will be ignored DATABASEADDRESSlocalhost6379 SERVERPORT1337 And your justfile contains set dotenvload serve echo Starting server with database DATABASEADDRESS on port SERVERPORT. server database DATABASEADDRESS port SERVERPORT just serve will output just serve Starting server with database localhost6379 on port 1337. server database DATABASEADDRESS port SERVERPORT Variables and Substitution Variables strings concatenation path joining and substitution using are supported tmpdir mktemp version 0. 2. 7 tardir tmpdir awesomesauce version tarball tardir. tar. gz publish rm f tarball mkdir tardir cp README. md. c tardir tar zcvf tarball tardir scp tarball meserver. comrelease rm rf tarball tardir Joining Paths The operator can be used to join two strings with a slash foo a b just evaluate foo ab Note that a is added even if one is already present foo a bar foo b just evaluate bar ab Absolute paths can also be constructed 1. 5. 0 foo b just evaluate foo b The operator uses the character even on Windows. Thus using the operator should be avoided with paths that use universal naming convention UNC i. e. those that start with since forward slashes are not supported with UNC paths. Escaping To write a recipe containing use braces echo I LOVE curly braces An unmatched is ignored so it doesnt need to be escaped. Another option is to put all the text youd like to escape inside of an interpolation braces echo I LOVE curly braces Yet another option is to use braces echo I LOVE curly braces Strings Doublequoted strings support escape sequences stringwithtab t stringwithnewline n stringwithcarriagereturn r stringwithdoublequote stringwithslash stringwithnonewline just evaluate tringwithcarriagereturn stringwithdoublequote stringwithnewline stringwithnonewline stringwithslash stringwithtab Strings may contain line breaks single hello double goodbye Singlequoted strings do not recognize escape sequences escapes tnr just evaluate escapes tnr Indented versions of both single and doublequoted strings delimited by triple single or triple doublequotes are supported. Indented string lines are stripped of leading whitespace common to all nonblank lines this string will evaluate to foonbarn x foo bar this string will evaluate to abcn wuvnbarn y abc wuv xyz Similar to unindented strings indented doublequoted strings process escape sequences and indented singlequoted strings ignore escape sequences. Escape sequence processing takes place after unindentation. The unindentation algorithm does not take escapesequence produced whitespace or newlines into account. Ignoring Errors Normally if a command returns a nonzero exit status execution will stop. To continue execution after a command even if it fails prefix the command with foo cat foo echo Done just foo cat foo cat foo No such file or directory echo Done Done Functions just provides a few builtin functions that might be useful when writing recipes. System Information arch Instruction set architecture. Possible values are aarch64 arm asmjs hexagon mips msp430 powerpc powerpc64 s390x sparc wasm32 x86 x8664 and xcore. os Operating system. Possible values are android bitrig dragonfly emscripten freebsd haiku ios linux macos netbsd openbsd solaris and windows. osfamily Operating system family possible values are unixand windows. For example systeminfo echo This is an arch machine. just systeminfo This is an x8664 machine The osfamily function can be used to create crossplatform justfiles that work on various operating systems. For an example see crossplatform. just file. Environment Variables envvarkey Retrieves the environment variable with name key aborting if it is not present. homedir envvarHOME test echo homedir just homeuser1 envvarordefaultkey default Retrieves the environment variable with name key returning defaultif it is not present. Invocation Directory invocationdirectory Retrieves the absolute path to the current directory when justwas invoked before justchanged it chdird prior to executing commands. On Windows invocationdirectoryuses cygpathto convert the invocation directory to a Cygwincompatible separated path. Use invocationdirectorynativeto return the verbatim invocation directory on all platforms. For example to call rustfmt on files just under the current directory from the userinvokers perspective use the following rule rustfmt find invocationdirectory name. rs exec rustfmt Alternatively if your command needs to be run from the current directory you could use e. g. build cd invocationdirectory. somescriptthatneedstoberunfromhere invocationdirectorynative Retrieves the absolute path to the current directory when justwas invoked before justchanged it chdird prior to executing commands. Justfile and Justfile Directory justfile Retrieves the path of the current justfile. justfiledirectory Retrieves the path of the parent directory of the current justfile. For example to run a command relative to the location of the current justfile script. justfiledirectoryscriptssomescript Just Executable justexecutable Absolute path to the justexecutable. For example executable echo The executable is at justexecutable just The executable is at binjust String Manipulation quotes Replace all single quotes with and prepend and append single quotes to s. This is sufficient to escape special characters for many shells including most Bourne shell descendants. replaces from to Replace all occurrences of fromin sto to. replaceregexs regex replacement Replace all occurrences of regexin sto replacement. Regular expressions are provided by the Rust regexcrate. See the syntax documentation for usage examples. trims Remove leading and trailing whitespace from s. trimends Remove trailing whitespace from s. trimendmatchs pat Remove suffix of smatching pat. trimendmatchess pat Repeatedly remove suffixes of smatching pat. trimstarts Remove leading whitespace from s. trimstartmatchs pat Remove prefix of smatching pat. trimstartmatchess pat Repeatedly remove prefixes of smatching pat. Case Conversion capitalizes 1. 7. 0 Convert first character of sto uppercase and the rest to lowercase. kebabcases 1. 7. 0 Convert sto kebabcase. lowercamelcases 1. 7. 0 Convert sto lowerCamelCase. lowercases Convert sto lowercase. shoutykebabcases 1. 7. 0 Convert sto SHOUTYKEBABCASE. shoutysnakecases 1. 7. 0 Convert sto SHOUTYSNAKECASE. snakecases 1. 7. 0 Convert sto snakecase. titlecases 1. 7. 0 Convert sto Title Case. uppercamelcases 1. 7. 0 Convert sto UpperCamelCase. uppercases Convert sto uppercase. Path Manipulation Fallible absolutepathpath Absolute path to relative pathin the working directory. absolutepath. bar. txtin directory foois foobar. txt. extensionpath Extension of path. extensionfoobar. txtis txt. filenamepath File name of pathwith any leading directory components removed. filenamefoobar. txtis bar. txt. filestempath File name of pathwithout extension. filestemfoobar. txtis bar. parentdirectorypath Parent directory of path. parentdirectoryfoobar. txtis foo. withoutextensionpath pathwithout extension. withoutextensionfoobar. txtis foobar. These functions can fail for example if a path does not have an extension which will halt execution. Infallible cleanpath Simplify pathby removing extra path separators intermediate. components and.. where possible. cleanfoobaris foobar cleanfoo.. is. cleanfoo. baris foobar. joina b This function usesJoin path on Unix and on Windows which can be lead to unwanted behavior. The operator e. g. a b which always uses should be considered as a replacement unless s are specifically desired on Windows. awith path b. joinfoobar bazis foobarbaz. Accepts two or more arguments. Filesystem Access pathexistspath Returns trueif the path points at an existing entity and falseotherwise. Traverses symbolic links and returns falseif the path is inaccessible or points to a broken symlink. Error Reporting errormessage Abort execution and report error messageto user. UUID and Hash Generation sha256string Return the SHA256 hash of stringas a hexadecimal string. sha256filepath Return the SHA256 hash of the file at pathas a hexadecimal string. uuid Return a randomly generated UUID. Recipe Attributes Recipes may be annotated with attributes that change their behavior. Enabling and Disabling Recipes The linux macos unix and windows attributes are configuration attributes. By default recipes are always enabled. A recipe with one or more configuration attributes will only be enabled when one or more of those configurations is active. This can be used to write justfiles that behave differently depending on which operating system they run on. The run recipe in this justfile will compile and run main. c using a different C compiler and using the correct output binary name for that compiler depending on the operating system unix run cc main. c. a. out windows run cl main. c main. exe Disabling Changing Directory 1. 9. 0 just normally executes recipes with the current directory set to the directory that contains the justfile. This can be disabled using the nocd attribute. This can be used to create recipes which use paths relative to the invocation directory or which operate on the current directory. For example this commit recipe nocd commit file git add file git commit Can be used with paths that are relative to the current directory because nocd prevents just from changing the current directory when executing commit. Command Evaluation Using Backticks Backticks can be used to store the result of commands localhost dumpinterfaces cut d f2 sed s. sed s g serve. serve localhost 8080 Indented backticks delimited by three backticks are deindented in the same manner as indented strings This backtick evaluates the command echo foonecho barn which produces the value foonbarn. stuff echo foo echo bar See the Strings section for details on unindenting. Backticks may not start with. This syntax is reserved for a future upgrade. Conditional Expressions if else expressions evaluate different branches depending on if two expressions evaluate to the same value foo if 2 2 Good else 1984 bar echo foo just bar Good It is also possible to test for inequality foo if hello goodbye xyz else abc bar echo foo just bar xyz And match against regular expressions foo if hello helo match else mismatch bar echo foo just bar match Regular expressions are provided by the regex crate whose syntax is documented on docs. rs. Since regular expressions commonly use backslash escape sequences consider using singlequoted string literals which will pass slashes to the regex parser unmolested. Conditional expressions shortcircuit which means they only evaluate one of their branches. This can be used to make sure that backtick expressions dont run when they shouldnt. foo if envvarRELEASE true getsomethingfromreleasedatabase else dummyvalue Conditionals can be used inside of recipes bar foo echo if foo bar hello else goodbye Note the space after the final Without the space the interpolation will be prematurely closed. Multiple conditionals can be chained foo if hello goodbye xyz else if a a abc else 123 bar echo foo just bar abc Stopping execution with error Execution can be halted with the error function. For example foo if hello goodbye xyz else if a b abc else error123 Which produce the following error when run error Call to function error failed 123 16 error123 Setting Variables from the Command Line Variables can be overridden from the command line. os linux test build. test test os build. build os just. build linux. test test linux Any number of arguments of the form NAMEVALUE can be passed before recipes just osplan9. build plan9. test test plan9 Or you can use the set flag just set os bsd. build bsd. test test bsd Getting and Setting Environment Variables Exporting just Variables Assignments prefixed with the export keyword will be exported to recipes as environment variables export RUSTBACKTRACE 1 test will print a stack trace if it crashes cargo test Parameters prefixed with a will be exported as environment variables test RUSTBACKTRACE1 will print a stack trace if it crashes cargo test Exported variables and parameters are not exported to backticks in the same scope. export WORLD world This backtick will fail with WORLD unbound variable BAR echo hello WORLD Running just a foo will fail with A unbound variable a A Becho A echo A B When export is set all just variables are exported as environment variables. Getting Environment Variables from the environment Environment variables from the environment are passed automatically to the recipes. printhomefolder echo HOME is HOME just HOME is homemyuser Loading Environment Variables from a. env File just will load environment variables from a. env file if dotenvload is set. The variables in the file will be available as environment variables to the recipes. See dotenvintegration for more information. Setting just Variables from Environment Variables Environment variables can be propagated to just variables using the functions envvar and envvarordefault. See environmentvariables. Recipe Parameters Recipes may have parameters. Here recipe build has a parameter called target build target echo Building target cd target make To pass arguments on the command line put them after the recipe name just build myawesomeproject Building myawesomeproject cd myawesomeproject make To pass arguments to a dependency put the dependency in parentheses along with the arguments default build main build target echo Building target cd target make Variables can also be passed as arguments to dependencies target main build version echo Building version cd version make build build target A commands arguments can be passed to dependency by putting the dependency in parentheses along with the arguments build target echo Building target push target build target echo Pushing target Parameters may have default values default all test target testsdefault echo Testing targettests. test tests tests target Parameters with default values may be omitted just test server Testing serverall. test tests all server Or supplied just test server unit Testing serverunit. test tests unit server Default values may be arbitrary expressions but concatenations or path joins must be parenthesized arch wasm test triplearch unknownunknown inputarch input. dat. test triple The last parameter of a recipe may be variadic indicated with either a or a before the argument name backup FILES scp FILES meserver. com Variadic parameters prefixed with accept one or more arguments and expand to a string containing those arguments separated by spaces just backup FAQ. md GRAMMAR. md scp FAQ. md GRAMMAR. md meserver. com FAQ. md 100 1831 1. 8KBs 0000 GRAMMAR. md 100 1666 1. 6KBs 0000 Variadic parameters prefixed with accept zero or more arguments and expand to a string containing those arguments separated by spaces or an empty string if no arguments are present commit MESSAGE FLAGS git commit FLAGS m MESSAGE Variadic parameters can be assigned default values. These are overridden by arguments passed on the command line test FLAGSq cargo test FLAGS substitutions may need to be quoted if they contain spaces. For example if you have the following recipe search QUERY lynx httpswww. google. comqQUERY And you type just search cat toupee just will run the command lynx httpswww. google. comqcat toupee which will get parsed by sh as lynx httpswww. google. comqcat and toupee and not the intended lynx and httpswww. google. comqcat toupee. You can fix this by adding quotes search QUERY lynx httpswww. google. comqQUERY Parameters prefixed with a will be exported as environment variables foo bar echo bar Running Recipes at the End of a Recipe Normal dependencies of a recipes always run before a recipe starts. That is to say the dependee always runs before the depender. These dependencies are called prior dependencies. A recipe can also have subsequent dependencies which run after the recipe and are introduced with an a echo A b a c d echo B c echo C d echo D running b prints just b echo A A echo B B echo C C echo D D Running Recipes in the Middle of a Recipe just doesnt support running recipes in the middle of another recipe but you can call just recursively in the middle of a recipe. Given the following justfile a echo A b a echo B start just c echo B end c echo C running b prints just b echo A A echo B start B start echo C C echo B end B end This has limitations since recipe c is run with an entirely new invocation of just Assignments will be recalculated dependencies might run twice and command line arguments will not be propagated to the child just process. Writing Recipes in Other Languages Recipes that start with are called shebang recipes and are executed by saving the recipe body to a file and running it. This lets you write recipes in different languages polyglot python js perl sh ruby nu python usrbinenv python3 printHello from python js usrbinenv node console. logGreetings from JavaScript perl usrbinenv perl print Larry Wall says Hin sh usrbinenv sh helloYo echo hello from a shell script nu usrbinenv nu let hello Hola echo hello from a nushell script ruby usrbinenv ruby puts Hello from ruby just polyglot Hello from python Greetings from JavaScript Larry Wall says Hi Yo from a shell script Hola from a nushell script Hello from ruby On Unixlike operating systems including Linux and MacOS shebang recipes are executed by saving the recipe body to a file in a temporary directory marking the file as executable and executing it. The OS then parses the shebang line into a command line and invokes it including the path to the file. For example if a recipe starts with usrbinenv bash the final command that the OS runs will be something like usrbinenv bash tmpPATHTOSAVEDRECIPEBODY. Keep in mind that different operating systems split shebang lines differently. Windows does not support shebang lines. On Windows just splits the shebang line into a command and arguments saves the recipe body to a file and invokes the split command and arguments adding the path to the saved recipe body as the final argument. Safer Bash Shebang Recipes If youre writing a bash shebang recipe consider adding set euxo pipefail foo usrbinenv bash set euxo pipefail helloYo echo hello from Bash It isnt strictly necessary but set euxo pipefail turns on a few useful features that make bash shebang recipes behave more like normal linewise just recipe set emakes bashexit if a command fails. set umakes bashexit if a variable is undefined. set xmakes bashprint each script line before its run. set o pipefailmakes bashexit if a command in a pipeline fails. This is bashspecific so isnt turned on in normal linewise justrecipes. Together these avoid a lot of shell scripting gotchas. Shebang Recipe Execution on Windows On Windows shebang interpreter paths containing a are translated from Unixstyle paths to Windowsstyle paths using cygpath a utility that ships with Cygwin. For example to execute this recipe on Windows echo binsh echo Hello The interpreter path binsh will be translated to a Windowsstyle path using cygpath before being executed. If the interpreter path does not contain a it will be executed without being translated. This is useful if cygpath is not available or you wish to pass a Windowsstyle path to the interpreter. Setting Variables in a Recipe Recipe lines are interpreted by the shell not just so its not possible to set just variables in the middle of a recipe foo x hello This doesnt work echo x It is possible to use shell variables but theres another problem. Every recipe line is run by a new shell instance so variables set in one line wont be set in the next foo xhello echo x This works ybye echo y This doesnt y is undefined here The best way to work around this is to use a shebang recipe. Shebang recipe bodies are extracted and run as scripts so a single shell instance will run the whole thing foo usrbinenv bash set euxo pipefail xhello echo x Sharing Environment Variables Between Recipes Each line of each recipe is executed by a fresh shell so it is not possible to share environment variables between recipes. Using Python Virtual Environments Some tools like Pythons venv require loading environment variables in order to work making them challenging to use with just. As a workaround you can execute the virtual environment binaries directly venv d foo python3 m venv foo run venv. foobinpython3 main. py Changing the Working Directory in a Recipe Each recipe line is executed by a new shell so if you change the working directory on one line it wont have an effect on later lines foo pwd This pwd will print the same directory cd bar pwd as this pwd There are a couple ways around this. One is to call cd on the same line as the command you want to run foo cd bar pwd The other is to use a shebang recipe. Shebang recipe bodies are extracted and run as scripts so a single shell instance will run the whole thing and thus a pwd on one line will affect later lines just like a shell script foo usrbinenv bash set euxo pipefail cd bar pwd Indentation Recipe lines can be indented with spaces or tabs but not a mix of both. All of a recipes lines must have the same indentation but different recipes in the same justfile may use different indentation. MultiLine Constructs Recipes without an initial shebang are evaluated and run linebyline which means that multiline constructs probably wont do what you want. For example with the following justfile conditional if true then echo True fi The extra leading whitespace before the second line of the conditional recipe will produce a parse error just conditional error Recipe line has extra leading whitespace 3 echo True To work around this you can write conditionals on one line escape newlines with slashes or add a shebang to your recipe. Some examples of multiline constructs are provided for reference. if statements conditional if true then echo True fi conditional if true then echo True fi conditional usrbinenv sh if true then echo True fi for loops for for file in ls. do echo file done for for file in ls. do echo file done for usrbinenv sh for file in ls. do echo file done while loops while while serverisdead do ping c 1 server done while while serverisdead do ping c 1 server done while usrbinenv sh while serverisdead do ping c 1 server done Command Line Options just supports a number of useful command line options for listing dumping and debugging recipes and variable just list Available recipes js perl polyglot python ruby just show perl perl usrbinenv perl print Larry Wall says Hin just show polyglot polyglot python js perl sh ruby Run just help to see all the options. Private Recipes Recipes and aliases whose name starts with a are omitted from just list test testhelper. bintest testhelper. binsupersecrettesthelperstuff just list Available recipes test And from just summary just summary test The private attribute 1. 10. 0 may also be used to hide recipes or aliases without needing to change the name private foo private alias b bar bar just list Available recipes bar This is useful for helper recipes which are only meant to be used as dependencies of other recipes. Quiet Recipes A recipe name may be prefixed with to invert the meaning of before each line quiet echo hello echo goodbye all done Now only the lines starting with will be echoed j quiet hello goodbye all done Shebang recipes are quiet by default foo usrbinenv bash echo Foo just foo Foo Adding to a shebang recipe name makes just print the recipe before executing it bar usrbinenv bash echo Bar just bar usrbinenv bash echo Bar Bar just normally prints error messages when a recipe line fails. These error messages can be suppressed using the noexitmessage attribute. You may find this especially useful with a recipe that recipe wraps a tool git args git args just git status fatal not a git repository or any of the parent directories. git error Recipe git failed on line 2 with exit code 128 Add the attribute to suppress the exit error message when the tool exits with a nonzero code noexitmessage git args git args just git status fatal not a git repository or any of the parent directories. git Selecting Recipes to Run With an Interactive Chooser The choose subcommand makes just invoke a chooser to select which recipes to run. Choosers should read lines containing recipe names from standard input and print one or more of those names separated by spaces to standard output. Because there is currently no way to run a recipe that requires arguments with choose such recipes will not be given to the chooser. Private recipes and aliases are also skipped. The chooser can be overridden with the chooser flag. If chooser is not given then just first checks if JUSTCHOOSER is set. If it isnt then the chooser defaults to fzf a popular fuzzy finder. Arguments can be included in the chooser i. e. fzf exact. The chooser is invoked in the same way as recipe lines. For example if the chooser is fzf it will be invoked with sh cu fzf and if the shell or the shell arguments are overridden the chooser invocation will respect those overrides. If youd like just to default to selecting recipes with a chooser you can use this as your default recipe default just choose Invoking justfiles in Other Directories If the first argument passed to just contains a then the following occurs The argument is split at the last. The part before the last is treated as a directory. justwill start its search for the justfilethere instead of in the current directory. The part after the last slash is treated as a normal argument or ignored if it is empty. This may seem a little strange but its useful if you wish to run a command in a justfile that is in a subdirectory. For example if you are in a directory which contains a subdirectory named foo which contains a justfile with the recipe build which is also the default recipe the following are all equivalent cd foo just build just foobuild just foo Additional recipes after the first are sought in the same justfile. For example the following are both equivalent just fooa b cd foo just a b And will both invoke recipes a and b in foojustfile. For consistency it possible to use path prefixes for all recipes just fooa foob But they must match just fooa barb error Conflicting path arguments foo and bar Include Directives The include directive currently unstable can be used to include the verbatim text of another file. If you have the following justfile include foobar. just a b echo A And the following text in foobar. just b echo B foobar. just will be included in justfile and recipe b will be defined just unstable b B just unstable a B A The include directive path can be absolute or relative to the location of the justfile containing it. include directives must appear at the beginning of a line. line. include directives are only processed before the first nonblank noncomment line. Included files can themselves contain include directives which are processed recursively. Hiding justfiles just looks for justfiles named justfile and. justfile which can be used to keep a justfile hidden. Just Scripts By adding a shebang line to the top of a justfile and making it executable just can be used as an interpreter for scripts cat script EOF usrbinenv just justfile foo echo foo EOF chmod x script. script foo echo foo foo When a script with a shebang is executed the system supplies the path to the script as an argument to the command in the shebang. So with a shebang of usrbinenv just justfile the command will be usrbinenv just justfile PATHTOSCRIPT. With the above shebang just will change its working directory to the location of the script. If youd rather leave the working directory unchanged use usrbinenv just workingdirectory. justfile. Note Shebang line splitting is not consistent across operating systems. The previous examples have only been tested on macOS. On Linux you may need to pass the S flag to env usrbinenv S just justfile default echo foo Dumping justfiles as JSON The dump command can be used with dumpformat json to print a JSON representation of a justfile. The JSON format is currently unstable so the unstable flag is required. Fallback to parent justfiles If a recipe is not found in a justfile and the fallback setting is set just will look for justfiles in the parent directory and up until it reaches the root directory. just will stop after it reaches a justfile in which the fallback setting is false or unset. As an example suppose the current directory contains this justfile set fallback foo echo foo And the parent directory contains this justfile bar echo bar just bar Trying.. justfile echo bar bar Avoiding Argument Splitting Given this justfile foo argument touch argument The following command will create two files some and argument. txt just foo some argument. txt The users shell will parse some argument. txt as a single argument but when just replaces touch argument with touch some argument. txt the quotes are not preserved and touch will receive two arguments. There are a few ways to avoid this quoting positional arguments and exported arguments. Quoting Quotes can be added around the argument interpolation foo argument touch argument This preserves justs ability to catch variable name typos before running for example if you were to write argument but will not do what you want if the value of argument contains single quotes. Positional Arguments The positionalarguments setting causes all arguments to be passed as positional arguments allowing them to be accessed with 1 2 and which can be then doublequoted to avoid further splitting by the shell set positionalarguments foo argument touch 1 This defeats justs ability to catch typos for example if you type 2 but works for all possible values of argument including those with double quotes. Exported Arguments All arguments are exported when the export setting is set set export foo argument touch argument Or individual arguments may be exported by prefixing them with foo argument touch argument This defeats justs ability to catch typos for example if you type argumant but works for all possible values of argument including those with double quotes. Configuring the Shell There are a number of ways to configure the shell for linewise recipes which are the default when a recipe does not start with a shebang. Their precedence from highest to lowest is The shelland shellargcommand line options. Passing either of these will cause justto ignore any settings in the current justfile. set windowsshell... set windowspowershelldeprecated set shell... Since set windowsshell has higher precedence than set shell you can use set windowsshell to pick a shell on Windows and set shell to pick a shell for all other platforms. Changelog A changelog for the latest release is available in CHANGELOG. md. Changelogs for previous releases are available on the releases page. just changelog can also be used to make a just binary print its changelog. Miscellanea Companion Tools Tools that pair nicely with just include Shell Alias For lightningfast command running put alias jjust in your shells configuration file. In bash the aliased command may not keep the shell completion functionality described in the next section. Add the following line to your. bashrc to use the same completion function as just for your aliased command complete F just o bashdefault o default j Shell Completion Scripts Shell completion scripts for Bash Zsh Fish PowerShell and Elvish are available in the completions directory. Please refer to your shells documentation for how to install them. The just binary can also generate the same completion scripts at runtime using the completions command just completions zsh just. zsh macOS Note Recent versions of macOS use zsh as the default shell. If you use Homebrew to install just it will automatically install the most recent copy of the zsh completion script in the Homebrew zsh directory which the builtin version of zsh doesnt know about by default. Its best to use this copy of the script if possible since it will be updated whenever you update just via Homebrew. Also many other Homebrew packages use the same location for completion scripts and the builtin zsh doesnt know about those either. To take advantage of just completion in zsh in this scenario you can set fpath to the Homebrew location before calling compinit. Note also that Oh My Zsh runs compinit by default. So your. zshrc file could look like this Init Homebrew which adds environment variables eval brew shellenv fpathHOMEBREWPREFIXsharezshsitefunctions fpath Then choose one of these options 1. If youre using Oh My Zsh you can initialize it here source ZSHohmyzsh. sh 2. Otherwise run compinit yourself autoload U compinit compinit Grammar A nonnormative grammar of justfiles can be found in GRAMMAR. md. just. sh Before just was a fancy Rust program it was a tiny shell script that called make. You can find the old version in extrasjust. sh. User justfiles If you want some recipes to be available everywhere you have a few options. First create a justfile in. user. justfile with some recipes. Recipe Aliases If you want to call the recipes in. user. justfile by name and dont mind creating an alias for every recipe add the following to your shells initialization script for recipe in just justfile. user. justfile summary do alias recipejust justfile. user. justfile workingdirectory. recipe done Now if you have a recipe called foo in. user. justfile you can just type foo at the command line to run it. It took me way too long to realize that you could create recipe aliases like this. Notwithstanding my tardiness I am very pleased to bring you this major advance in justfile technology. Forwarding Alias If youd rather not create aliases for every recipe you can create a single alias alias. jjust justfile. user. justfile workingdirectory. Now if you have a recipe called foo in. user. justfile you can just type. j foo at the command line to run it. Im pretty sure that nobody actually uses this feature but its there. Customization You can customize the above aliases with additional options. For example if youd prefer to have the recipes in your justfile run in your home directory instead of the current directory alias. jjust justfile. user. justfile workingdirectory Node. js package. json Script Compatibility The following export statement gives just recipes access to local Node module binaries and makes just recipe commands behave more like script entries in Node. js package. json files export PATH. nodemodules. bin envvarPATH Alternatives and Prior Art There is no shortage of command runners Some more or less similar alternatives to just include make The Unix build tool that inspired just. There are a few different modern day descendents of the original make including FreeBSD Make and GNU Make. task A YAMLbased command runner written in Go. maid A Markdownbased command runner written in JavaScript. microsoftjust A JavaScriptbased command runner written in JavaScript. cargomake A command runner for Rust projects. mmake A wrapper around makewith a number of improvements including remote includes. robo A YAMLbased command runner written in Go. mask A Markdownbased command runner written in Rust. makesure A simple and portable command runner written in AWK and shell. haku A makelike command runner written in Rust. Contributing just welcomes your contributions just is released under the maximally permissive CC0 public domain dedication and fallback license so your changes must also be released under this license. Janus Janus is a tool that collects and analyzes justfiles and can determine if a new version of just breaks or changes the interpretation of existing justfiles. Before merging a particularly large or gruesome change Janus should be run to make sure that nothing breaks. Dont worry about running Janus yourself Casey will happily run it for you on changes that need it. Minimum Supported Rust Version The minimum supported Rust version or MSRV is current stable Rust. It may build on older versions of Rust but this is not guaranteed. New Releases New releases of just are made frequently so that users quickly get access to new features. Release commit messages use the following template Release x. y. z Bump version x. y. z x. y. z Update changelog Update changelog contributor credits Update dependencies Update man page Update version references in readme Frequently Asked Questions What are the idiosyncrasies of Make that Just avoids make has some behaviors which are confusing complicated or make it unsuitable for use as a general command runner. One example is that under some circumstances make wont actually run the commands in a recipe. For example if you have a file called test and the following makefile test. test make will refuse to run your tests make test make test is up to date. make assumes that the test recipe produces a file called test. Since this file exists and the recipe has no other dependencies make thinks that it doesnt have anything to do and exits. To be fair this behavior is desirable when using make as a build system but not when using it as a command runner. You can disable this behavior for specific targets using makes builtin. PHONY target name but the syntax is verbose and can be hard to remember. The explicit list of phony targets written separately from the recipe definitions also introduces the risk of accidentally defining a new nonphony target. In just all recipes are treated as if they were phony. Other examples of makes idiosyncrasies include the difference between and in assignments the confusing error messages that are produced if you mess up your makefile needing to use environment variables in recipes and incompatibilities between different flavors of make. Whats the relationship between Just and Cargo build scripts cargo build scripts have a pretty specific use which is to control how cargo builds your Rust project. This might include adding flags to rustc invocations building an external dependency or running some kind of codegen step. just on the other hand is for all the other miscellaneous commands you might run as part of development. Things like running tests in different configurations linting your code pushing build artifacts to a server removing temporary files and the like. Also although just is written in Rust it can be used regardless of the language or build system your project uses. Further Ramblings I personally find it very useful to write a justfile for almost every project big or small. On a big project with multiple contributors its very useful to have a file with all the commands needed to work on the project close at hand. There are probably different commands to test build lint deploy and the like and having them all in one place is useful and cuts down on the time you have to spend telling people which commands to run and how to type them. And with an easy place to put commands its likely that youll come up with other useful things which are part of the projects collective wisdom but which arent written down anywhere like the arcane commands needed for some part of your revision control workflow install all your projects dependencies or all the random flags you might need to pass to the build system. Some ideas for recipes Deployingpublishing the project Building in release mode vs debug mode Running in debug mode or with logging enabled Complex git workflows Updating dependencies Running different sets of tests for example fast tests vs slow tests or running them with verbose output Any complex set of commands that you really should write down somewhere if only to be able to remember them Even for small personal projects its nice to be able to remember commands by name instead of Reverse searching your shell history and its a huge boon to be able to go into an old project written in a random language with a mysterious build system and know that all the commands you need to do whatever you need to do are in the justfile and that if you type just something useful or at least interesting will probably happen. For ideas for recipes check out this projects justfile or some of the justfiles out in the wild. Anyways I think thats about it for this incredibly longwinded README. I hope you enjoy using just and find great success and satisfaction in all your computational endeavors,No Headline,Extremely Negative,230,"['sin', 'object', 'object', 'issue']",[],"[John, Doe, John, Doe, John, John]"
26,Show HN: I wrote a WebAssembly Interpreter and Toolkit in C,Web49: WebAssembly Interpeter. Contribute to FastVM/Web49 development by creating an account on GitHub.,https://github.com/FastVM/Web49,Web49 Web49 is a WebAssembly toolkit and interpreter. v0. 0. 1Try the first release Web49 contains a few tools for working with WebAssembly. interpreters miniwasm fast wasm interpreter suports multiple wasm formats wasm binary format 100 complete wasm text format 95 complete wasm spect test 75 complete includes a custom WASI implementation raywasm wasm interpreter based on miniwasm includes raylib bindings generated from json miniwasm wasm binary tools rewrite much simpler than WABTs tools much smaller than Binaryens tools wat2wasm convert wasm text into wasm binary wasm2wat generates nearly identical wat as binaryen or wabt turn wasm binary into wasm text supports all of wasm 1. 0 and some extensions wasm2wasm shrink numbers in wasm files generated by llvm saves 04 bytes per 32 bit number saves 010 bytes per 64 bit number round trip parse and reemit shrink numbers in wasm files generated by llvm Benchmarks Benchmarks performed by wasm3 and Web49s miniwasm. Ran on an 2020 Macbook Air M1 8GiB ram using bench. py. All Benchmarks compiled with emcc O2 One can also view results run on github actions One can run the benchmarks for themselves. python3 with pip emcc wasm3 make tested with GNUMake works with BSDMake too and a C compiler gccclangtcc works msvc would work if someone rewrites two macros define NEXT break define LABELX case X probably a couple others git clone httpsgithub. comfastvmweb49 cd web49 make CCgcc gcc is fastest in my tests python3 m pip install matplotlib python3 bench. py,No Headline,Extremely Negative,1540,"['Errors', 'errors', 'errors', 'Unknown', 'arbitrary', 'issue', 'Discord', 'bug', 'bash', 'bash', 'fail', 'incompatible', 'break', 'bugs', 'break', 'unstable', 'unstable', 'incompatible', 'insensitive', 'error', 'fails', 'error', 'suppress', 'error', 'false', 'false', 'false', 'bash', 'bash', 'Bash', 'bash', 'Rust', 'breaks', 'ignore', 'Errors', 'fails', 'Rust', 'fail', 'unwanted', 'broken', 'Error', 'Abort', 'error', 'randomly', 'disabled', 'inequality', 'error', 'error', 'error', 'error', 'error', 'crashes', 'crashes', 'fail', 'fail', 'arbitrary', 'limitations', 'bash', 'bash', 'split', 'split', 'Bash', 'bash', 'bash', 'Bash', 'strictly', 'bash', 'fails', 'undefined', 'fails', 'problem', 'undefined', 'bash', 'challenging', 'bash', 'error', 'error', 'dumping', 'bash', 'bash', 'bash', 'error', 'fails', 'error', 'fatal', 'error', 'suppress', 'error', 'fatal', 'fuzzy', 'split', 'strange', 'error', 'Conflicting', 'unstable', 'unstable', 'unstable', 'splitting', 'Dumping', 'dump', 'unstable', 'unstable', 'false', 'Splitting', 'splitting', 'ignore', 'bash', 'Bash', 'Rust', 'shortage', 'Rust', 'Rust', 'Rust', 'breaks', 'gruesome', 'breaks', 'worry', 'Rust', 'Rust', 'Rust', 'Rust', 'Bump', 'confusing', 'complicated', 'refuse', 'disable', 'hard', 'risk', 'confusing', 'error', 'mess', 'Rust', 'miscellaneous', 'Rust', 'slow']",[],"[Kakoune, Kakoune, trimendmatchs, pat, pat, trimendmatchess, pat, pat, pat, trimstartmatchess, pat, pat, foois, foobar, txtis, foo, ., txtis, foobar, baris, foobar, ., Larry, Wall, Hin, sh, usrbinenv, Hola, Larry, Wall, echo, x, Larry, Wall, foo, Foo, fooa, barb, maid, cargomake, mask, makesure, haku, Casey]"
27,Serverless Video Transcription inspired by Cyberpunk 2077,Contribute to elanmart/cbp-translate development by creating an account on GitHub.,https://github.com/elanmart/cbp-translate,1. Introduction 1. 1 The main idea I finally got around to playing Cyberpunk 2077 the other day and I noticed that the game has one interesting feature when a character speaks a foreign language the text first appears above them in the original form and then gets sort of livetranslated into English. Ive then asked myself how much work would it take to build something like that with modern DL stack Is it possible to do it over a weekend 1. 2 The rough requirements I wanted to have a system which would Process short video clips e. g. a single scene Work with multiple characters speakers Detect and transcribe speech in both English and Polish Translate the speech to any language Assign each phrase to a speaker Show the speaker on the screen Add subtitles to the original video in a way mimicking the Cyberpunk example Have a nice frontend Run remotely in the cloud 1. 3 The TLDR With the amazing ML ecosystem we have today its definitely possible to build a PoC of a system like that in a couple of evenings. The offtheshelf tools are quite robust and mostly extremely easy to integrate. Whats more the abundance of pretrained models meant that I could build the whole app without running a single gradient update or handlabeling a single example. As for the timelines it definitely took me more time than I anticipated but actually most of the time was spent on nonML issues like figuring out how to add Unicode characters to a video frame. Heres a 60s clip of an interview conducted in Polish translated to English. You can see that we a very clean setup like this the results actually look quite OK politicalinterviewRMFtranslated. mp4 And heres a part of an interview with Keanu Reeves who plays a major character in Cyberpunk 2077 talking to Steven Colbert translated to Polish. Note that in this case the speaker diarization is not perfect and the speaker IDs get mixed up for a moment midvideo keanureevesinterviewtranslated. mp4 2. Implementation I glued together a couple of tools to make this thing fly ffmpegpython for processing the video files e. g. extracting audio streaming raw frames Whisper for speech recognition NVIDIA NeMo for speaker diarization note I also tested PyAnnote but the results were not satisfactory DeepL for translation RetinaFace for face detection DeepFace for face embedding scikitlearn for detecting unique faces via clustering Gradio for a nice demo frontend Modal for serverless deployment Theres also PIL OpenCV used to annotate the video frames and ytdlp to download samples from YT. Heres a sketch of how these things work together to produce the final output 2. 1 Handling the speech Extracting audio from a webm mp4 is trivial with ffmpeg def extractaudiopath str pathout Optionalstr None Extract audio from a video file using ffmpeg audio ffmpeg. inputpath. audio output ffmpeg. outputaudio pathout output ffmpeg. overwriteoutputoutput ffmpeg. runoutput quietTrue return pathout Once we have the sound extracted we can process it with 2. 1. 1 Whisper There isnt much to say about Whisper really. Its a fantastic tool which recognizes english speech better than me. It handles multiple languages and works okay even with overlapping speech. Ive decided to feed the whole audio stream to whisper as a single input but if you wanted to improve this part of the code you could experiment with partitioning the audio for each speaker but my bet is that this will not give any better results. 2. 1. 2 DeepL I could use a pretrained Neural Machine Translation model here or use Whisper since it also does translation but I wanted to get the highest quality possible. In my experience DeepL works better than Google Translate and their API gives you 500k characters month for free. They also provide a convenient python interface. To improve this part of the code one could try to translate the text from each speaker separately maybe then the translation would be even more coherent But this would strongly depend on our ability to accurately assign the phrases to speakers. 2. 1. 3 Speaker Diarization NeMo and PyAnnote Speaker diarization is a process assigning speaker IDs to each time point in the audio signal. 2. 1. 3. 1 PyAnnote I initially used PyAnnote for this purpose since its available on HuggingFace and extremely straightforward to integrate into your codebase pipeline Pipeline. frompretrained pyannotespeakerdiarization2. 1. 1 useauthtokenauthtoken cachedircachedir dia pipelinepathaudio Unfortunately the quality was not really satisfactory and errors in this part of the pipeline were hurting all of the downstream steps. 2. 1. 3. 2 NeMo I then turned to NeMo from good folks at NVIDIA. In their words NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition ASR texttospeech synthesis TTS large language models LLMs and natural language processing NLP. I found it to be quite reliable especially for english. It still struggles with short segments of overlapping speech but its definitely good enough for the demo. The biggest downside is that NeMo is a research toolkit. Therefore simple tasks like give me unique IDs for this audio file result in a code that is much more messy than the PyAnnote version. Note that I mainly tested it on rather highquality interviewtype audio. I do not know how this would translate to other scenarios or very different languages e. g. Japanese. 2. 1. 4 Matching speaker IDs to spoken phrases I used a simple heuristic here where for every section of speech output from NeMo we find the phrase detected by Whisper with the largest overlap. This part of the code could definitely be improved with a more sophisticated approach. It would also be good to look more into the timestamps returned by the two systems since for some reason I had an impression that an offset 2. 2 Handling video streams This is pretty straightforward with cv2 and ffmpeg. The main tip is that for video processing generators are the way to go you probably dont want to load 1 minute video into a numpy array 1920 1080 3 24 60 entries will take 35GB of RAM. 2. 2. 1 Detecting faces in video Detecting faces is luckily super straightforward with modern tools like RetinaFace or MTCNN. In this first step we run a pretrained model to detect all faces visible in each frame. We then crop align and resize them as required by the downstream embedding model as in this example from the original repo This step is quite robust and reliable the only downside is that it relies on Tensorflow and the code can only handle single frame at a time. Its quite timeconsuming to run this detection for every frame in a video so this part of the code could definitely use some optimizations. With a modern GPU it takes several minutes to process 60s of video. Luckily with Modal we can use massive parallelization so the runtime is shorter even if processing happens on singleCPU machines. 2. 2. 2 Embedding faces and assigning them unique IDs Once weve located faces in each frame we can use a pretrained model to extract embeddings for each of them. For this Ive grabbed the FaceNet512 model from DeepFace library. Once embeddings are extracted we still need to assign them unique IDs. To do this I went with a simple hierarchical clustering algorithm or specifically Agglomerative Clustering from scikitlearn Agglomerative Clustering will recursively merge clusters as long as the distance between them is below a certain threshold. That threshold is model and metricspecific. Here I used same value which is used by DeepFace when performing face verification. This part of the code could be improved in many ways Improve the clustering algorithm by either Using a different algorithm e. g. DBSCAN Using more domain knowledge e. g. the fact that faces with similar locations in consecutive frames are likely to be the same person no two faces in a single frame can be a single person etc. Investigate if it would be a good idea to identify a couple of best frames where the face is in the best position and use them as a template. Enforce temporal consistency predictions should not be made for each frame in isolation. Improve the embeddings themselves e. g. by using a combination of models or different distance metrics Heres a visual representation of how the Agglomerative Clustering works by changing the threshold cutoff on the Yaxis you will end up with different number of clusters. 2. 2. 3 Matching Face IDs to Speaker IDs For this we employ another simple heuristic for each face we create a set of frames where that face was detected. We then do the same for speakers create a set of frames where a given speaker can be heard. Now for each face ID we find the speaker ID for which Jaccard index between the two sets is minimized. 2. 2. 4 Generating the frames Once we have annotated every frame with a speaker ID face ID phrase in original language and phrase in the translated language we can finally add subtitles. Even though our system does not work in real time I wanted to give it a similar look to the Cyberpunk example so as a last processing step I calculate how many characters from a recognized phrase should be displayed on the frame. What remains now is to figure out how to place the subtitles such that they fit on the screen etc. This part of the code could be improved to handle more languages. To place UTF8 characters on the screen I need to explicitly pass a path to a font file to PIL. The problem is that different languages require different fonts so the current solution wont work e. g. for Korean. 2. 3 Deployment 2. 3. 1 Gradio frontend Last thing I wanted to check is how easy it is to deploy this system on a cloud platform. Getting the frontend ready can be trivially done with Gradio. 2. 3. 2 Serverless backend with Modal and FastAPI We could try to deploy the model with Huggingface Spaces but I wanted to try something a bit more productionready. I went ahead with Modal a serverless platform built by Erik Bernhardsson and his team. You can read more about it in his blog post Modal is really appealing since it allows me to write the code exactly how I imagined the programming for the cloud should look like. What locally youd write as def runasraudiopath str return whisper. transcribeaudiopath def processsingleframeframe np. ndarray text str frame addsubtitlesframe text return frame With Modal becomes stub. functionimagegpuimage gpuTrue def runasraudiopath str return whisper. transcribeaudiopath stub. functionimagecpuimage def processsingleframeframe np. ndarray text str frame addsubtitlesframe text return frame So with minimal boilerplate we now have a code that can run remotely within seconds. Pretty wild. There are obviously still some rough edges Modal is still in beta and I had to work around one last issue when running a FastAPI app there is a 45 second limit for each request. And since processing a video takes a bit longer I used a notsonice workaround where pressing Submit for the first time gives you the job id and you can use that id to fetch the final result Limitations This is very obviously just a demo proofofconcept The main limitations are Processing 30s of video takes several minutes on a modern PC The approach used here will not work well for clips with multiple scenes Matching faces to voices relies on simple cooccurrence heuristic and will not work in certain scenarios e. g. if the whole conversation between two people is recorded from a single angle All the steps of the pipeline rely on imperfect tools e. g. diarization or simplistic heuristics e. g. finding unique faces with agglomerative clustering The pipeline was only tested on a handful of examples Development With Modal First of all youll need a Modal account see httpsmodal. com Youll then need to add your HuggingFace HUGGINGFACETOKEN and DeepL DEEPLKEY authentication tokens as Secrets in Modal dashboard. Once this is set up you should be able to simply run python m venv. venv source. venvbinactivate python m pip install r requirementsmodal. txt You can then run it with python cbptranslateapp. py Locally First export the necessary env variables export DEEPLKEY... export HUGGINGFACETOKEN... export MODALRUNLOCALLY1 Then its best if you look at the steps defined in cbptranslatemodalremote. py Roughly it goes something like this Apt sudo apt install ffmpeg libsndfile1 git buildessential We skip conda steps assuming you have cuda and cudnn installed echo Skipping CUDA installation pip python m venv. venv source. venvbinactivate python m pip install upgrade pip setuptools python m pip install r requirementslocal. txt Install the package for development python setup. py develop Run the CLI python python cbptranslatecli. py pathin. assetsvideoskeanureevesinterview. mp4 pathout. translated. mp4 language PL Note on gitlfs There are several large files included in this repo which are stored using gitlfs. To clone the repo without downloading the large files run GITLFSSKIPSMUDGE1 git clone...,No Headline,Extremely Negative,200,['break'],[],[]
28,Conc: Better Structured Concurrency for Go,Better structured concurrency for go. Contribute to sourcegraph/conc development by creating an account on GitHub.,https://github.com/sourcegraph/conc,conc better structured concurrency for go conc is your toolbelt for structured concurrency in go making common tasks easier and safer. go get github. comsourcegraphconc At a glance Use conc. WaitGroupif you just want a safer version of sync. WaitGroup Use pool. Poolif you want a concurrencylimited task runner Use pool. ResultPoolif you want a concurrent task runner that collects task results Use pool. ResultErrorPoolif your tasks are fallible Use pool. ResultContextPoolif your tasks should be canceled on failure Use stream. Streamif you want to process an ordered stream of tasks in parallel with serial callbacks Use iter. Mapif you want to concurrently map a slice Use iter. ForEachif you want to concurrently iterate over a slice Use panics. Catcherif you want to catch panics in your own goroutines All pools are created with pool. New or pool. NewWithResultsT then configured with methods p. WithMaxGoroutinesconfigures the maximum number of goroutines in the pool p. WithErrorsconfigures the pool to run tasks that return errors p. WithContextctxconfigures the pool to run tasks that should be canceled on first error p. WithFirstErrorconfigures error pools to only keep the first returned error rather than an aggregated error p. WithCollectErroredconfigures result pools to collect results even when the task errored Goals The main goals of the package are Make it harder to leak goroutines Handle panics gracefully Make concurrent code easier to read Goal 1 Make it harder to leak goroutines A common pain point when working with goroutines is cleaning them up. Its really easy to fire off a go statement and fail to properly wait for it to complete. conc takes the opinionated stance that all concurrency should be scoped. That is goroutines should have an owner and that owner should always ensure that its owned goroutines exit properly. In conc the owner of a goroutine is always a conc. WaitGroup. Goroutines are spawned in a WaitGroup with WaitGroup. Go and WaitGroup. Wait should always be called before the WaitGroup goes out of scope. In some cases you might want a spawned goroutine to outlast the scope of the caller. In that case you could pass a WaitGroup into the spawning function. func main var wg conc. WaitGroup defer wg. Wait startTheThingwg func startTheThingwg conc. WaitGroup wg. Gofunc... For some more discussion on why scoped concurrency is nice check out this blog post. Goal 2 Handle panics gracefully A frequent problem with goroutines in longrunning applications is handling panics. A goroutine spawned without a panic handler will crash the whole process on panic. This is usually undesirable. However if you do add a panic handler to a goroutine what do you do with the panic once you catch it Some options Ignore it Log it Turn it into an error and return that to the goroutine spawner Propagate the panic to the goroutine spawner Ignoring panics is a bad idea since panics usually mean there is actually something wrong and someone should fix it. Just logging panics isnt great either because then there is no indication to the spawner that something bad happened and it might just continue on as normal even though your program is in a really bad state. Both 3 and 4 are reasonable options but both require the goroutine to have an owner that can actually receive the message that something went wrong. This is generally not true with a goroutine spawned with go but in the conc package all goroutines have an owner that must collect the spawned goroutine. In the conc package any call to Wait will panic if any of the spawned goroutines panicked. Additionally it decorates the panic value with a stacktrace from the child goroutine so that you dont lose information about what caused the panic. Doing this all correctly every time you spawn something with go is not trivial and it requires a lot of boilerplate that makes the important parts of the code more difficult to read so conc does this for you. Goal 3 Make concurrent code easier to read Doing concurrency correctly is difficult. Doing it in a way that doesnt obfuscate what the code is actually doing is more difficult. The conc package attempts to make common operations easier by abstracting as much boilerplate complexity as possible. Want to run a set of concurrent tasks with a bounded set of goroutines Use pool. New. Want to process an ordered stream of results concurrently but still maintain order Try stream. New. What about a concurrent map over a slice Take a peek at iter. Map. Browse some examples below for some comparisons with doing these by hand. Examples Each of these examples forgoes propagating panics for simplicity. To see what kind of complexity that would add check out the Goal 2 header above. Spawn a set of goroutines and waiting for them to finish Process each element of a stream in a static pool of goroutines Process each element of a slice in a static pool of goroutines Concurrently map a slice Process an ordered stream concurrently Status This package is currently pre1. 0. There are likely to be minor breaking changes before a 1. 0 release as we stabilize the APIs and tweak defaults. Please open an issue if you have questions concerns or requests that youd like addressed before the 1. 0 release. Currently a 1. 0 is targeted for March 2023.,No Headline,Extremely Negative,380,"['rough', 'issues', 'trivial', 'Unfortunately', 'errors', 'hurting', 'struggles', 'downside', 'messy', 'downside', 'isolation', 'problem', 'rough', 'issue', 'limit', 'Limitations', 'limitations', 'imperfect', 'simplistic']",[],"[Keanu, Reeves, Steven, Colbert, Jaccard, Erik, Bernhardsson]"
29,"33 Years Later, There's Finally a Capture Card for Recording Game Boy Gameplay","If you needed any more proof that the 33-year-old Nintendo Game Boy still has a devoted fan base, Sebastian Staacks has created a simple plug-and-play device that allows gameplay from the handheld to be streamed to a computer over USB and recorded, without re…",https://gizmodo.com/game-boy-capture-card-streaming-twitch-youtube-native-1849918530,If you needed any more proof that the 33yearold Nintendo Game Boy still has a devoted fan base Sebastian Staacks has created a simple plugandplay device that allows gameplay from the handheld to be streamed to a computer over USB and recorded without requiring any modifications to the console. This isnt the first time weve covered Staacks prowess with making the Game Boy do things its never done before. Earlier this year they created a custom cartridge that allowed the handheld to stream video wirelessly which they demonstrated by showing Star Wars playing on the Game Boys ugly green and gray screen. That was followed up with a hack that turned the classic Nintendo handheld into a game streaming device capable of playing modern titles like Grand Theft Auto V. Those hacks eventually gave Staacks the idea for a device that Game Boy fans probably didnt realize they needed. Although it wasnt a feature at launch there are now a few ways to capture Game Boy gameplay footage. The original console can be modded with a video out connection or gamers can opt for modern alternatives like the Analogue Pocket which can connect to a capture card over HDMI. The easiest solution is to just play classic Game Boy titles through a software emulator running natively on a PC but for competitive players or speed runners hoping to set official records the original Game Boy hardware is a must. The GB Interceptor is reminiscent of the Game Shark and Game Genie devices of yesteryear which interfaced between a console and a game cartridge to facilitate cheating or other game modifications. You insert the GB Interceptor into a Game Boys cartridge slot insert a game cartridge into the GB Interceptor and then connect it to a PC with a USBC cable. The PC treats the signal from the GB Interceptor like a signal from a webcam allowing Game Boy gameplay to be recorded or even streamed across the internet. Yes that means you can use the Game Boy Camera as your webcam for Zoom meetings if you so choose. For those not familiar with the inner workings of the Game Boy there is no video information passed between the game cart and the console for the GB Interceptor to... intercept. Instead what its essentially doing is intercepting game play commands entered through the Game Boys controls then replicating them perfectly through a custom Game Boy emulator Staacks created. So whats being streamed over USBC is a perfect realtime copy of what the player sees on the Game Boys native screen. In its current form the GB Interceptor does have a couple of limitations. It only works with original Game Boy games although the custom cartridge can be used on the Game Boy Color or Game Boy Advance devicesjust with OG games. It also cant recreate a games audio so that would need to be separately captured using the Game Boys headphone jack. If thats not a dealbreaker Staacks has provided all of the necessary code and source files to make your own on GitHub plus a detailed build video shared on YouTube but unfortunately doesnt seem to have any plans to manufacture and sell these en masse.,No Headline,Extremely Negative,620,"['errors', 'error', 'error', 'error', 'error', 'leak', 'leak', 'pain', 'fail', 'opinionated', 'problem', 'panic', 'panic', 'undesirable', 'panic', 'panic', 'Ignore', 'error', 'panic', 'bad', 'wrong', 'bad', 'bad', 'wrong', 'panic', 'panicked', 'panic', 'lose', 'panic', 'trivial', 'difficult', 'difficult', 'difficult', 'static', 'static', 'breaking', 'issue', 'concerns']",['fire'],[]
