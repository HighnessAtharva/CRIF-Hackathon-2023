Article,Title,Description,URL,Content,Published,Headline,Headline Sentiment,Offense Rating,Negative Words,Offensive Words,Tags
1,"TIOBE Calculates C++, C, and Python Rose the Most in Popularity in 2022","""The Tiobe index gauges language popularity using a formula that assesses searches on programming languages in Google, Bing, Yahoo, Wikipedia, and other search engines,"" writes InfoWorld. And they add that this year the ""vaunted"" C++ programming language was â€¦",https://developers.slashdot.org/story/23/01/08/0252232/tiobe-calculates-c-c-and-python-rose-the-most-in-popularity-in-2022,TIOBE Calculates C C and Python Rose the Most in Popularity in 2022 infoworld. com 83 TIOBEs announcement includes their calculation that C rose 4. 62 in popularity in 2022 Runners up are C 3. 82 and Python 2. 78. Interestingly C surpassed Java to become the number 3 of the TIOBE index in November 2022. The reason for Cs popularity is its excellent performance while being a high level objectoriented language. Because of this it is possible to develop fast and vast software systems over millions of lines of code in C without necessarily ending up in a maintenance nightmare. So which programming languages are most popular now For what its worth heres TIOBEs latest ranking Python C C Java C Visual Basic JavaScript SQL Assembly Language PHP InfoWorld adds that Helping C popularity was the publication of new language standards with interesting features such as C 11 and C 20. More from TIOBE What else happened in 2022 Performance seemed to be important. C competitor Rust entered the top 20 again being at position 26 one year ago but this time it seems to be for real. Lua which is known for its easy interfacing with C jumped from position 30 to 24. F is another language that made an interesting move from position 74 to position 33 in one years time. Promising languages such as Kotlin from 29 to 25 Julia from 28 to 29 and Dart from 37 to 38 still have a long way to go before they reach the top 20. Lets see what happens in 2023.,2023-01-08T08:34:00Z,"
			Get a Special Deal on This Top Python Bundle
		",Extremely Positive,70,"['Layoff', 'Skeptic', 'Divisive']",[],"[Chris, Mall, 'harassment', Elon, Musk]"
2,A Guide To Command-Line Data Manipulation,"No more random scripts in Python and JavaScript to transform CSV or JSON data. In this article, Alvin Bryan shows you how to use Miller, a small and powerful CLI tool, to do all your data processing.",https://www.smashingmagazine.com/2022/12/guide-command-line-data-manipulation-cli-miller/,A Guide To CommandLine Data Manipulation Allow me to preface this article by saying that Im not a terminal person. I dont use Vim. I find sed grep and awk convoluted and counterintuitive. I prefer seeing my files in a nice UI. Despite all that I got into the habit of reaching for commandline interfaces CLIs when I had small dedicated tasks to complete. Why Ill explain all of that below. In this article youll also learn how to use a CLI tool named Miller to manipulate data from CSV TSV andor JSON files. Why Use The Command Line Everything that Im showing here can be done with regular code. You can load the file parse the CSV data and then transform it using regular JavaScript Python or any other language. But there are a few reasons why I reach out for commandline interfaces CLIs whenever I need to transform data Easier to read. It is faster for me to write a script in JavaScript or Python for my usual data processing. But a script can be confusing to come back to. In my experience commandline manipulations are harder to write initially but easier to read afterward. Easier to reproduce. Thanks to package managers like Homebrew CLIs are much easier to install than they used to be. No need to figure out the correct version of Node. js or Python the package manager takes care of that for you. Ages well. Compared to modern programming languages CLIs are old. They change a lot more slowly than languages and frameworks. What Is Miller The main reason I love Miller is that its a standalone tool. There are many great tools for data manipulation but every other tool I found was part of a specific ecosystem. The tools written in Python required knowing how to use pip and virtual environments for those written in Rust it was cargo and so on. On top of that its fast. The data files are streamed not held in memory which means that you can perform operations on large files without freezing your computer. As a bonus Miller is actively maintained John Kerl really keeps on top of PRs and issues. As a developer I always get a satisfying feeling when I see a neat and maintained opensource project with great documentation. Installation Linux aptget install milleror Homebrew. macOS brew install millerusing Homebrew. Windows choco install millerusing Chocolatey. Thats it and you should now have the mlr command available in your terminal. Run mlr help topics to see if it worked. This will give you instructions to navigate the builtin documentation. You shouldnt need it though thats what this tutorial is for How mlr Works Miller commands work the following way mlr inputoutput file formats verbs file Example mlr csv filter color red example. csv Lets deconstruct csvspecifies the input file format. Its a CSV file. filteris what were doing on the file called a verb in the documentation. In this case were filtering every row that doesnt have the field colorset to red. There are many other verbs like sortand cutthat well explore later. example. csvis the file that were manipulating. Operations Overview We can use those verbs to run specific operations on your data. Theres a lot we can do. Lets explore. Data Ill be using a dataset of IMDb ratings for American TV dramas created by The Economist. You can download it here or find it in the repo for this article. Note For the sake of brevity Ive renamed the file from mlr csv head. IMDbEconomisttvratings. csv to tvratings. csv. Above I mentioned that every command contains a specific operation or verb. Lets learn our first one called head. What it does is show you the beginning of the file the head rather than print the entire file in the console. You can run the following command mlr csv head. tvratings. csv And this is the output youll see titleIdseasonNumbertitledateavratingsharegenres tt2879552111. 22. 63201603108. 4890. 51DramaMysterySciFi tt3148266112 Monkeys201502278. 34070. 46AdventureDramaMystery tt3148266212 Monkeys201605308. 81960. 25AdventureDramaMystery tt3148266312 Monkeys201705199. 03690. 19AdventureDramaMystery tt3148266412 Monkeys201806269. 13630. 38AdventureDramaMystery tt1837492113 Reasons Why201703318. 4372. 38DramaMystery tt1837492213 Reasons Why201805187. 50892. 19DramaMystery tt0285331124200202168. 56416. 67ActionCrimeDrama tt0285331224200302098. 70287. 13ActionCrimeDrama tt0285331324200402098. 71735. 88ActionCrimeDrama This is a bit hard to read so lets make it easier on the eye by adding opprint. mlr csv opprint head. tvratings. csv The resulting output will be the following titleId seasonNumber title date avrating share genres tt2879552 1 11. 22. 63 20160310 8. 489 0. 51 DramaMysterySciFi tt3148266 1 12 Monkeys 20150227 8. 3407 0. 46 AdventureDramaMystery tt3148266 2 12 Monkeys 20160530 8. 8196 0. 25 AdventureDramaMystery tt3148266 3 12 Monkeys 20170519 9. 0369 0. 19 AdventureDramaMystery tt3148266 4 12 Monkeys 20180626 9. 1363 0. 38 AdventureDramaMystery tt1837492 1 13 Reasons Why 20170331 8. 437 2. 38 DramaMystery tt1837492 2 13 Reasons Why 20180518 7. 5089 2. 19 DramaMystery tt0285331 1 24 20020216 8. 5641 6. 67 ActionCrimeDrama tt0285331 2 24 20030209 8. 7028 7. 13 ActionCrimeDrama tt0285331 3 24 20040209 8. 7173 5. 88 ActionCrimeDrama Much better isnt it Note Rather than typing csv opprint every time we can use the c2p option which is a shortcut. Chaining Thats where the fun begins. Rather than run multiple commands we can chain the verbs together by using the then keyword. Remove columns You can see that theres a titleId column that isnt very useful. Lets get rid of it using the cut verb. mlr c2p cut x f titleId then head. tvratings. csv It gives you the following output seasonNumber title date avrating share genres 1 11. 22. 63 20160310 8. 489 0. 51 DramaMysterySciFi 1 12 Monkeys 20150227 8. 3407 0. 46 AdventureDramaMystery 2 12 Monkeys 20160530 8. 8196 0. 25 AdventureDramaMystery 3 12 Monkeys 20170519 9. 0369 0. 19 AdventureDramaMystery 4 12 Monkeys 20180626 9. 1363 0. 38 AdventureDramaMystery 1 13 Reasons Why 20170331 8. 437 2. 38 DramaMystery 2 13 Reasons Why 20180518 7. 5089 2. 19 DramaMystery 1 24 20020216 8. 5641 6. 67 ActionCrimeDrama 2 24 20030209 8. 7028 7. 13 ActionCrimeDrama 3 24 20040209 8. 7173 5. 88 ActionCrimeDrama Fun Fact This is how I first learned about Miller I was playing with a CSV dataset for httpsdetails. town that had a useless column and I looked up how to remove a column from CSV command line. I discovered Miller loved it and then pitched an article to Smashing magazine. Now here we are Filter This is the verb that I first showed earlier. We can remove all the rows that dont match a specific expression letting us clean our data with only a few characters. If we only want the rating of the first seasons of every series in the dataset this is how you do it mlr c2p filter seasonNumber 1 then head. tvratings. csv Sorting We can sort our data based on a specific column like it would be in a UI like Excel or macOS Numbers. Heres how you would sort your data based on the series with the highest rating mlr c2p sort nr avrating then head. tvratings. csv The resulting output will be the following titleId seasonNumber title date avrating share genres tt0098887 1 Parenthood 19901113 9. 6824 1. 68 ComedyDrama tt0106028 6 Homicide Life on the Street 19971205 9. 6 0. 13 CrimeDramaMystery tt0108968 5 Touched by an Angel 19981115 9. 6 0. 08 DramaFamilyFantasy tt0903747 5 Breaking Bad 20130220 9. 554 18. 95 CrimeDramaThriller tt0944947 6 Game of Thrones 20160525 9. 4943 15. 18 ActionAdventureDrama tt3398228 5 BoJack Horseman 20180914 9. 4738 0. 45 AnimationComedyDrama tt0103352 3 Are You Afraid of the Dark 19940223 9. 4349 2. 6 DramaFamilyFantasy tt0944947 4 Game of Thrones 20140509 9. 4282 11. 07 ActionAdventureDrama tt0976014 4 Greek 20110307 9. 4 0. 01 ComedyDrama tt0090466 4 L. A. Law 19900405 9. 4 0. 1 Drama We can see that Parenthood from 1990 has the highest rating on IMDb who knew Saving Our Operations By default Miller only prints your processed data to the console. If we want to save it to another CSV file we can use the operator. If we wanted to save our sorted data to a new CSV file this is what the command would look like mlr csv sort nr avrating. tvratings. csv sorted. csv Convert CSV To JSON Most of the time you dont use CSV data directly in your application. You convert it to a format that is easier to read or doesnt require additional dependencies like JSON. Miller gives you the c2j option to convert your data from CSV to JSON. Heres how to do this for our sorted data mlr c2j sort nr avrating. tvratings. csv sorted. json Case study Top 5 Athletes With Highest Number Of Medals In Rio 2016 Lets apply everything we learned above to a realworld use case. Lets say that you have a detailed dataset of every athlete who participated in the 2016 Olympic games in Rio and you want to know who the 5 with the highest number of medals are. First download the athlete data as a CSV then save it in a file named athletes. csv. Lets open up the following file mlr c2p head. athletes. csv The resulting output will be something like the following id name nationality sex dateofbirth height weight sport gold silver bronze info 736041664 A Jesus Garcia ESP male 19691017 1. 72 64 athletics 0 0 0 532037425 A Lam Shin KOR female 19860923 1. 68 56 fencing 0 0 0 435962603 Aaron Brown CAN male 19920527 1. 98 79 athletics 0 0 1 521041435 Aaron Cook MDA male 19910102 1. 83 80 taekwondo 0 0 0 33922579 Aaron Gate NZL male 19901126 1. 81 71 cycling 0 0 0 173071782 Aaron Royle AUS male 19900126 1. 80 67 triathlon 0 0 0 266237702 Aaron Russell USA male 19930604 2. 05 98 volleyball 0 0 1 382571888 Aaron Younger AUS male 19910925 1. 93 100 aquatics 0 0 0 87689776 Aauri Lorena Bokesa ESP female 19881214 1. 80 62 athletics 0 0 0 Optional Clean Up The File The CSV file has a few fields we dont need. Lets clean it up by removing the info id weight and dateofbirth columns. mlr csv I cut x f idinfoweightdateofbirth athletes. csv Now we can move to our original problem we want to find who won the highest number of medals. We have how many of each medal bronze silver and gold the athletes won but not the total number of medals per athlete. Lets compute a new value called medals which corresponds to this total number bronze silver and gold added together. mlr c2p put medalsbronzesilvergold then head. athletes. csv It gives you the following output name nationality sex height sport gold silver bronze medals A Jesus Garcia ESP male 1. 72 athletics 0 0 0 0 A Lam Shin KOR female 1. 68 fencing 0 0 0 0 Aaron Brown CAN male 1. 98 athletics 0 0 1 1 Aaron Cook MDA male 1. 83 taekwondo 0 0 0 0 Aaron Gate NZL male 1. 81 cycling 0 0 0 0 Aaron Royle AUS male 1. 80 triathlon 0 0 0 0 Aaron Russell USA male 2. 05 volleyball 0 0 1 1 Aaron Younger AUS male 1. 93 aquatics 0 0 0 0 Aauri Lorena Bokesa ESP female 1. 80 athletics 0 0 0 0 Ababel Yeshaneh ETH female 1. 65 athletics 0 0 0 0 Sort by the highest number of medals by adding a sort. mlr c2p put medalsbronzesilvergold then sort nr medals then head. athletes. csv Respectively the resulting output will be the following name nationality sex height sport gold silver bronze medals Michael Phelps USA male 1. 94 aquatics 5 1 0 6 Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 Madeline Dirado USA female 1. 76 aquatics 2 1 1 4 Nathan Adrian USA male 1. 99 aquatics 2 0 2 4 Penny Oleksiak CAN female 1. 86 aquatics 1 1 2 4 Simone Manuel USA female 1. 78 aquatics 2 2 0 4 Alexandra Raisman USA female 1. 58 gymnastics 1 2 0 3 Restrict to the top 5 by adding n 5 to your head operation. mlr c2p put medalsbronzesilvergold then sort nr medals then head n 5. athletes. csv You will end up with the following file name nationality sex height sport gold silver bronze medals Michael Phelps USA male 1. 94 aquatics 5 1 0 6 Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 As a final step lets convert this into a JSON file with the c2j option. Here is our final command mlr c2j put medalsbronzesilvergold then sort nr medals then head n 5. athletes. csv top5. json With a single command weve computed new data sorted the result truncated it and converted it to JSON. name Michael Phelps nationality USA sex male height 1. 94 weight 90 sport aquatics gold 5 silver 1 bronze 0 medals 6 Other entries omitted for brevity. Bonus If you wanted to show the top 5 women you could add a filter. mlr c2p put medalsbronzesilvergold then sort nr medals then filter sex female then head n 5. athletes. csv Respectively you would end up with the following output name nationality sex height sport gold silver bronze medals Katie Ledecky USA female 1. 83 aquatics 4 1 0 5 Simone Biles USA female 1. 45 gymnastics 4 0 1 5 Emma McKeon AUS female 1. 80 aquatics 1 2 1 4 Katinka Hosszu HUN female 1. 75 aquatics 3 1 0 4 Madeline Dirado USA female 1. 76 aquatics 2 1 1 4 Conclusion I hope this article showed you how versatile Miller is and gave you a taste of the power of commandline tools. Feel free to scourge the internet for the best CLI next time you find yourself writing yet another random script. Resources Miller Dataset of IMDb ratings for American TV dramas Further Reading on Smashing Magazine Powerful Terminal And CommandLine CLI Tools For Modern Web Development How Should Designers Learn To Code The Terminal And Text Editors Part 1 How To Develop An Interactive Command Line Application Using Node. js A Deep Dive Into Serverless UI With TypeScript,2022-12-27T11:00:00Z,No Headline,Extremely Negative,210,"['nightmare', 'Rust']",[],[]
3,Mapping Python to LLVM,Modernizing high-performance computing.,https://blog.exaloop.io/python-llvm/,Firstly thank you to everyone who engaged with tried out or gave feedback on Codon over the past few weeks This is the first in a series of blog posts that describe how Codon works internally various design decisions performance considerations and so on. At a high level the Codon compiler works in these steps Parse source code into an abstract syntax tree AST. Perform type checking on the AST using a modified HindleyMilnerlike algorithm. Convert the typed AST to an intermediate representation called CIR. Perform numerous analyses transformations and optimizations on the CIR. Convert from CIR to LLVM IR. Run the LLVM optimization pipeline among other things. This post will mainly focus on the LLVM IR generation and how we map various Python constructs to LLVM IR. Sometimes this is very straightforward while other times it can be very tricky to get right as well see below. This post assumes a basic familiarity with LLVM and code generation. See Eli Benderskys excellent blog posts for an informal introduction or refresher or this great tutorial. Getting started Python types to LLVM types One of the first things we need to consider is how to convert from Python types to LLVM IR types. Some of the conversions are quite easy int can become an LLVM i64 note that this deviates from Pythons arbitrary width integers but it turns out 64 bits are more than enough for most applications float can become an LLVM double bool can become an LLVM i8 we could use i1 in theory but i8 is compatible with CC What about tuples We can convert tuple types to structs containing the tuples element types. For example the type of 42 3. 14 True becomes the LLVM structure type i64 double i8. Since tuples are immutable we can pass these structs by value and in many cases tuples can be completely optimized out by LLVMs optimization passes. What about userdefined classes We can actually do something very similar to what we did for tuples except instead of passing by value we dynamically allocate and pass by pointer which allows mutations to be handled correctly. For example consider class C a int b float c bool When we instantiate C under the hood well dynamically allocate memory to store the contents of the same i64 double i8 and return a pointer to that memory as the result of instantiation after calling C. init. There are a few other LLVM types that we expose through Codon like PtrT to represent a pointer to an object of type T. Most of the other Python types though can be constructed from these building blocks. For example the builtin collection types like list dict and set are all implemented within Codon as classes some other builtin types are implemented as named tuples and so on. Operators Now that we know how to do type conversions we need to think about how to actually do operations on those types. For example how do we add two ints as in 2 2 Python implements various operators like etc. through its magic method interface. For example a b typically becomes a. addb a b becomes a. subb and so on. It turns out we can do the same in Codon and in fact all operators are defined by magic methods within Codons standard library. For example list. add might look like class List... def addself other result copyself result. extendother return result Magic methods are convenient because they allow all operators to be implemented in standard library code rather than having some be hardcoded in the compiler. But you may ask what about operators of primitive types like int. add Thats where Codons inline LLVM IR functionality comes into play primitive operators are all implemented using inline LLVM IR via the llvm decorator class int... llvm def addself other int int tmp add i64 self other ret i64 tmp llvm def addself other float float tmp1 sitofp i64 self to double tmp2 fadd double tmp1 other ret double tmp2 Note that Codon supports method overloading the compiler will choose the correct add based on the righthand sides type. Now actually figuring out which magic methods need to be called for a given expression is a complex task in itself and is handled by the type checker which well discuss in detail in a future blog post. For now well assume we have all of this information at our disposal which is indeed the case by the time were generating LLVM IR. Control flow At this point we more or less know how to generate code for expressions so the natural next step is higherlevel control flow like if while for etc. Unlike most expressions control flow constructs require us to generate multiple basic blocksblocks of straightline code without any branches themselves linked together by branches or conditional branches. The basic block is a fundamental concept in LLVM and compilers in general and acts as the substrate for LLVM IR. The compiler itself maintains a pointer to the current basic block B. Different control flow constructs require us to generate a number of basic blocks and link them together in various ways. Lets look at an example if condition truebranch else falsebranch Compilation would roughly proceed as follows remember we always generate code in the current basic block B Create four new basic blocks Bmathrmcond Bmathrmtrue Bmathrmfalse and Bmathrmexit. Generate a branch to Bmathrmcond. Set B gets Bmathrmcond and generate code C for condition then generate a conditional branch to Bmathrmtrue and Bmathrmfalse based on C. Set B gets Bmathrmtrue and generate code for truebranch then add a branch to Bmathrmexit. Set B gets Bmathrmfalse and generate code for falsebranch then add a branch to Bmathrmexit. Set B gets Bmathrmexit. As a diagram graph TD B0B 0 Bcond BcondB cond C is true Btrue Bcond C is false Bfalse BtrueB true BexitB exit BfalseB false Bexit Lets look at one other example Heres how compilation would go Create three new basic blocks Bmathrmcond Bmathrmbody and Bmathrmexit Generate a branch to Bmathrmcond. Set B gets Bmathrmcond and generate code C for condition then generate a conditional branch to Bmathrmbody and Bmathrmexit based on C. Set B gets Bmathrmbody and generate code for body then add a branch to Bmathrmcond. Set B gets Bmathrmexit. Again as a diagram graph TD B0B 0 Bcond BcondB cond C is true Bbody Bcond C is false BexitB exit BbodyB body Bcond What about break and continue These turn out to be very straightforward when thinking in terms of basic blocks break simply becomes a branch to Bmathrmexit and continue becomes a branch to Bmathrmcond. Other control flow constructs like elif work in an analogous way and in fact can be constructed using just if else and while. There is one particular statement that requires special care however as well see next. try except finally While most of Pythons control flow constructs are relatively easy to compile try except finally is quite possibly one of the most difficult and nuanced things to get right when it comes to mapping Python to LLVM IR on a number of different levels. Exception handling itself is actually not too difficult as we can use the Itanium C ABI for zerocost exceptions along with LLVMs exception handling instructions landingpad resume invoke and appropriate personality function. We wont go into too much detail since this is all very similar to what C does. Just know that there is some additional bookkeeping required for knowing when were inside a try block which requires us to call functions with the LLVM invoke instruction rather than the usual call and to specify a basic block to branch to if an exception occurs i. e. the basic block corresponding to except. The real difficulty with try except finally comes from the finally. It turns out finally has certain semantics that many programmers might not even be aware of. For example take a look at this code def foo try return 1 finally return 2 What does foo return If youre not familiar with finally semantics you might be inclined to say 1 but the correct answer is 2 finally blocks are always executed which makes sense as they will typically be performing some cleanup action that shouldnt be skipped. This has some important implications for compilation namely branches always need to be aware of enclosing. Lets take another concrete example remember when I said try finally blocks break and continue are straightforward That turns out to not be entirely true in the presence of finally def bar for i in range10 printi try if i 5 break finally continue What does bar print Well when i 5 well reach the break statement but the otherwise the break needs to actually branch to the finally block finally block would we skipped over. Now the finally itself has a continue which overrides the previous break and resumes the loop. So in the end all the integers 0 9 are printed. To actually generate correct code for try except finally we need a couple different ingredients Firstly we need to maintain a stack of enclosing try except finally blocks since if we reach a return break or continue we need to know whether we really need to branch to some finally block. Next we need to set up a state machine for each series of nested try except finally blocks since once we do reach the finally we need to know how we got there in order to determine what action we need to take next. For instance if we got there via a return we need to actually execute that return statement at the end of the block or perhaps we got there by catching an exception that needs to be delegated to a parent try except finally. At the end of the day we need to construct a state machine with the following states NORMAL We reached the finally through normal execution of the code. Nothing special needs to be done we can just branch to the next block normally. THROWN Weve thrown an exception and are executing the finally before propagating the exception out of the function. The exception object itself will be stored in a predefined place that we can access from the finally block. CAUGHT Weve caught an exception and are reaching the finally through some except block. Again nothing special needs to be done here we can just branch to the next block normally. RETURN Weve reached the finally after encountering an enclosed return statement. After executing the finally we need to execute the return. The return value itself will be stored in a predefined place that we can access from the finally block. BREAK Weve reached the finally after encountering a break. We need to actually execute the break after executing the finally block. CONTINUE Weve reached the finally after encountering a continue. We need to actually execute the continue after executing the finally block. Importantly these actions are recursive so when we say execute the that may entail branching to return after executing the finally block another enclosing finally block and repeating the same action. Lets look at a real example of what this state machine looks like. Heres a function def baz for i in range5 try try if i 3 return i finally if i 2 break finally if i 4 continue return i And here are the various transitions between the states for different conditions throughout the loop stateDiagramv2 NORMAL NORMAL RETURN i 3 NORMAL BREAK i 3 NORMAL CONTINUE i 2 RETURN BREAK i 2 RETURN CONTINUE i 2 RETURN otherwise BREAK CONTINUE i 4 BREAK otherwise CONTINUE NORMAL The internal state machine will transition between the states based on these conditions until the loop terminates signified by reaching the end state. Variables and functions LLVM IR uses static single assignment form or SSA which effectively means LLVM IR variables must be assigned exactly once. As a result we cant map Python variables directly to LLVM IR variables. Instead we map each Python variable to a stackallocated piece of memory becomes x alloca i64 align 8 store i64 42 i64 x align 8 alloca is an LLVM IR instruction that allocates space on the current stack frame alloca i64 allocates space for a 64bit integer. Treating variables this way is standard practice when compiling to LLVM IR and CC compilers will do the same e. g. long x 42 produces this exact code with Clang. Notice also that alloca actually returns a pointer so when we dereference a variable by e. g. using x in a later expression the compiler inserts an implicit dereference of the underlying alloca pointer. In C we can avoid this dereference via the addressof operator as in x Codon actually supports this via the ptr builtin ptrx returns the address of x. This is often useful when interfacing with C functions that take a pointer as an argument. Many variables are implicitly introduced by the parser andor type checker. For example a common Python idiom for swapping the values of two variables will implicitly be transformed into tmp b a a tmp0 b tmp1 thus introducing the new variable tmp. Well discuss these types of frontend transformations in much more detail in a subsequent blog post. As for functions we can actually map Python functions directly to LLVM functions def fooa int b int return a b becomes define i64 fooi64 0 i64 1 a alloca i64 align 8 b alloca i64 align 8 store i64 0 i64 a align 8 store i64 1 i64 b align 8 a0 load i64 i64 a align 8 b0 load i64 i64 b align 8 ans add nsw i64 a0 b0 ret i64 ans Notice that we allocate space for the arguments via alloca since they should be treated like normal variables inside the function. The only complication to this otherwise fairly straightforward conversion is generators which well talk about next. Generators Generators are one of the hallmarks of the Python language and are used extensively in the standard library. For example many of the builtin functions operate on or return generators maplambda x x 1 1 2 3 generator giving 2 3 4 reversedabc generator giving c b a zipabc 1 2 3 generator giving a 1 b 2 c 3 Generators can be defined by using yield def squaresn for i in range1 n 1 yield i i listsquares5 gives all the square numbers up to 5 2 1 4 9 16 25. Since generators are so fundamental in Python we need a way to handle them efficiently in LLVM. Fortunately LLVM supports coroutines which we can use as the foundation for implementing generators. Note that C20 also adds support for coroutines. You can learn about all the intricacies of LLVM coroutines in the docs. Briefly though coroutines are like functions that allow suspension and resumption much like what happens with Pythons yield. Coroutines maintain their state i. e. local variables position in the function yielded value in whats called a coroutine frame. Coroutines in LLVM are indeed also like normal functions but delineate their resumesuspend points with special intrinsics and return a handle to their coroutine frames. Here are some of the more important LLVM intrinsics well use when generating code for coroutines llvm. coro. id Gives us a token that can identify the coroutine which well pass to many of the other intrinsics. llvm. coro. size. i64 Tells us the size of the coroutine frame for dynamic allocation. llvm. coro. begin Gives us a handle to the coroutine frame. llvm. coro. suspend Marks a suspension point. llvm. coro. end Marks the end of the coroutine and destroys the coroutine frame. These intrinsics are all used within the coroutine function itself but how do we actually call and manipulate the coroutine externally Well there are intrinsics for that as well llvm. coro. resume Resumes a coroutine given a coroutine handle. llvm. coro. done Checks if a coroutine is at its final suspend point. llvm. coro. promise Returns a pointer to the coroutine promise a region of memory that stores values yielded from the coroutine. llvm. coro. destroy Destroys a finished coroutine. With these primitives in hand we can implement all we need to support generators Functions with yield will be converted to LLVM coroutines. We can generate code for yield statements by storing the yielded value in the coroutine promise then calling llvm. coro. suspend. We can implement Pythons next builtin by calling llvm. coro. done to see if the given generator is finished then calling llvm. coro. resume to resume it and finally llvm. coro. promise to obtain the generated value. We can implement for x in generator by repeatedly calling llvm. coro. resume llvm. coro. promise until llvm. coro. done tells us to stop. Lets look at an example for i in range3 printi Heres the simplified LLVM IR well generate for this snippet entry g call i8 rangei64 3 br label for for call void llvm. coro. resumei8 g done call i1 llvm. coro. donei8 g br i1 done label exit label body body p1 call i8 llvm. coro. promisei8 g i32 8 i1 false p2 bitcast i8 p1 to i64 i load i64 i64 p2 call void printi32 i br label for exit call void llvm. coro. destroyi8 g Just to recap The call to range gives us a handle with type i8 to the range generator. We resume the generator with llvm. coro. resume note that all coroutines will be initially suspended to match Pythons generator semantics. If llvm. coro. done tells us the generator is done we exit the loop and call llvm. coro. destroy to destroy it. Otherwise in the body of the loop we obtain the next value for i by calling llvm. coro. promise the other arguments are simply the alignment of the promise i32 8 and whether we want to obtain a promise given a handle or vice versa i1 false. At this point you might be asking yourself isnt that a lot of code to just loop over 0 1 2 and print each value Thats where LLVMs coroutine optimization passes come in it turns out coroutines are particularly amenable to compiler optimizations that can typically eliminate all of the coroutine overhead entirely. Specifically by far the most common case when dealing with generators in Python is for them to be created and destroyed in the same function in which case the coroutine passes can inline them elide heap allocation for the frame and so on. Heres the result of running the coroutine passes on our example above call void printi64 0 call void printi64 1 call void printi64 2 We were able to get rid of everything coroutinerelated By default LLVMs coroutine passes rely on an analysis of llvm. coro. destroy calls. Codons LLVM fork instead relies on an escape analysis of the coroutine handle which we found to be much better at determining which coroutines can be optimized. Program structure We have most of the key ingredients at this point but what about the overarching program structure Python doesnt have an explicit main function as an entry point like C does. LLVM on the other hand does so we need to reconcile that difference. The way we handle this in Codon is by putting everything at the top level into its own implicit function becomes a 0 def main global a a 42 printa 2 This also allows us to do some setup and initialization as well such as initializing the runtimeGC setting up sys. argv etc. This approach usually works fairly seamlessly but there is one subtle case that needs special care exporting Codon functions in a shared library. For example consider the following def bar int... some complex operation M bar def foon int int return n M and assume we want to generate a shared library libfoo. so from Codon that exposes the foo function. If we do so naively the global variable M will actually not be initialized we need to run the implicit main we created in order to initialize M. We solve this by running main as a global constructor if were compiling to a shared object. Optimizations Now is a good time to talk about optimizations. LLVM has an extensive repertoire of IR passes for doing various things. These include many of the standard compiler optimizations like constant folding dead code elimination loop invariant code motion etc. These passes are grouped into standard pipelines like the O0 pipeline or the O3 pipeline which correspond to the O0 and O3 flags on a CC compiler like Clang. Codon uses these default optimization pipelines but also adds a few of its own LLVM passes. One example is AllocationRemover which 1 removes unused heap GC allocations and 2 demotes small heap allocations to the stack i. e. to an alloca instruction. This turns out to be useful when instantiating classes for example creating an instance of a class C that never escapes its enclosing function can actually be done purely on the stack def foo c Chello codegend as dynamic allocation demoted to alloca... C never escapes foo The same goes for the builtin collection types like lists or dictionaries a 1 2 3 printa0 a1 a2 optimized to 6 no allocations take place Codon uses a couple other custom LLVM passes but the vast majority of its custom optimizations are performed in its own IR which well discuss in a future post. Examples To conclude this post I want to showcase a few neat examples that illustrate many of these pieces coming together combined with LLVMs optimization passes. Ive simplified the naming in the LLVM IR outputs for clarity. Userdefined generator with loop def squaresn for i in range1 n 1 yield i2 x 0 for s in squares1000 x s printx This gets optimized away entirely and the final answer 333833500 is printed directly answer call i64 i8 inttostri64 333833500 Tail recursion elimination from sys import argv N intargv1 def collatzn if n 1 return 0 else return 1 collatz3n 1 if n 2 else n 2 printcollatzN The recursion is eliminated and replaced with a loop collatzentry tmp1 icmp eq i64 N 1 br i1 tmp1 label collatzexit label if. false if. false tmp2 phi i64 tmp7 ternary. exit N collatzentry accumulator phi i64 tmp8 ternary. exit 0 collatzentry tmp3 and i64 tmp2 1 not1 icmp eq i64 tmp3 0 br i1 not1 label ternary. false label ternary. true ternary. true tmp4 mul i64 tmp2 3 tmp5 add i64 tmp4 1 br label ternary. exit ternary. false tmp6 sdiv i64 tmp2 2 br label ternary. exit ternary. exit tmp7 phi i64 tmp5 ternary. true tmp6 ternary. false tmp8 add i64 accumulator 1 not2 icmp eq i64 tmp7 1 br i1 not2 label collatzexit label if. false collatzexit answer phi i64 0 collatzentry tmp8 ternary. exit List of tuples from sys import argv from random import random from math import hypot Random points in quadrant points random random for in range10000000 How many points are insideoutside of quartercircle inside outside 0 0 for x y in points if hypotx y 1 inside 1 else outside 1 Estimate pi using the ratio pi 4 inside inside outside printpi Tuples are eliminated and the inside and outside variables are converted to LLVM IR SSA variables for. body idx phi i64 idxadd for. body 0 for. cleanup inside phi i64 insideadd for. body 0 for. cleanup outside phi i64 outsideadd for. body 0 for. cleanup p1 getelementptr double double ptr points i64 idx x load double ptr p1 align 8 p2 getelementptr double double ptr points i64 idx i32 1 y load double ptr p2 align 8 h tail call double hypotdouble x double y tmp1 fcmp uge double h 1. 000000e00 tmp2 zext i1 tmp1 to i64 outsideadd add i64 outside tmp2 tmp3 xor i1 tmp1 true tmp4 zext i1 tmp3 to i64 insideadd add i64 inside tmp4 idxadd add nuw nsw i64 idx 1 exitcond icmp eq i64 idx N br i1 exitcond label for. exit label for. body Conclusion I hope this post helped shed some light on the various methods we can employ to compile Python code to LLVM. You can view the LLVM IR output of your own code with codon build llvm remember to add the release flag to enable optimizations. There are many things we took for granted here like how we determine the data types to begin with or how we put the source code in a format thats suitable for code generation. These among other things will be topics of future posts in this series. Stay tuned,2023-01-09T16:52:56Z,A Guide To Command-Line Data Manipulation,Somewhat Positive,580,"['convoluted', 'confusing', 'Rust', 'freezing', 'issues', 'hard', 'useless', 'Breaking', 'Bad', 'Afraid', 'Dark', 'problem', 'Restrict', 'scourge']","['Homicide', 'sex', 'sex', 'sex', 'sex', 'sex', 'sex', 'sex', 'Dive']","[John, Kerl, Miller, Miller, Miller, Miller, Jesus, Garcia, A, Lam, Shin, Aaron, Brown, Aaron, Cook, Aaron, Gate, Aaron, Royle, Aaron, Russell, Aaron, Younger, Aauri, Lorena, Bokesa, A, Jesus, Garcia, A, Lam, Shin, Aaron, Brown, Aaron, Cook, Aaron, Gate, Aaron, Royle, Aaron, Russell, Aaron, Younger, Aauri, Lorena, Bokesa, Ababel, Yeshaneh, Michael, Phelps, Katie, Ledecky, Simone, Biles, Emma, McKeon, Katinka, Hosszu, Madeline, Dirado, Nathan, Adrian, Penny, Oleksiak, Simone, Manuel, Alexandra, Raisman, Michael, Phelps, Katie, Ledecky, Simone, Biles, Emma, McKeon, Katinka, Hosszu, Michael, Phelps, Katie, Ledecky, Simone, Biles, Emma, McKeon, Katinka, Hosszu, Madeline, Dirado, Miller, Miller]"
4,Pure Python Distributed SQL Engine,Open source SQL engine in Python. Contribute to marsupialtail/quokka development by creating an account on GitHub.,https://github.com/marsupialtail/quokka/blob/master/blog/why.md,Why I wrote a SQL engine in only Python Ive worked on Quokka for a year now. Before I started I was on leave from Stanford working on a startup writing assembly speeding up machine learning primitives like matrix multiplications and decision trees. After a while I realized though I could perhaps make a living doing the above what most customers want sped up is not ML model inferencetraining but data pipelines. After all most ML in industry today seems to be lightweight models applied to heavily engineered features not GPT3 on Common Crawl. Virtually all feature engineering today is done with SQLDataFrames with a prized library of userdefinedfunctions UDFs that encode business logic. Think fraud detection search recommendation personalization pipelines. In model training materializing those features is often the bottleneck especially if the actual model used is not a neural network. People now either use a managed feature platform like Tecton Feathr. ai or roll their own pipelines with SparkSQL. With robust UDF support SparkSQL seems to be the defacto standard for these feature engineering workloads and is used under the hood by virually every managed feature platform. Unless you are on GCP in which case BigQuery is also a strong contender. Of course these problems only happen when you have big data 100GB cant use Pandas and need to use a distributed framework like SparkSQL. Having lost all the money I made from my startup on shitcoins and the stock market I returned to my PhD program to build a better distributed query engine Quokka to speed up those feature engineering workloads. When I set out I had several objectives Easy to install and run especially for distributed deployments. Good support for Python UDFswhich might involve numpy sklearn or even Pytorch. At least 2x SparkSQL performance. Otherwise what are we even doing here. Fault tolerant. This is perhaps not important for small scale frameworks but its critical for TBscale jobs that run over many hours that are a painintheass to restart. SparkSQL can recover from worker failures due to spot instance preemptions Quokka should be as well. The first two objectives strongly scream Python as the language of choice for Quokka. PySpark supports Python UDFs reasonably well but there are a myriad of inconveniences that arise from its Java background you have to sudo install all required Python packages on EMR Python UDF error messages will not get displayed properly no finegrained control of intraUDF parallelism etc. While each issue seems minor on its own these footguns are extremely annoying for Pythonnative data scientists like me whose last experience with Java was AP Computer Science. I had pushback on this I know major tech players who maintain UDF libraries in ScalaJava and senior engineers who claim Java is not so bad and all serious engineers should know it anyways. My argument I got a CS degree at MIT without writing a single line of Java. I know many who did the same. I want to empower data scientists without a formal CS education and its unlikely their first language of choice is Java based on the number of tutorial videos available on YouTube. Ever wondered why Tensorflow4j exists Do you even want to learn how to use it instead of just writing PyTorch But how do you build a distributed engine on top of Python After all Python is not known for its distributed prowess... Until Ray came about. Not going to waste space here describing how amazing it is but its basically Akka in Python that actually works. It allows you to easily instantiate a custom Python class object on a remote worker machine and call its functions which is pretty much all you need to build a distributed query engine. Ray also lets you easily spin up remote clusters with a few lines of code and manage arbitrary Python dependencies programmatically which easily satisfied my first two objectives. Well now what about performance Python is so reputed for being slow there are memes for it. However Pythons slowness actually works in its favor since its so slow people have built amazing opensource libraries for it in C or Rust that speeds up common operations like numpy Pandas or Polars If you use those libraries as much as possible then your code can actually be extremely performant e. g. if you implement a data analytics workflow using only columnar Pandas APIs it will beat a handcoded Java or even C program almost any day. Specifically for a distributed query engine you want A library to parse and optimize SQL the one and only SQLGlot. Its a SQL parser optimizer planner and transpiler and can even execute simple SQL queries in pure Python. Very fast kernels for SQL primitives like joins filtering and aggregations. Quokka uses Polars to implement these. I sponsor Polars on Github and you should too. I am also exploring DuckDB but I have found Polars to be faster so far. Fast reading writing decoding encoding CSVs and Parquet files. Quokka uses Apache Arrow which is probably the fastest opensource option and will become even faster when my PR gets merged A tool to send data quickly from one worker to another without having to serialize. Rays object store provides a builtin solution but Quokka opts for the higherperformance Arrow Flight. In database parlance Quokka adopts a pipelined approach where multiple processing stages can happen at once similar to Presto or Snowflake. For example if your workload involves reading from two data sources in S3 and joining them Quokka can execute the download decode and join all at once. This offers higher performance than SparkSQLs model where stages execute one at a time Spark would download the data sources first and only after they are fully buffered in memory or on disk would it execute the join stage. Of course the pipelined execution model leads to complications with fault handling. In Spark fault tolerance is pretty simple if a stage fails it just gets reexecuted. Only one stage could fail at a time. In Quokka multiple stages can fail at once and its hard to see how you can recover from failures without just restarting the whole damned thing. Efficient fault handling in pipelined query execution is actually the main academic innovation of Quokka and how I hope to earn my PhD so its perhaps best to save it for another post. But a sneak peak similar to how Kafka writes control information to Zookeper or how Kubernetes manages control plane with etcd Quokka maintains a consistent global state in Redis. This allows Quokka to reason about failures and recover from them using dynamically tracked lineage. OK enough talk Whats the state of Quokka right now and can I use it Yes of course. Quokka currently supports a DataFramelike API documented here. It should work on local machine no problem and should be a lot faster than Pandas. Distributed setup is a bit more involved and only supports EC2 right now. SQL support is in the works and currently passes half of the TPCH benchmark. For a look at how to implement these queries in the DataFrame API check here. Quokkas performance is similar to Trino who is not fault tolerant at the moment on these queries for Parquet and a lot faster than everybody else if the input is in CSV format. Quokka is under active development. In the new year I hope to add a lot more functionality related to feature engineering i. e. range joins PIT joins window functions etc. as well as improve upon the SQL execution engines performance potentially using SIMD kernels. I will also add some more data sources like the Twitter API Ethereum API JSON files and probably JDBC. Finally I plan to add support for connecting to an existing RayKubernetes cluster and GCPAzure. Quokkas core execution engine is only 1000 lines of code. It is meant to be simple and easily extensible. I welcome any contributions on new data source readers or executors If you think Quokkas cool please join the Discord raise a Github issue or shoot me an email zihengwstanford. edu.,2022-12-30T20:08:29Z,Mapping Python to LLVM,Somewhat Positive,630,"['tricky', 'arbitrary', 'object', 'primitive', 'primitive', 'false', 'false', 'false', 'break', 'break', 'difficult', 'difficult', 'difficulty', 'break', 'break', 'break', 'break', 'break', 'break', 'object', 'BREAK', 'break', 'break', 'break', 'BREAK', 'BREAK', 'BREAK', 'BREAK', 'static', 'complication', 'false', 'vice', 'false', 'naively', 'object', 'dead', 'elimination', 'elimination', 'false', 'false', 'false', 'false', 'false', 'false']","['destroy', 'destroy', 'destroy', 'destroy']","[Eli, Benderskys, llvm, ., coro, ., done]"
5,Underappreciated challenges with Python packaging,"pypackaging-native is a collection of content about key Python packaging topics and issues for projects using native code - with a focus in particular scientific, data science and ML/AI projects in the PyData ecosystem.",https://pypackaging-native.github.io/,Home Introduction Packaging is an important and timeconsuming part of authoring and maintaining Python packages. This is particularly true for projects that are not pure Python but contain code that needs to be compiled and have to deal with distributing compiled extensions and with build dependencies. Many projects in the PyData ecosystem which includes scientific computing data science and MLAI projects fall into that category. This site aims to provide an overview of the most important Python packaging issues for such projects with indepth explanations and references. The content on this site is meant to provide insights and good reference material. This will hopefully provide common ground when discussing potential solutions for those problems or design changes in Python packaging as a whole or in individual packaging tools. The content is divided into meta topics and key issues. Meta topics are mainly descriptions of aspects of Python packaging that are more or less inherent to the whole design of it and consequences and limitations that follow from that. Key issues are more specific pain points felt by projects with native code. Key issues may also be more tractable to devise solutions or workarounds for. How are these topics chosen and ranked The initial list of topics was constructed by soliciting input from 25 people who together are a representative subset of stakeholders maintainers of widely used PyData projects like NumPy scikitlearn Apache Arrow CuPy Matplotlib SciPy H5py Jupyter Hub and Spyder maintainers of package repositories package managers and build systems Pip PyPI Conda Condaforge Spack Nix pypabuild Meson and numpy. distutils engineers from hardware vendors like Intel and NVIDIA engineers responsible for deploying software for HPC users educators and organisers of user groups WiMLDS SciPy Lectures Data Umbrella Adding new topics and making changes to existing content on this site happens through community input on GitHub. Meta topics Build package management concepts and terminology The multiple purposes of PyPI PyPIs authorled social model and its limitations Lack of a build farm for PyPI Expectations that projects provide ever more wheels Key issues Native dependencies This is by some distance the most important issue. Several types of native dependencies are discussed in detail Depending on packages for which an ABI matters Packaging projects with GPU code Metadata handling on PyPI Distributing a package containing SIMD code Unsuspecting users getting failing from source builds Contributing All contributions are very welcome and appreciated Ways to contribute include Improving existing content on the website extending or clarifying descriptions adding relevant references diagrams etc. Providing feedback on existing content Proposing new topics for inclusion on the website and writing the content for them... and anything else you consider useful The content for this website is maintained on GitHub. Acknowledgements Initial development of this website was sponsored by Intel Initial development effort was led by Quansight Labs Created December 20 2022,2023-01-03T19:45:06Z,Name already in use,Extremely Positive,370,"['problems', 'lost', 'Fault', 'critical', 'failures', 'scream', 'error', 'issue', 'annoying', 'bad', 'unlikely', 'waste', 'object', 'arbitrary', 'slow', 'slow', 'Rust', 'object', 'fault', 'fault', 'fails', 'fail', 'fail', 'hard', 'failures', 'damned', 'fault', 'sneak', 'failures', 'problem', 'fault', 'Discord', 'issue']",['fraud'],"[Ray, Ray, Kafka, Kubernetes]"
6,Why I'm still using Python,16 years and counting...,https://mostlypython.substack.com/p/why-im-still-using-python,Why Im still using Python 16 years and counting... Ive been using Python since 2006 and every year I ask myself if its still the right language for me. I dont want to get stuck using a language just because its the one Ive become comfortable using. Languages are constantly evolving and if theres a language that better suits my needs Ill happily invest the time needed to make a change. In the end though Python is still right for me for the same reasons it was right in 2006 it lets me get the work done that I want to do enjoyably and efficiently. Theres also the added bonus of getting to be part of one of the best communities Ive ever been involved in. I grew up in the 70s and 80s and my father was a software engineer at the time. We had a kit computer in our basement before most people had even thought of having a computer at home. I learned the fundamentals of programming when I was nine or ten years old the first programs I wrote were in BASIC. Over the next twenty years I dabbled in a variety of languages LOGO Pascal C Fortran Perl Javascript Java and PHP. I was a hobbyist programmer and I enjoyed learning a new language every few years. In 2006 I was working on a larger for me project in Java and a friend told me I should check out Python. Your programs will do the same things theyll just be one third as long as they were in Java. That was a bold claim but as I looked at a thousandline file it seemed like a pretty good idea to find out if he was right. Rewriting that first project in Python was magical. As I reimplemented sections of the project I watched my files grow shorter and they looked cleaner as well. Id always enjoyed programming but writing Python felt different. Ideas that were newer at the time such as semantic whitespace and not needing to declare variable types went from strange new patterns to ideas that made perfect sense in retrospect. My files looked consistent and wellstructured and they were much easier to read review and debug. Also they were just plain fun to write. When the project was finished the files were in fact less than half the length of the corresponding Java files. My programming efforts shifted from hobbyist to professional over the next ten years and as my projects grew more significant Python continued to serve me well. The code got out of my way much more than it had in the other languages Id been using. I was still doing programming work but I found myself spending more of my time thinking about the realworld problems I cared to solve and less time thinking about syntax and languagespecific constructs. I went to my first Python conference in 2012. I was intimidated about going because I was a teacher first and a programmer second and I assumed everyone there would be a professional programmer. When I showed up I found an entirely welcoming community. Half the people there were clearly better programmers than Id ever be because its what they focused on. But half the people there were just like me they had realworld problems they wanted to solve and they were finding that Python could help them work more effectively and more efficiently. My life got better the moment I stepped into the Python community and its been one of the best parts of my life ever since. Im still interested in other languages my innate curiosity about programming will always be there. But work life and parenting life doesnt leave me as much time for exploratory learning as I used to have. I want to learn Go Rust a functional language like Haskell and others as well but I dont have a compelling reason to spend significant time on those languages at this point. Im sure I will at some point but for now I have every reason to stick with Python for most of my work. There are aspects of aging that I dont enjoy but I deeply appreciate the decadeslong perspective I have on programming languages and the role of technology in society overall. Its been fascinating to see the development from lowerlevel languages to higherlevel languages over the course of half a lifetime. Most criticisms I see leveled at Python are still completely unfounded. Many times the criticism can be addressed by using the language in a different way. Python isnt a perfect fit for all problem domains. There are some areas where most experienced Python programmers would recognize its not the best fit. So be it if Im not working in one of those areas then Python is still probably the best fit for me. I used to hear that Python wasnt the best at any one thing but it was second best at most things. I agreed with that line of reasoning for a long time but these days Python is as good as any of its peers for many things and its still quite effective in many areas where it might not objectively be the best fit. Heading into 2023 I couldnt be more excited to continue using Python. I hope you are as well.,2022-12-30T16:06:02Z,No Headline,Extremely Negative,330,"['fall', 'issues', 'problems', 'issues', 'limitations', 'issues', 'pain', 'issues', 'limitations', 'Lack', 'issues', 'issue', 'Unsuspecting', 'failing']",[],[]
7,"Hackaday Podcast 200: Happy New Year, the Ultimate Game Boy, and Python All the Things","This week, Editor-in-Chief Elliot Williams and Managing Editor Tom Nardi ring in the New Year withâ€¦well, pretty much the same stuff they do every other week. After taking some time to talk abâ€¦",https://hackaday.com/2023/01/06/hackaday-podcast-200-happy-new-year-the-ultimate-game-boy-and-python-all-the-things/,This week EditorinChief Elliot Williams and Managing Editor Tom Nardi ring in the New Year withwell pretty much the same stuff they do every other week. After taking some time to talk about the nuts and bolts of the podcast in honor of Episode 200 discussion moves on to favorite stories of the week including an impeccably cloned Dyson lamp one hackers yearslong quest to build the ultimate Game Boy developing hardware in Python building a breadboard computer with the 6502s simplified sibling and the latest developments surrounding the NABU settop box turned retrocomputer. The episode wraps up with a review of some of the biggest themes we saw in 2022 and how theyre likely to shape the tech world in the coming years. Check out the links below if you want to follow along and as always tell us what you think about this episode in the comments Download it in living MP3. Episode 200 Show Notes Whats that Sound Think you know this weeks sound Fill out the form for a chance to win a Hackaday Podcast tshirt. Interesting Hacks of the Week DIYson Lamp Hides Cables Between The Seams An Epic Quest To Build The Ultimate Game Boy The Whole Thing In Python Squeezing A Minimalist 6502 Retrocomputer Onto A Single Breadboard A Homebrew SMD Vise Built From Scrap Wood NABU PC Gets CPU Upgrade Emulates A TRS80 Quick Hacks Elliots Picks Toms Picks,2023-01-06T17:00:33Z,No Headline,Extremely Negative,280,"['stuck', 'strange', 'problems', 'problems', 'Rust', 'criticisms', 'unfounded', 'criticism', 'problem']",[],[]
8,Writing a Python SQL engine from scratch,Python SQL Parser and Transpiler. Contribute to tobymao/sqlglot development by creating an account on GitHub.,https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md,Writing a Python SQL engine from scratch Introduction When I first started writing SQLGlot in early 2021 my goal was just to translate SQL queries from SparkSQL to Presto and vice versa. However over the last year and a half Ive ended up with a fullfledged SQL engine. SQLGlot can now parse and transpile between 18 SQL dialects and can execute all 24 TPCH SQL queries. The parser and engine are all written from scratch using Python. This post will cover why I went through the effort of creating a Python SQL engine and how a simple query goes from a string to actually transforming data. The following steps are briefly summarized Why I started working on SQLGlot because of my work on the experimentation and metrics platform at Netflix where I built tools that allowed data scientists to define and compute SQLbased metrics. Netflix relied on multiple engines to query data Spark Presto and Druid so my team built the metrics platform around PyPika a Python SQL query builder. This way definitions could be reused across multiple engines. However it became quickly apparent that writing python code to programmatically generate SQL was challenging for data scientists especially those with academic backgrounds since they were mostly familiar with R and SQL. At the time the only Python SQL parser was sqlparse which is not actually a parser but a tokenizer so having users write raw SQL into the platform wasnt really an option. Some time later I randomly stumbled across Crafting Interpreters and realized that I could use it as a guide towards creating my own SQL parsertranspiler. Why did I do this Isnt a Python SQL engine going to be extremely slow The main reason why I ended up building a SQL engine was... just for entertainment. Its been fun learning about all the things required to actually run a SQL query and seeing it actually work is extremely rewarding. Before SQLGlot I had zero experience with lexers parsers or compilers. In terms of practical use cases I planned to use the Python SQL engine for unit testing SQL pipelines. Big data pipelines are tough to test because many of the engines are not open source and cannot be run locally. With SQLGlot you can take a SQL query targeting a warehouse such as Snowflake and seamlessly run it in CI on mock Python data. Its easy to mock data and create arbitrary UDFs because everything is just Python. Although the implementation is slow and unsuitable for large amounts of data 1 million rows theres very little overheadstartup and you can run queries on test data in a couple of milliseconds. Finally the components that have been built to support execution can be used as a foundation for a faster engine. Im inspired by what Apache Calcite has done for the JVM world. Even though Python is commonly used for data there hasnt been a Calcite for Python. So you could say that SQLGlot aims to be that framework. For example it wouldnt take much work to replace the Python execution engine with numpypandasarrow to become a respectablyperforming query engine. The implementation would be able to leverage the parser optimizer and logical planner only needing to implement physical execution. There is a lot of work in the Python ecosystem around high performance vectorized computation which I think could benefit from a pure Pythonbased ASTplan. Parsing and planning doesnt have to be fast when the bottleneck of running queries is processing terabytes of data. So having a Pythonbased ecosystem around SQL is beneficial given the ease of development in Python despite not having bare metal performance. Parts of SQLGlots toolkit are being used today by the following Ibis A Python library that provides a lightweight universal interface for data wrangling. Uses the Python SQL expression builder and leverages the optimizerplanner to convert SQL into dataframe operations. mysqlmimic PurePython implementation of the MySQL server wire protocol Parses transforms SQL and executes INFORMATIONSCHEMA queries. Quokka Pushbased vectorized query engine Parse and optimizes SQL. Splink Fast accurate and scalable probabilistic data linkage using your choice of SQL backend. Transpiles queries. How There are many steps involved with actually running a simple query like SELECT bar. a b 1 AS b FROM bar JOIN baz ON bar. a baz. a WHERE bar. a 1 In this post Ill walk through all the steps SQLGlot takes to run this query over Python objects. Tokenizing The first step is to convert the sql string into a list of tokens. SQLGlots tokenizer is quite simple and can be found here. In a while loop it checks each character and either appends the character to the current token or makes a new token. Running the SQLGlot tokenizer shows the output. Each keyword has been converted to a SQLGlot Token object. Each token has some metadata associated with it like linecolumn information for error messages. Comments are also a part of the token so that comments can be preserved. Parsing Once a SQL statement is tokenized we dont need to worry about white space and other formatting so its easier to work with. We can now convert the list of tokens into an AST. The SQLGlot parser is a handwritten recursive descent parser. Similar to the tokenizer it consumes the tokens sequentially but it instead uses a recursive algorithm. The tokens are converted into a single AST node that presents the SQL query. The SQLGlot parser was designed to support various dialects so it contains many options for overriding parsing functionality. The AST is a generic representation of a given SQL query. Each dialect can override or implement its own generator which can convert an AST object into syntaticallycorrect SQL. Optimizing Once we have our AST we can transform it into an equivalent query that produces the same results more efficiently. When optimizing queries most engines first convert the AST into a logical plan and then optimize the plan. However I chose to optimize the AST directly for the following reasons Its easier to debug and validate the optimizations when the input and output are both SQL. Rules can be applied a la carte to transform SQL into a more desirable form. I wanted a way to generate canonical sql. Having a canonical representation of SQL is useful for understanding if two queries are semantically equivalent e. g. SELECT 1 1and SELECT 2. Ive yet to find another engine that takes this approach but Im quite happy with this decision. The optimizer currently does not perform any physical optimizations such as join reordering. Those are left to the execution layer as additional statistics and information could become relevant. The optimizer currently has 17 rules. Each of these rules is applied transforming the AST in place. The combination of these rules creates canonical sql that can then be more easily converted into a logical plan and executed. Some example rules are qualifytables and qualifycolumns Adds all dbcatalog qualifiers to tables and forces an alias. Ensure each column is unambiguous and expand stars. SELECT FROM x SELECT db. x AS x simplify Boolean and math simplification. Check out all the test cases. NOT FALSE AND x x AND TRUE OR 1 3 x x 1 1 2 normalize Attempts to convert all predicates into conjunctive normal form. DNF A AND B OR B AND C AND D CNF A OR C AND A OR D AND B unnestsubqueries Converts subqueries in predicates into joins. The subquery can be converted into a left join SELECT FROM x AS x WHERE SELECT y. a AS a FROM y AS y WHERE x. a y. a 1 SELECT FROM x AS x LEFT JOIN SELECT y. a AS a FROM y AS y WHERE TRUE GROUP BY y. a AS u0 ON x. a u0. a WHERE u0. a 1 AND NOT u0. a IS NULL pushdownpredicates Push down filters into the innermost query. SELECT FROM SELECT FROM x AS x AS y WHERE y. a 1 SELECT FROM SELECT FROM x AS x WHERE y. a 1 AS y WHERE TRUE annotatetypes Infer all types throughout the AST given schema information and function type definitions. Planning After the SQL AST has been optimized its much easier to convert into a logical plan. The AST is traversed and converted into a DAG consisting of one of five steps. The different steps are Scan Selects columns from a table applies projections and finally filters the table. Sort Sorts a table for order by expressions. Set Applies the operators unionunion allexceptintersect. Aggregate Applies an aggregationgroup by. Join Joins multiple tables together. The logical plan is quite simple and contains the information required to convert it into a physical plan execution. Executing Finally we can actually execute the SQL query. The Python engine is not fast but its very small 400 LOC It iterates the DAG with a queue and runs each step passing each intermediary table to the next step. In order to keep things simple it evaluates expressions with eval. Because SQLGlot was built primarily to be a transpiler it was simple to create a Python SQL dialect. So a SQL expression x 1 can just be converted into scopex 1. Whats next SQLGlots main focus will always be on parsingtranspiling but I plan to continue development on the execution engine. Id like to pass TPCDS. If someone doesnt beat me to it I may even take a stab at writing a PandasArrow execution engine. Im hoping that over time SQLGlot will spark the Python SQL ecosystem just like Calcite has for Java. Special thanks SQLGlot would not be what it is without its core contributors. In particular the execution engine would not exist without Barak Alon and George Sittas. Get in touch If youd like to chat more about SQLGlot please join my Slack Channel,2023-01-03T16:35:29Z,No Headline,Extremely Negative,220,"['Hacks', 'Scrap', 'Hacks']",[],"[Elliot, Williams, Tom, Nardi]"
9,Delphi VCL and FMX Libraries for Python,"Delphi VCL & FMX Libraries for Python .emb-events event-411 h3 not(.event-date), .hide-on-featured What Are The VCL and FMX Libraries? VCL and FireMonkey(FMX) are mature GUI libraries used by thousands of Delphi and C++Builder developers around",https://www.embarcadero.com/delphi-libraries-for-python,What Are The VCL and FMX Libraries VCL and FireMonkeyFMX are mature GUI libraries used by thousands of Delphi and CBuilder developers around the world. What Are The DelphiVCL and DelphiFMX Libraries for Python The DelphiVCL and DelphiFMX libraries for Python are a set of Python modules that put the robust and mature VCL and FireMonkey FMX GUI libraries in the hands of Python developers. The libraries are built with the same opensource Python4Delphi library that powers the popular PyScripter Python IDE. DelphiVCL for Python is focused on native Windows development and employs the Windowsonly VCL framework while DelphiFMX for Python employs the crossplatform FireMonkey framework and brings a powerful flexible GUI framework to Windows Linux macOS and Android. Do I need to know Delphi to use the DelphiVCL and DelphiFMX libraries for Python No. Although knowledge of Delphi is a big advantage when working with these libraries using the VCL and FMX libraries for Python development does not require previous knowledge of Delphi. Where To Get The Libraries DelphiVCL Python Module The Visual Component Library VCL is a key part of Delphis stateoftheart user interface support. It contains most native Windows controls and controls with additional features and functionality. DelphiFMX Python Module FireMonkey FMX is Delphis crossplatform GUI library. It takes advantage of GPU hardware acceleration using OpenGL or DirectX to create slick modern and highperformance user interfaces. The DelphiFMX Python module supports Windows macOS Linux and Android development with Python. Both modules are freely available via GitHub or the PIP Python Package Manager,2023-01-08T06:39:52Z,Name already in use,Extremely Positive,170,"['scratch', 'vice', 'scratch', 'challenging', 'randomly', 'stumbled', 'slow', 'mock', 'mock', 'arbitrary', 'slow', 'object', 'error', 'worry', 'object', 'FALSE', 'stab', 'Slack']",[],"[Barak, Alon, George, Sittas]"
10,"Control Pandas, Polars, or SQL from One DSL",Codd method-chained SQL generator and Pandas data processing in Python. - GitHub - WinVector/data_algebra: Codd method-chained SQL generator and Pandas data processing in Python.,https://github.com/WinVector/data_algebra,dataalgebra dataalgebra is a piped data wrangling system based on Codds relational algebra and experience working with data manipulation languages at scale. The primary purpose of the package is to support an easy to compose and maintain grammar of data processing steps that in turn can be used to generate database specific SQL. The package also implements the same transforms for Pandas DataFrames. The package is available on PyPi and can be installed with pip install dataalgebra. A good introduction can be found here and many worked examples are here. A catalog of expression methods is found here. The pydoc documentation is here. And the README is a good place to check for news or updates. Currently the system is primarily adapted and testing for Pandas Polars Google BigQuery PostgreSQL SQLite and Spark. Porting and extension is designed to be easy. This is to be the Python equivalent of the R packages rquery rqdatatable and cdata. This package supplies piped Coddtransform style notation that can perform data engineering in Pandas or still in development Polars and generate SQL queries from the same specification. Installing Install dataalgebra with pip install dataalgebra Announcement This article introduces the dataalgebra project a data processing tool family available in R and Python. These tools are designed to transform data either inmemory or on remote databases. For an example with video of using dataalgebra to rearrange data layout please see here. The key question is what operators or major steps are supported by the data algebra and what methods operations on columns are supported. The operators are documented here and which methods can be used in which contexts is linsted here. Also please check the README for news. In particular we will discuss the Python implementation also called dataalgebra and its relation to the mature R implementations rquery and rqdatatable. Introduction The project intent is to realize a method chained data processing language based on Codds relational operators that is easy to maintain has helpful tooling and has very similar realizations or dialects for SQLdatabases accessed from Python useful working at scale with PostgreSQLor Apache Spark Sparkexample here. Pandas DataFrameobjects in Python. SQLdatabases access from Rimplementation is here and is mature and ready for production use. The intent is the notation should look idiomatic in each language. Working in Python should feel like working in Python and working in R should feel like working in R. The data semantics however are designed to be close to the SQL realizations given the close connection of SQL to the relational algebra in particular row numbering starts at 1 and row and column order is not preserved except at roworder steps or selectcolumns steps respectively. The intent is it should be very easy to use the system in either Python or R a boon to multilanguage data science projects and it is easy to port either code or experience from one system to another a boon for porting projects or for data scientists working with more than one code base or computer language. Related work includes Codds relational algebra SQL data. table dfply dplython LINQ Apache Calcite dplyr dtplyr table. express Pandas pandasply Polars SQLAlchemy rquery cdata siuba tidypolars Preql The dataalgebra principles include Writing data transforms as a pipeline or methodchain of many simple transform steps. Treating data transform pipelines or directed acyclic graphs DAGs as themselves being sharable data. Being able to use the same transform specification many places in memory on databases in R in Python. The dataalgebra supplies two primary services Building composite data processing pipelines which we demonstrate in this note. Building record transforms which we demonstrate here. Example Lets start with a pipeline example in Python for a record transform example please see here. For our example we will assume we have a data set of how many points different subjects score in a psychological survey. The goal is transform the data so that we see what fraction of the subjects answers are in each category subject to an exponential transform as often used in logistic regression. We then treat the persubject renormalized data as a probability or diagnosis. The exact meaning of such a scoring method are not the topic of this note. It is a notional example to show a nontrivial data transformation need. In particular having to normalize persubject divide some set of scores persubject by a persubject total is a classic pain point in dataprocessing. In classic SQL this can only be done by joining against a summary table or in more modern SQL with a window function. We want to show by working in small enough steps this can be done simply. Set up Lets start our Python example. First we import the packages we are going to use and set a few options. import polars as pl import dataalgebra as da import dataalgebra. BigQuery da. version 1. 5. 1 Now lets type in our example data. Notice this is an inmemory Polars Data. Frame. dlocal pl. DataFrame subjectID1 1 2 2 surveyCategory withdrawal behavior positive reframing withdrawal behavior positive reframing assessmentTotal 5. 2. 3. 4. irrelevantCol1 irrel14 irrelevantCol2 irrel24 dlocal Lets also copy this data to a database. Normally big data is already in the system one wants to work with so the copying over is just to simulate the data already being there. dbhandle dataalgebra. BigQuery. examplehandle printdbhandle BigQueryDBHandledbmodelBigQueryModel conngoogle. cloud. bigquery. client. Client object at 0x7fb1c0cad270 remotetabledescription dbhandle. inserttable dlocal. topandas tablenamed allowoverwriteTrue remotetabledescription. head Normally one does not read data back from a database but instead materializes results in the database with SQL commands such as CREATE TABLE tablename AS SELECT.... Also note case in columns is a bit of nightmare. It is often best to lowercase them all. Back to the dataalgebra Now we continue our example by importing the dataalgebra components we need. Now we use the dataalgebra to define our processing pipeline ops. We are writing this pipeline using a method chaining notation. This notation will look very much like a pipe to R magrittr users. scale 0. 237 ops da. descrddlocal. extendprobability fassessmentTotal scale. exp. extendtotal probability. sum partitionbysubjectID. extendprobability probability total. extendrownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability. selectrowsrownumber 1. selectcolumnssubjectID surveyCategory probability. renamecolumnsdiagnosis surveyCategory We are deliberately writing a longer pipeline of simple steps so we can use the same pipeline locally with Pandas or Polars and potentially great scale with PostgreSQL or Apache Spark. A more concise variation of this pipeline can be found in the R example here. The intent is the user can build up very sophisticated processing pipelines using a small number of primitive steps. The pipelines tend to be long but can still be very efficient as they are well suited for use with Polars and with SQL query optimizers. Most of the heavy lifting is performed by the very powerful window functions triggered by use of partitionby and orderby available on the extend step. Multiple statements can be combined into extend steps but only when they have the same windowstructure and dont create and use the same value name in the same statement except for replacement which is shown in this example. Many conditions are checked and enforced during pipeline construction making debugging very easy. For a more Pythonic way of writing the same pipeline we can show how the code would have been formatted by black. pysource ops. topythonprettyTrue printpysource TableDescription tablenamed columnnames subjectID surveyCategory assessmentTotal irrelevantCol1 irrelevantCol2. extendprobability assessmentTotal 0. 237. exp. extendtotal probability. sum partitionbysubjectID. extendprobability probability total. extend rownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability. selectrowsrownumber 1. selectcolumnssubjectID surveyCategory probability. renamecolumnsdiagnosis surveyCategory In either case the pipeline is read as a sequence of operations top to bottom and left to right. What it is saying is We start with a table named d that is known to have columns subjectID surveyCategory assessmentTotal irrelevantCol1 and irrelevantCol2. We produce a new table by transforming this table through a sequence of extend operations which add new columns. The first extendcomputes probability expscaleassessmentTotal this is similar to the inverselink step of a logistic regression. We assume when writing this pipeline we were given this math as a requirement. The next few extendsteps total the probabilitypersubject this is controlled by the partitionbyargument and then rank the normalized probabilities persubject grouping again specified by the partitionbyargument and order controlled by the orderbyclause. The first We then select the persubject topranked rows by the selectrowsstep. And finally we clean up the results for presentation with the selectcolumns renamecolumns and orderrowssteps. The names of these methods are intended to evoke what they do. The point is each step is deliberately so trivial one can reason about it. However the many steps in sequence do quite a lot. SQL Once we have the ops object we can do quite a lot with it. We have already exhibited the prettyprinting of the pipeline. Next we demonstrate translating the operator pipeline into SQL. sql dbhandle. tosqlops printsql dataalgebra SQL httpsgithub. comWinVectordataalgebra dialect BigQueryModel 1. 5. 1 string quote identifier quote WITH tablereference0 AS SELECT subjectID surveyCategory assessmentTotal FROM dataalgebratest. test1. d extend1 AS SELECT. extend probability assessmentTotal 0. 237. exp subjectID surveyCategory EXPassessmentTotal 0. 237 AS probability FROM tablereference0 extend2 AS SELECT. extend total probability. sum partitionbysubjectID subjectID surveyCategory probability SUMprobability OVER PARTITION BY subjectID AS total FROM extend1 extend3 AS SELECT. extend probability probability total subjectID surveyCategory probability total AS probability FROM extend2 extend4 AS SELECT. extend rownumber 1. cumsum partitionbysubjectID orderbyprobability reverseprobability subjectID surveyCategory probability SUM1 OVER PARTITION BY subjectID ORDER BY probability DESC AS rownumber FROM extend3 selectrows5 AS SELECT. selectrowsrownumber 1 subjectID surveyCategory probability FROM extend4 WHERE rownumber 1 SELECT. renamecolumnsdiagnosis surveyCategory surveyCategory AS diagnosis subjectID probability FROM selectrows5 Older SQL with use of with or common table expressions can be hard to read as SQL expresses composition by innernesting inside SELECT statements happen first. The operator pipeline expresses composition by sequencing or methodchaining which can be a lot more legible. In this example we use the SQL99 common table expression WITH notation to manage the composition in a more legible manner. A huge advantage of the SQL is we can send it to the database for execution as we do now. Also notice the generated SQL has applied query narrowing columns not used in the outer queries are removed from the inner queries. The irrelevant columns are not carried into the calculation as they would be with a SELECT. This early optimization comes in quite handy. dbhandle. readquerysql What comes back is one row per subject with the highest persubject diagnosis and the estimated probability. Again the math of this is outside the scope of this note think of that as something coming from a specification the ability to write such a pipeline is our actual topic. The hope is that the dataalgebra pipeline is easier to read write and maintain than the SQL query. If we wanted to change the calculation we would just add a stage to the dataalgebra pipeline and then regenerate the SQL query. Polars An advantage of the pipeline is it can also be directly used on Pandas or Polars DataFrames. Lets see how that is achieved. ops. evald dlocal There is also a shorthand notation for single table source pipelines ops. transformdlocal eval takes a dictionary of DataFrames names matching names specified in the pipeline and returns the result of applying the pipeline to this data. Currently our Pandas and Polars implementation only allows very simple window functions. This is why we didnt write probability probabilitysumprobability but instead broken the calculation into multiple steps by introducing the total column the SQL realization does in fact support more complex window functions. This is a small issue with the grammar but our feeling encourage simple steps is in fact a good thing improves debuggability and in SQL the query optimizers likely optimize the different query styles into very similar realizations anyway. Pandas The exact same pipeline can be applied directly to Pandas data frames. ops. transformdlocal. topandas ExportImport Because our operator pipeline is a Python object with no references to external objects such as the database connection it can be saved through standard methods such as pickling. Some Advantages of dataalgebra A dataalgebra operator pipeline carries around usable knowledge of the data transform. For example report all source table columns used by the query ops. columnsused d assessmentTotal subjectID surveyCategory what columns does this operation produce ops. columnnames subjectID diagnosis probability Conclusion The dataalgebra is part of a powerful crosslanguage and mutliimplementaiton family data manipulation tools. These tools can greatly reduce the development and maintenance cost of data science projects while improving the documentation of project intent. Win Vector LLC is looking for sponsors and partners to further the package. In particular if your group is using both R and Python in bigdata projects where SQL is a need including Apache Spark or are porting a project from one of these languages to another please get in touch. be neat dbhandle. close Note mysql is not fully supported as it doesnt name quoted common table expression columns in an obvious way. Current primary databases are PostgreSQL Google Big Query SparkSQL and SQLite.,2022-12-30T21:24:53Z,Delphi VCL & FMX Libraries for Python,Somewhat Positive,0,[],[],[]
11,Python Malware Starting to Employ Anti-Debug Techniques,"First time anti-debug techniques are discovered in PyPI malware. Read how these techniques are implemented, including analysis and tips from JFrog Security Research.",https://jfrog.com/blog/pypi-malware-creators-are-starting-to-employ-anti-debug-techniques/,PyPI malware creators are starting to employ AntiDebug techniques The JFrog Security Research team continuously monitors popular opensource software OSS repositories with our automated tooling and reports any vulnerabilities or malicious packages discovered to repository maintainers and the wider community. Most PyPI malware today tries to avoid static detection using various techniques starting from primitive variable mangling to sophisticated code flattening and steganography techniques. Use of these techniques makes the package extremely suspicious but it does prevent novice researchers from understanding the exact operation of the malware using static analysis tools. However any dynamic analysis tool such as a malware sandbox quickly removes the malwares static protection layers and reveals the underlying logic. Recently it seems that attackers have stepped up a notch weve recently detected and disclosed the cookiezlog package which seemed to employ Antidebugging code designed to thwart dynamic analysis tools in addition to regular obfuscation tools and techniques. This is the first time our research team or any publication have spotted these kinds of defenses in PyPI malware. In this post we will give an overview of the techniques used in this Python malware and how to unpack similar malware. Installation triggers Similar to most malicious packages the cookiezlog package runs immediately upon installation. This is achieved via develop and install triggers in setup. py class PostDevelopCommanddevelop def runself execute install. runself class PostInstallCommandinstall def runself execute install. runself... setup namecookiezlog version0. 0. 1 descriptionExtra Package for Roblox grabbing... cmdclass develop PostDevelopCommand install PostInstallCommand Static Obfuscation Part 1 The trivial stuff The first and simplest layer of protection is zlibencoded code which is executed immediately after the package is installed def execute import marshalzlibexecmarshal. loadszlib. decompressbxx9cMx90xc1Jxc3x10x86xebxb5Oxb1xecx01xd9xdd4Ix93x08x84xe0Axa8xa1x1ex85x98x0c6hvxd7... The decoded payload downloads a file from a hardcoded URL and executes it on the victims machine URL httpscdn. discordapp. comattachments10377234414800896001039359352957587516Cleaner. exe response requests. getURL openCleaner. exe wb. writeresponse. content os. systemset COMPACTLAYERRunAsInvoker start Cleaner. exe The executable is a Windows PE file. Looking at the strings in the executable we can see that its not actual native code but rather a Python script packed into the PE format strings Cleaner. exe grep PyIns Cannot open PyInstaller archive from executable s or external archive s PyInstaller FormatMessageW failed. PyInstaller pyiwin32utilstoutf8 failed. It can be quickly unpacked with the opensource tool PyInstaller Extractor. The extracted code contains a lot of files primarily thirdparty libraries. The most interesting extracted file is main. pyc which contains the malware code as Python bytecode. Static Obfuscation Part 2 Unpacking PyArmor Normally we would be able to decompile the bytecode in main. pyc to Python source code using tools such as uncompyle6. However in this case another run of strings on main. pyc shows that the binary has been obfuscated with PyArmor pytransformr pyarmor Distobfmain. py PyArmor is a commercial packer and obfuscator which applies obfuscation techniques to the original code encrypts it and protects it from analysis. Fortunately for the researchers PyArmor keeps much of the information thats necessary for introspection. Knowing this we can try to restore the names of the functions and constants used in the original code. Although PyArmor does not have any publiclyavailable unpacker it can be fully unpacked with some manual effort. In this case we chose to perform a quick unpacking shortcut by using library injection since we were mostly interested in the original symbols and strings. Trying to run the packed module as a standalone script produces an error specifying that the system doesnt have the required module python. exe. main. pyc Traceback most recent call last File distobfmain. py line 3 in File line 1 in ModuleNotFoundError No module named psutil Because the module looks for the psutil module we can create a module with the same name somewhere in the PYTHONPATH and it will be executed in the context of the process. This can be used as an easy entry point for injecting our own code into the process. We created our own file named psutil. py in the same directory as the protected file main. pyc with the following code import inspect for frame in inspect. stack for c in frame. frame. fcode. coconsts if not inspect. iscodec continue dis. showcodec The snippet uses the inspect module which allows to get a runtime information about the code being executed it iterates over execution frames and extracts the names of the code blocks and referenced constants. After running our snippet it returned a list of strings that allowed us to discern the capabilities and origin of the malicious code. The most interesting strings were the URL of an injection module pointing to the possible attackers repository and references to antiVM functionalities in the code Injector appd. d httpsraw. githubusercontent. comSyntheticcinjection1maininjection. js WEBHOOK IP index. js checkvm None VMwareService. exe VMwareTray. exe AntiDebug Techniques The Syntheticc GitHub profile mentioned in the strings was still available at the time of writing. The profiles repositories contain a bunch of opensource hacking tools. Among others there was a repository called Advanced Anti Debug containing methods that could be used to prevent analysis of the malware We can split the dynamic methods the malware used into two categories AntiDebug and AntiVM. The AntiDebug checks look for suspicious system activity related to any debuggers or disassemblers and includes the following functions checkprocesses looks whether debugger process runs on the system comparing the active process list to the list of over 50 known tools including idau64. exe IDA Pro Disassembler x64dbg. exe x64dbg Debugger Windbg. exe WinDbg Debugger Devenv. exe Visual Studio IDE Processhacker. exe Process Hacker PROCNAMES ProcessHacker. exe httpdebuggerui. exe wireshark. exe fiddler. exe regedit. exe... for proc in psutil. processiter if proc. name in PROCNAMES proc. kill checkresearchtools has almost the same functionality comparing substrings of process names to a humble list of five traffic analysis tools wireshark Wireshark network protocol analyzer fiddler Fiddler proxy http HTTP Debugger and possibly more tools traffic generic term packet generic term If any of these processes are found to be running the AntiDebug code tries to kill the process via psutil. Process. kill not a very subtle approach. Malware that is more stealthconscious would just stop running without any indication instead of interacting with external processes. The other antidebug techniques try to make sure the malware is not running inside a virtual machine checkdll checks the system root directories for DLLs indicating that the system is running under a VMWare vmGuestLib. dll or VirtualBox vboxmrxnp. dll virtual machine guest. checkvm checks if any VMwarerelated processes are running specifically VMwareService. exe or VMwareTray. exe. checkregistry looks for keys used by virtual machines for example a wellknown registry key that gets added when VMWare drivers are installed HKEYLOCALMACHINESYSTEM ControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000DriverDesc def checkregistry if systemREG QUERY HKEYLOCALMACHINESYSTEMControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000DriverDesc 2 nul 1 and systemREG QUERY HKEYLOCALMACHINESYSTEMControlSet001ControlClass4D36E968E32511CEBFC108002BE103180000ProviderName 2 nul 1exitprogramDetected Vm handle OpenKeyHKEYLOCALMACHINE SYSTEMCurrentControlSetServicesDiskEnum try if VMware in QueryValueExhandle 00 or VBOX in QueryValueExhandle 00 exitprogramDetected Vm finally CloseKeyhandle Last but not least the checkspecs function analyzes the current machine usage def checkspecs if intstrvirtualmemory0102410241024. split. 0 4 exitprogramMemory Ammount Invalid if intstrdiskusage0102410241024. split. 0 50 exitprogramStorage Ammount Invalid if intcpucount 1 exitprogramCpu Counts Invalid If there is a small amount of memory disk space or only one CPU it assumes that the process is running inside a virtual machine. All of the checks mentioned above are relatively simple but with the respectable protection against static analysis the malware already employed it offers adequate protection against novice researchers especially ones who only use automated analysis tools which wouldnt be able to breach the defenses of this specific malware. The Payload Simple Password Grabber The payload is disappointingly simple compared to the amount of defenses used by the malware but it is still harmful. The payload is a password grabber which gathers autocomplete passwords saved in the data caches of popular browsers and sends them to the C2 server in this case a Discord hook httpsdiscord. comapiwebhooks1039353898445582376cvrsu8CslmIYzNyXMpkjbkNEyO0yjg08x5Ra7mPdgooQquALPINn1YfD5CuJ11dM7h. From the strings extracted from the malware we can deduce that in addition to the industry standard Discord token leaker functionality the payload also hunts for passwords of several financial services as can be seen by strings used by the sendinfo function Name sendinfo Filename Argument count 0... Constants 0 None 1 USERPROFILE... 5 coinbase... 7 binance... 9 paypal... Summary We can once again see that malware developers constantly evolve their arsenal adding new methods of evasion and new layers of protection against analysis of their tools. Just a couple of years ago the only tools that PyPI malware authors used were simple payload encoders. Today we see that malware thats uploaded to OSS repositories is becoming more complex has a few levels of static and dynamic protection and utilize combinations of commercial and homebrew tools. This is similar to their colleagues in the world of native malware and as such we are expecting OSSrepo malware to continue to evolve perhaps with advanced techniques such as custom polymorphic encoding and deeper antidebug methods. Stay uptodate with JFrog Security Research Follow the latest discoveries and technical updates from the JFrog Security Research team in our security research blog posts and on Twitter at JFrogSecurity.,2022-12-23T07:04:29Z,No Headline,Extremely Negative,320,"['regression', 'pain', 'object', 'nightmare', 'primitive', 'regression', 'trivial', 'object', 'hard', 'irrelevant', 'broken', 'issue', 'object']",[],"[extend, rownumber, 1, ., evald, dlocal]"
12,Signed distance functions in 46 lines of Python,A walkthrough of 46 lines of code that render a 3D ASCII donut using signed distance functions.,https://vgel.me/posts/donut/,Signed distance functions in 46 lines of Python Signed distance functions are a really cool method of 3D rendering But they unfortunately have a reputation for being difficult to understand. It makes sense whythey usually get shown off in beautiful but complicated ShaderToy examples written in GLSL an unfamiliar language for most programmers 1. But at their core SDFs are a really simple idea. Im going to prove that by walking you through a program that raymarches an animated SDF donut in only 46 lines of Python. Just for fun and to make it easy to port to your favorite language that can also print strings to the terminal well also be doing it with ASCII art instead of a graphics API. So come along By the end you wont just have this deliciouslooking spinning ASCII donut but an understanding of a cool rendering technique you can use for all kinds of neat things. Setting up So to start off with lets slap down some Python to render our ASCII frame. Well also add in a game loop so we can do animation import time def samplex int y int str draw an alternating checkboard pattern if x y inttime. time 2 return else return while True loop over each position and sample a character framechars for y in range20 for x in range80 framechars. appendsamplex y framechars. appendn print out a control sequence to clear the terminal then the frame on windows add import os os. system to the beginning of the program to enable ANSI escape sequences. print0332J. joinframechars cap at 30fps time. sleep130 This renders a 80x20 checkerboard which alternates every second This is a nice foundation but not too visually interesting so lets move on. The task ahead is simple for each character on the screen decide what the character should beeasy Drawing a circle Lets start with something simple. We have an x and a y coordinate so we can easily draw a circle if not anything 3D yet. Theres a few different ways we could go about this but in the spirit of for each character on the screen decide what character it should be well do a characterbycharacter approach. The basic algorithm will be for each x y coordinate of a character Calculate the distance of x yfrom the center of the screen x02 y02 aka x2y2 Subtract the desired circle radius. That way if the point is inside or on the edge of the circle the value will be 0 and otherwise it will be 0. Now test that value against 0 to either return if the point is inside or on the edge of the circle or otherwise. Well also remap x and y to 1.. 1 and. 5... 5 respectively to gesture in the direction of resolution independence and to keep the aspect ratio correct 2 2080 0. 5 as y covers only 20 characters while x covers 80 and terminal characters are roughly twice as tall as they are wide. This prevents the circle from looking instead like an unappetizingly squished bean. import math time def circlex float y float float since the range of x is 1.. 1 the circles radius will be 40 meaning the circles diameter is 40 of the screen radius 0. 4 calculate the distance from the center of the screen and subtract the radius so d will be 0 inside the circle 0 on the edge and 0 outside return math. sqrtx2 y2 radius def samplex float y float str return a if were inside the circle and otherwise if circlex y 0 return else return while True framechars for y in range20 for x in range80 remap to 1.. 1 range for x... remappedx x 80 2 1... and corrected for aspect ratio range for y remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 Hey thats a circle if Ive ever seen one We didnt use time. time in this one so the circle isnt animatedwell bring animation back later I promise. A 2D donut A circle is just half a 2D donutthe hole is just another circle if you think about it. 2 So lets add the other circle to make a 2D donut Theres a few different ways to do this see if you can figure out a way on your own but a nice way is to define it as a radius thickness around that radius import math time def donut2dx float y float float same radius as before though the donut will appear larger as half the thickness is outside this radius radius 0. 4 how thick the donut will be thickness 0. 3 take the abs of the circle calculation from before subtracting thickness 2. abs... will be 0 on the edge of the circle and increase as you move away. therefore abs... thickness 2 will be 0 only thickness 2 units away from the circles edge on either side giving a donut with a total width of thickness return absmath. sqrtx2 y2 radius thickness 2 def samplex float y float str if donut2dx y 0 return else return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 This representation radius thickness is artistically nice because radius and thickness are relatively independent parameters they can be changed with little reference to each other. 3 But its also nice for our code since its only a slight tweak from how we were calculating the distance before we calculated the distance from the center of the circle and now we calculate the distance from the edge of that circle. By subtracting thickness 2 from that edge distance the result will be 0 if the point is less than thickness 2 from the edge of the circle resulting in a ring of the given thickness centered on the edge of the circle with the given radius. The other cool thing is that the tweak was so small that our code was able to stay almost exactly the same. We just had to update the signed distance functionoops gave it awayand didnt have to change the rest of the rendering loop More generally regardless of what SDF we use our rendering loop can stay the samewere just sampling distances at pixels. Going 3D mode Time to enter the exciting decade of the 1990s and bring 3D graphics to the table Well step back from the complexity of the donut slightly and start by rendering a sphere which has an almost identical SDF to a circle but with a new term Z def spherex float y float z float float radius 0. 4 return math. sqrtx2 y2 z2 radius Before X was the horizontal axis and Y the vertical Z will give our image depth. 4 Well also reuse the same framechars loop as before. The only function that needs to substantially change is sample which now needs to handle the third dimension. Fundamentally we need a function that takes in an x y 2D point and samples 3D space in some way related to that point. In other words we need to come up with the correct Z to sample to get a reasonable character to render. We could cheat and simply always sample at z 0 that would render a 2D slice of our 3D world showing the inside of any object that happened to cross the z0 plane. 5 But to get a more interesting view we need to simulate the real world. Imagine an eye which is to a first approximation a 2D plane how does it see distant objects Well the sun shoots out rays which either hit the eyeplane directly not good wear sunglasses folks or bounce off one or more objects and then hit the eye. We could treat our screen the same way for each call to samplex y well shoot out millions of rays from a simulated light hoping that at least one will bounce off an object and pass through x y cameraz. That approach would be slightly slow however. The odds of any given ray hitting that specific point would be vanishingly small and most of the work would be completely wasted. You could throw all of useast1 at this python code sorry Reddit need to borrow your servers for a second but lets take a shortcut instead. In samplex y we only care about the ray of light that passes through x y cameraz. So why bother with all the other rays Well shoot the ray backwards Well start it at x y cameraz and at each step query the SDF to get the distance from the rays current point to the scene in any direction. If the distance is less than some threshold we hit the scene Otherwise we can safely march the ray forward by whatever distance is returned since we know the scene is at least that distance away in the forward direction. It may be moreimagine the ray passes close to the scene but never hits it while the ray is near the scene the distance queried from the SDF will be small so the ray will be forced to move slowly but eventually after some large number of steps it will move past the scene and start moving quickly again. Well arbitrarily limit the number of marches steps to 30 and return the background character if a ray doesnt hit something by then. With all of that this is what our new 3D sample function looks like def samplex float y float str start z far back from the scene which is centered at 0 0 0 so nothing clips z 10 well step at most 30 steps before assuming we missed the scene for step in range30 get the distance just like in 2D d spherex y z test against 0. 01 not 0 were a little more forgiving with the distance in 3D for faster convergence if d 0. 01 we hit the sphere return else didnt hit anything yet move the ray forward we can safely move forward by d without hitting anything since we know thats the distance to the scene z d we didnt hit anything after 30 steps return the background return And putting it all together heres the code for rendering a sphere import math time def spherex float y float z float float radius 0. 4 return math. sqrtx2 y2 z2 radius def samplex float y float str radius 0. 4 z 10 for step in range30 d spherex y z if d 0. 01 return else z d return this is unchanged while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 OK this isnt very impressive I admit it. If you didnt know any better youd probably accuse me of just reusing the circle image from before But I promise this is a sphere reallylets keep moving and trust for now that its 3D. A 3D donut To move towards our 3D donut next well need to replace the simple sphere SDF with the more complex torus donut SDF. The rest of the code stays the same import math time def donutx float y float z float float radius 0. 4 thickness 0. 3 first we get the distance from the center and subtract the radius just like the 2d donut. this value is the distance from the edge of the xy circle along a line drawn between x y 0 and 0 0 0 the center of the donut. xyd math. sqrtx2 y2 radius now we need to consider z which since were evaluating the donut at 0 0 0 is the distance orthogonal on the z axis to that x y 0.. 0 0 0 line. we can use these two values in the usual euclidean distance function to get the 3D version of our 2D donut distance from edge value. d math. sqrtxyd2 z2 then we subtract thickness 2 as before to get the signed distance just like in 2D. return d thickness 2 unchanged from before except for sspheredonutg def samplex float y float str z 10 for step in range30 d donutx y z if d 0. 01 return else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 As before this donut isnt very impressive but since its not symmetrical we can now add some motion to prove its 3D. A 3D donut that really spins All we need to do to make the donut spin is transform the points in sample before we evaluate the SDF def samplex float y float str... for step in range30 calculate the angle based on time to animate the donut spinning time. time 2 rotate the input coordinates which is equivalent to rotating the sdf tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz... This will rotate the donut around the y axis which is why y is unchanged. We calculate 6 persample 7 and then calculate a rotation matrix by hand because Im too lazy to import numpy real programmers dont use dependencies. Here it is incontext import math time def donutx float y float z float float radius 0. 4 thickness 0. 3 return math. sqrtmath. sqrtx2 y2 radius2 z2 thickness 2 def samplex float y float str z 10 for step in range30 time. time 2 tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz if d 0. 01 return else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 See thats definitely 3D no nits about it But since were only at 32 lines count em lets kick it up a notch and add some simple lighting and texturing with a normal vector estimator. Lighting it up and adding frosting To add lighting and a frosting texture well need to calculate normal vectors for our scene. A normal 8 vector is defined for an object at every point on that objects surface and is the vector that sticks straight out from that pointimagine the spines on a cactus or someones hair after touching a staticy balloon. Most surfaces have an analytic way to calculate the normal vector but that can get difficult when you start combining multiple SDFs into a complex scene. Plus who wants to write out a normal function for each SDF they write That sucks So we can cheat and instead estimate the normal by sampling the SDF on each axis around a target point Sdf typing. Callablefloat float float float def normalsdf Sdf x float y float z float tuplefloat float float an arbitrary small amount to offset around the point 0. 001 calculate each axis independently nx sdfx y z sdfx y z ny sdfx y z sdfx y z nz sdfx y z sdfx y z normalize the result to length 1 norm math. sqrtnx2 ny2 nz2 return nx norm ny norm nz norm To make sense of how this function works imagine the case where a component of the normal say x is 0. That means the SDF at that point is flat on the x axis so sdfx y z sdfx y z. As those values diverge the x component of the normal will grow either more positive or negative rotating it around. Its just an estimation but for rendering its usually good enough and even advanced demos will often use it. The downside is it can be quite slow since it needs to sample the SDF six times for every call As the scene SDF grows more complicated that can become a big performance drain. But for us its good enough Well calculate the normal in sample if the ray hits and use it to compute some lighting and texturing if d 0. 01 nty ntz normaldonut tx y tz islit nty 0. 15 isfrosted ntz 0. 5 if isfrosted return if islit else else return if islit else. We dont care about the x component of the normal just y and z. Well use y to calculate the lighting assuming that if a surface is facing up the normals y is close to 1 it should be lit. Well use z to show the frostingby thresholding against different values we can make the donut more or less frosted. The best way to gain an intuition for what these values mean is to play with them so try changing them around in a local copy of the code below and watch what happens import math time typing def donutx float y float z float float radius 0. 4 thickness 0. 3 return math. sqrtmath. sqrtx2 y2 radius2 z2 thickness 2 Sdf typing. Callablefloat float float float def normalsdf Sdf x float y float z float tuplefloat float float 0. 001 nx sdfx y z sdfx y z ny sdfx y z sdfx y z nz sdfx y z sdfx y z norm math. sqrtnx2 ny2 nz2 return nx norm ny norm nz norm def samplex float y float str z 10 for step in range30 time. time 2 tx x math. cos z math. sin tz x math. sin z math. cos d donuttx y tz if d 0. 01 nty ntz normaldonut tx y tz islit nty 0. 15 isfrosted ntz 0. 5 if isfrosted return if islit else else return if islit else. else z d return while True framechars for y in range20 for x in range80 remappedx x 80 2 1 remappedy y 20 2 1 2 2080 framechars. appendsampleremappedx remappedy framechars. appendn print0332J. joinframechars time. sleep130 Theres our final 3D donut lit textured and spinning as promised in only 46 lines of code Thanks for sticking through and I hope this article is inspiring to try and make your own SDF creations. If youre interested in learning more about SDFs there are many resources online but none I could recommend more highly than Inigio Quilezs websitehe is an SDF master and has made some truly mindblowing art using SDFs. He also has a Youtube channel where he posts videos of his work and tutorials. I recommend this video where he makes a landscape using SDFs. Enjoy and thanks for reading to the end 1 Though a cool one thats worth learning 2 Technically a 2D donut is a circle minus a circle so a donut circle circle 0. And a donut is shaped like a 0... coincidence I think not 3 If you instead for example defined the donut as two radii where a point is filled if innerradius distance from center outerradius you permit a nonsense construction where innerradius outerradius. 4 vgel. me is a bastion of the Yup that is to say correct way of rendering. 5 This is how many 4D games work they render a 3D slice of a 4D world and then confusingly render that 3D slice to a 2D screen.... That slicing has always sort of bothered me since it allows you to see inside objects in a way a real 4D being couldnt. Ive experimented with and one day want to publish or see made a 4D game that does 4D raymarching to generate a 3D mesh and then renders that mesh so you can only see the surfaces of 4D objects not their interiors. 6 Yes I could have just written theta but I installed a polytonic Greek IME for language study and the installation process was sufficiently annoying that I intend to get as much use out of it as possible 7... which means if sample was sufficiently slow there would be an interesting warp effect. You can emulate this without making things actually slow with time. time2 xy8015 which looks really cool. Try... xy8015math. sintime. time for some even trippier visuals 8 normal in this context being based on the original meaning of the Latin word normlis rightangled from norma a carpenters square. The more common use of normal to mean regular or as it ought to be was a metaphorical extension of that older meaningsort of like the much newer phrase squared away. Its like poetry it rhymes Previous entrymmap1Tb A Rust arena allocator abusing Linux overcommit Next entry2022 A Mitigated Success,2022-12-20T22:00:30Z,PyPI malware creators are starting to employ Anti-Debug techniques,Extremely Negative,630,"['malicious', 'static', 'primitive', 'mangling', 'suspicious', 'static', 'static', 'thwart', 'malicious', 'Static', 'trivial', 'Static', 'error', 'malicious', 'split', 'suspicious', 'split', 'Invalid', 'split', 'Invalid', 'Invalid', 'static', 'breach', 'disappointingly', 'harmful', 'Discord', 'Discord', 'evasion', 'static']","['kill', 'kill', 'kill']",[]
13,Show HN: Self Hosted OpenStreetMap using only Sqlite3 and Python,Reverse Geocode for OpenStreetmap. Contribute to punnerud/rgcosm development by creating an account on GitHub.,https://github.com/punnerud/rgcosm,RGCosm Reverse Geocode for OpenStreetmap Locally hosted OpenStreetmap using sqlite3 for reverse geocode. So you easily can find adresses based on coordinates. Download the pbf file from httpsdownload. geofabrik. de Then use convert. py to create the database python3 convert. py You have to change norwaylatest. osm. pbf in convert. py into your filename. The norwaylatest. osm. pbf is about 1GB and the sqlite3 end up 10GB. With indexes 16GB. So dont try with the biggest areas for starting. Takes about 15minutes for the norway file. To speed up your queries it is highly recommended to add indexes. This increase the size around 50 and takes a couple of minutes to create CREATE INDEX nodes index lat ON nodes lat CREATE INDEX nodes index lon ON nodes lon Adding indexes change the search time for my Norway file from 10 to 0. 15 seconds. Changing the lookaround query can also reduce the search time at the risk that you miss an adress if the closest adress is more far away. Mac users I found this to work for installation of osmium for Python brew install cmake brew install wheel brew install osmiumtool python3 m pip install osmium Premade sqlite3 database for Norway with indexes if you just want to try it httpswww. dropbox. coms4rrhxpfzulqbvxrosm. dbdl0,2022-12-30T22:45:50Z,No Headline,Extremely Negative,700,"['unfortunately', 'difficult', 'complicated', 'unfamiliar', 'slap', 'Subtract', 'subtract', 'cheat', 'object', 'object', 'slow', 'wasted', 'sorry', 'bother', 'limit', 'missed', 'accuse', 'subtract', 'subtract', 'sin', 'sin', 'lazy', 'sin', 'sin', 'object', 'difficult', 'sucks', 'cheat', 'arbitrary', 'negative', 'downside', 'slow', 'complicated', 'drain', 'sin', 'sin', 'nonsense', 'bothered', 'annoying', 'slow', 'warp', 'slow', 'Rust']",['hole'],"['lawsuit', Inigio, Quilezs]"
